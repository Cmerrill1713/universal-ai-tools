#!/usr/bin/env python3
"""
Local AI TTS Service - Uses Ollama and local synthesis for high-quality voices
No external APIs - everything runs locally
"""

import argparse
import base64
import logging
import os
import subprocess
import tempfile
import time
from typing import Any, Dict

from flask import Flask, jsonify, request
from flask_cors import CORS

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('local-ai-tts-service')

# Initialize Flask app
app = Flask(__name__)
CORS(app)


class LocalAITTS:
    """Local AI TTS using Ollama and advanced synthesis"""

    def __init__(self):
        self.ollama_available = self._check_ollama()
        self.voices = {
            'narrator_male': {
                'voice': 'Alex',
                'description': 'Professional male narrator - clear and authoritative',
                'pitch': 0.0,
                'rate': 200,
                'emotions': {
                    'professional': {'rate': 200, 'pitch': 0.0},
                    'friendly': {'rate': 220, 'pitch': 0.1},
                    'calm': {'rate': 180, 'pitch': -0.1},
                    'excited': {'rate': 250, 'pitch': 0.2},
                    'serious': {'rate': 190, 'pitch': -0.05}
                }
            },
            'narrator_female': {
                'voice': 'Samantha',
                'description': 'Professional female narrator - warm and engaging',
                'pitch': 0.0,
                'rate': 200,
                'emotions': {
                    'professional': {'rate': 200, 'pitch': 0.0},
                    'friendly': {'rate': 220, 'pitch': 0.1},
                    'calm': {'rate': 180, 'pitch': -0.1},
                    'excited': {'rate': 250, 'pitch': 0.2},
                    'serious': {'rate': 190, 'pitch': -0.05}
                }
            },
            'conversational_male': {
                'voice': 'Daniel',
                'description': 'Conversational male voice - natural and approachable',
                'pitch': 0.0,
                'rate': 200,
                'emotions': {
                    'professional': {'rate': 200, 'pitch': 0.0},
                    'friendly': {'rate': 220, 'pitch': 0.1},
                    'calm': {'rate': 180, 'pitch': -0.1},
                    'excited': {'rate': 250, 'pitch': 0.2},
                    'happy': {'rate': 230, 'pitch': 0.15}
                }
            },
            'conversational_female': {
                'voice': 'Moira',
                'description': 'Conversational female voice - natural and approachable',
                'pitch': 0.0,
                'rate': 200,
                'emotions': {
                    'professional': {'rate': 200, 'pitch': 0.0},
                    'friendly': {'rate': 220, 'pitch': 0.1},
                    'calm': {'rate': 180, 'pitch': -0.1},
                    'excited': {'rate': 250, 'pitch': 0.2},
                    'happy': {'rate': 230, 'pitch': 0.15}
                }
            },
            'child': {
                'voice': 'Junior',
                'description': 'Child voice - playful and energetic',
                'pitch': 0.3,
                'rate': 220,
                'emotions': {
                    'excited': {'rate': 280, 'pitch': 0.4},
                    'happy': {'rate': 250, 'pitch': 0.3},
                    'calm': {'rate': 200, 'pitch': 0.2},
                    'playful': {'rate': 260, 'pitch': 0.35}
                }
            },
            'elderly': {
                'voice': 'Albert',
                'description': 'Elderly voice - mature and wise',
                'pitch': -0.2,
                'rate': 180,
                'emotions': {
                    'professional': {'rate': 180, 'pitch': -0.2},
                    'calm': {'rate': 160, 'pitch': -0.3},
                    'wise': {'rate': 170, 'pitch': -0.25},
                    'serious': {'rate': 175, 'pitch': -0.2}
                }
            },
            'robot': {
                'voice': 'Zarvox',
                'description': 'Robot voice - mechanical and precise',
                'pitch': -0.1,
                'rate': 200,
                'emotions': {
                    'professional': {'rate': 200, 'pitch': -0.1},
                    'serious': {'rate': 190, 'pitch': -0.15},
                    'calm': {'rate': 180, 'pitch': -0.2},
                    'mechanical': {'rate': 210, 'pitch': -0.05}
                }
            }
        }

        logger.info(f"Local AI TTS initialized with {len(self.voices)} voice profiles")
        logger.info(f"Ollama available: {self.ollama_available}")

    def _check_ollama(self) -> bool:
        """Check if Ollama is available"""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except Exception:
            return False

    def generate_speech(self, text: str, speaker: str = "narrator_male", emotion: str = "professional") -> Dict[str, Any]:
        """Generate speech using local AI synthesis"""
        try:
            logger.info(f"Generating speech: '{text[:50]}...' with speaker: {speaker}, emotion: {emotion}")

            # Get voice configuration
            voice_config = self.voices.get(speaker, self.voices['narrator_male'])
            voice_name = voice_config['voice']
            emotion_config = voice_config['emotions'].get(emotion, voice_config['emotions']['professional'])

            # Enhance text with AI if Ollama is available
            enhanced_text = self._enhance_text_with_ai(text, emotion) if self.ollama_available else text

            # Generate speech using advanced macOS synthesis
            return self._generate_advanced_speech(enhanced_text, voice_name, emotion_config, speaker, emotion)

        except Exception as e:
            logger.error(f"Speech generation failed: {e}")
            return self._mock_generate_speech(text, speaker)

    def _enhance_text_with_ai(self, text: str, emotion: str) -> str:
        """Use Ollama to enhance text for better speech synthesis"""
        try:
            emotion_prompts = {
                'professional': 'Make this text sound more professional and clear:',
                'friendly': 'Make this text sound more friendly and warm:',
                'calm': 'Make this text sound more calm and soothing:',
                'excited': 'Make this text sound more excited and energetic:',
                'serious': 'Make this text sound more serious and authoritative:',
                'happy': 'Make this text sound more happy and cheerful:',
                'playful': 'Make this text sound more playful and fun:',
                'wise': 'Make this text sound more wise and thoughtful:',
                'mechanical': 'Make this text sound more mechanical and precise:'
            }

            prompt = f"{emotion_prompts.get(emotion, 'Improve this text for speech:')} {text}"

            result = subprocess.run([
                'ollama', 'run', 'llava:7b',
                prompt
            ], capture_output=True, text=True, timeout=10)

            if result.returncode == 0 and result.stdout.strip():
                enhanced = result.stdout.strip()
                # Take the first sentence or paragraph
                enhanced = enhanced.split('\n')[0].split('.')[0] + '.'
                logger.info(f"AI enhanced text: {enhanced[:100]}...")
                return enhanced
            else:
                return text

        except Exception as e:
            logger.warn(f"AI text enhancement failed: {e}")
            return text

    def _generate_advanced_speech(self, text: str, voice_name: str, emotion_config: Dict, speaker: str, emotion: str) -> Dict[str, Any]:
        """Generate speech using advanced macOS synthesis with emotion"""
        try:
            # Create temporary file for audio output
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_path = temp_file.name

            # Build advanced say command with emotion-based parameters
            rate = emotion_config['rate']
            pitch = emotion_config['pitch']

            # Add SSML-like enhancements for better quality
            enhanced_text = self._add_speech_enhancements(text, emotion)

            # Use say command with advanced parameters (fix format issue)
            cmd = [
                'say',
                '-v', voice_name,
                '-r', str(rate),
                '-o', temp_path.replace('.wav', '.aiff'),  # Use AIFF format
                enhanced_text
            ]

            logger.info(f"Running advanced synthesis: {voice_name} at {rate} rate")

            start_time = time.time()
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            generation_time = time.time() - start_time

            aiff_path = temp_path.replace('.wav', '.aiff')
            if result.returncode == 0 and os.path.exists(aiff_path):
                # Get file size
                file_size = os.path.getsize(aiff_path)

                # Read and encode audio data
                with open(aiff_path, 'rb') as f:
                    audio_data = base64.b64encode(f.read()).decode('utf-8')

                # Clean up temp file
                os.unlink(aiff_path)

                # Estimate duration
                duration = max(1.0, len(text) * 0.08)

                logger.info(f"âœ… Advanced synthesis successful: {voice_name}, {file_size} bytes")

                return {
                    "success": True,
                    "audio_data": audio_data,
                    "audio_format": "wav",
                    "sample_rate": 22050,
                    "duration": duration,
                    "generation_time": generation_time,
                    "speaker": speaker,
                    "emotion": emotion,
                    "voice_used": voice_name,
                    "voice_description": self.voices[speaker]['description'],
                    "file_size": file_size,
                    "text": text,
                    "enhanced_text": enhanced_text,
                    "provider": "Local AI TTS",
                    "ollama_enhanced": self.ollama_available
                }
            else:
                raise Exception(f"Say command failed: {result.stderr}")

        except Exception as e:
            logger.error(f"Advanced synthesis failed: {e}")
            return self._mock_generate_speech(text, speaker)

    def _add_speech_enhancements(self, text: str, emotion: str) -> str:
        """Add speech enhancements based on emotion"""
        enhancements = {
            'excited': '[[rate 1.2]]',
            'happy': '[[rate 1.1]]',
            'calm': '[[rate 0.9]]',
            'serious': '[[rate 0.95]]',
            'playful': '[[rate 1.15]]',
            'wise': '[[rate 0.9]]',
            'mechanical': '[[rate 1.05]]'
        }

        # Add pauses for better speech flow
        enhanced = text.replace('.', '. [[slnc 200]]')
        enhanced = enhanced.replace(',', ', [[slnc 100]]')
        enhanced = enhanced.replace('!', '! [[slnc 300]]')
        enhanced = enhanced.replace('?', '? [[slnc 250]]')

        # Add emotion-based rate if available
        if emotion in enhancements:
            enhanced = f"{enhancements[emotion]} {enhanced}"

        return enhanced

    def _mock_generate_speech(self, text: str, speaker: str) -> Dict[str, Any]:
        """Mock speech generation when synthesis fails"""
        logger.info(f"Mock generating speech: '{text[:50]}...' with speaker: {speaker}")

        try:
            import struct
            import wave

            sample_rate = 22050
            duration = max(1.0, len(text) * 0.08)
            num_samples = int(sample_rate * duration)

            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_path = temp_file.name

            with wave.open(temp_path, 'wb') as wav_file:
                wav_file.setnchannels(1)
                wav_file.setsampwidth(2)
                wav_file.setframerate(sample_rate)

                silence = struct.pack('<h', 0) * num_samples
                wav_file.writeframes(silence)

            with open(temp_path, 'rb') as f:
                audio_data = base64.b64encode(f.read()).decode('utf-8')

            os.unlink(temp_path)

            return {
                "success": True,
                "audio_data": audio_data,
                "audio_format": "wav",
                "sample_rate": sample_rate,
                "duration": duration,
                "generation_time": 0.1,
                "speaker": speaker,
                "emotion": "neutral",
                "voice_used": "mock_voice",
                "voice_description": "Mock voice",
                "text": text,
                "mock": True,
                "provider": "Local AI TTS (mock)"
            }

        except Exception as e:
            logger.error(f"Mock speech generation failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "text": text,
                "speaker": speaker
            }

    def list_voices(self) -> Dict[str, Any]:
        """List available local AI voices"""
        voices = []
        for speaker_id, config in self.voices.items():
            voices.append({
                "id": speaker_id,
                "name": config['voice'],
                "description": config['description'],
                "emotions": list(config['emotions'].keys()),
                "provider": "Local AI TTS",
                "ollama_enhanced": self.ollama_available
            })

        return {
            "voices": voices,
            "count": len(voices),
            "provider": "Local AI TTS",
            "ollama_available": self.ollama_available
        }

    def health_check(self) -> Dict[str, Any]:
        """Health check"""
        return {
            "status": "healthy",
            "provider": "Local AI TTS",
            "ollama_available": self.ollama_available,
            "voices_available": len(self.voices),
            "local_only": True,
            "no_external_apis": True
        }


# Global TTS instance
local_ai_tts = LocalAITTS()


@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    health = local_ai_tts.health_check()
    health.update({
        "service": "local-ai-tts-service",
        "timestamp": int(time.time()),
    })
    return jsonify(health), 200


@app.route('/voices', methods=['GET'])
def list_voices():
    """List available local AI voices"""
    try:
        voices = local_ai_tts.list_voices()
        return jsonify({
            "success": True,
            "message": "Local AI voices retrieved successfully",
            "data": voices
        })
    except Exception as e:
        logger.error(f"List voices error: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route('/synthesize', methods=['POST'])
def synthesize_speech():
    """Synthesize speech using local AI"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                "success": False,
                "error": "No data provided"
            }), 400

        text = data.get('text')
        speaker = data.get('speaker', 'narrator_male')
        emotion = data.get('emotion', 'professional')

        if not text or not isinstance(text, str):
            return jsonify({
                "success": False,
                "error": "Text is required and must be a string"
            }), 400

        if len(text) > 2000:
            return jsonify({
                "success": False,
                "error": "Text length exceeds maximum (2000 characters)"
            }), 400

        logger.info(f"Synthesizing speech: speaker={speaker}, text_length={len(text)}")

        result = local_ai_tts.generate_speech(text, speaker, emotion)

        if result.get("success"):
            return jsonify({
                "success": True,
                "message": "Speech synthesized successfully with local AI",
                "data": result
            })
        else:
            return jsonify({
                "success": False,
                "error": result.get("error", "Synthesis failed")
            }), 500

    except Exception as e:
        logger.error(f"Synthesis error: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route('/status', methods=['GET'])
def get_status():
    """Get service status and capabilities"""
    try:
        voices = local_ai_tts.list_voices()
        health = local_ai_tts.health_check()

        return jsonify({
            "success": True,
            "message": "Local AI TTS service status retrieved successfully",
            "data": {
                **health,
                "voices": voices,
                "capabilities": [
                    "text_to_speech",
                    "local_ai_enhancement",
                    "emotion_based_synthesis",
                    "ollama_integration",
                    "advanced_speech_enhancements",
                    "no_external_apis"
                ],
                "supported_formats": ["wav"],
                "max_text_length": 2000,
                "provider": "Local AI"
            }
        })
    except Exception as e:
        logger.error(f"Status error: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "success": False,
        "error": "Endpoint not found"
    }), 404


@app.errorhandler(500)
def internal_error(error):
    logger.error(f"Internal server error: {str(error)}")
    return jsonify({
        "success": False,
        "error": "Internal server error"
    }), 500


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Local AI TTS Service")
    parser.add_argument("--port", type=int, default=8088,
                        help="Port to run the service on")
    parser.add_argument("--host", type=str, default="0.0.0.0",
                        help="Host to bind to")

    args = parser.parse_args()

    logger.info(f"Starting Local AI TTS Service on {args.host}:{args.port}")
    logger.info(f"Available local AI voices: {len(local_ai_tts.voices)}")
    logger.info(f"Ollama integration: {local_ai_tts.ollama_available}")

    app.run(
        host=args.host,
        port=args.port,
        debug=False,
        threaded=True
    )
