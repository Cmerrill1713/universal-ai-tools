# Ollama configuration for local development
AI_INFERENCE_API_HOST=http://host.docker.internal:11434

# Alternative OpenAI-compatible configuration (optional)
OPENAI_BASE_URL=http://host.docker.internal:11434/v1
OPENAI_API_KEY=sk-ollama-local