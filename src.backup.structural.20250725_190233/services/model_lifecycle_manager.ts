/* eslint-disable no-undef */;
/**;
 * Model Lifecycle Manager;
 * Handles intelligent model loading, warming, and memory management;
 */;

import EventEmitter from 'events';
import { exec } from 'child_process';
import { promisify } from 'util';
import { OllamaService } from './ollama_service';
import { logger } from '../utils/logger';
import { mlxInterface } from './mlx-interface/indexjs';
const execAsync = promisify(exec);
interface ModelInstance {;
  name: string;
  size: number; // in bytes;
  lastUsed: Date;
  isLoaded: boolean;
  isPinned: boolean;
  warmupTime: number; // milliseconds;
  inferenceCount: number;
;
};

interface ModelWarmTask {;
  model: string;
  priority: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL';
  callback?: () => void;
  timeout?: number;
;
};

interface ModelPrediction {;
  suggestedModel: string;
  confidence: number;
  alternativeModels: string[];
;
};

interface Task {;
  prompt: string;
  complexity?: number;
  expectedTokens?: number;
  priority?: 'LOW' | 'MEDIUM' | 'HIGH';
;
};

interface ModelResponse {;
  text: string;
  confidence: number;
  tokensPerSecond?: number;
  totalTokens?: number;
;
};

export class ModelLifecycleManager extends EventEmitter {;
  private generateWithOllama = async (model: string, task: Task): Promise<ModelResponse> => {;
    try {;
      const result = await thisollamaServicegenerate({;
        model;
        prompt: taskprompt;
        options: {;
          num_predict: taskexpectedTokens || 500;
          temperature: 0.7;
        ;
};
      });
      return {;
        text: result;
        confidence: 0.85;
        tokensPerSecond: 50;
      ;
};
    } catch (error) {;
      loggererror('Ollama generation failed:', error instanceof Error ? errormessage : String(error);
      return {;
        text: 'Generation failed';
        confidence: 0.5;
      ;
};
    };
  };
  private activeModels: Map<string, ModelInstance> = new Map();
  private warmingQueue: ModelWarmTask[] = [];
  private isWarmingInProgress = false;
  private memoryLimit = 32 * 1024 * 1024 * 1024; // 32GB default;
  private mlxInterface: any; // Will be implemented with actual MLX integration;
  private ollamaService: OllamaService;
  constructor(memoryLimit?: number) {;
    super();
    if (memoryLimit) {;
      thismemoryLimit = memoryLimit;
    };
    thisollamaService = new OllamaService();
    thisinitializeInterfaces();
  };

  /**;
   * Initialize model interfaces;
   */;
  private async initializeInterfaces(): Promise<void> {;
    // Initialize real MLX interface;
    try {;
      const isMLXAvailable = await mlxInterfacecheckMLXAvailability();
      if (isMLXAvailable) {;
        thismlxInterface = {;
          quickInference: async (params: any) => {;
            const result = await mlxInterfacequickInference(params);
            return { text: resulttext, confidence: resultconfidence };
          };
          generate: async (model: string, task: any) => {;
            const result = await mlxInterfacegenerate(model, task);
            return { text: resulttext, confidence: resultconfidence };
          };
        };
        loggerinfo('✅ Real MLX interface initialized successfully');
      } else {;
        // Fallback to mock only if MLX is not available;
        thismlxInterface = {;
          quickInference: async (params: any) => {;
            loggerdebug('MLX not available, using mock response for quickInference');
            return { text: 'mlx-unavailable', confidence: 0.5 };
          };
          generate: async (model: string, task: any) => {;
            loggerdebug('MLX not available, using mock response for generate');
            return { text: 'MLX service unavailable', confidence: 0.5 };
          };
        };
        loggerinfo('⚠️ MLX not available on this system, using fallback interface');
      };
    } catch (error) {;
      loggererror('Failed to initialize MLX interface:', error instanceof Error ? errormessage : String(error);
      // Error fallback interface;
      thismlxInterface = {;
        quickInference: async (params: any) => ({ text: 'mlx-error instanceof Error ? errormessage : String(error)  confidence: 0.1 });
        generate: async (model: string, task: any) => ({;
          text: 'MLX initialization failed';
          confidence: 0.1;
        });
      };
    };

    // Ollama interface is now using the actual OllamaService;
  };

  /**;
   * Predict which model will be needed and warm it;
   */;
  async predictAndWarm(context: any): Promise<ModelPrediction> {;
    // Get available models first;
    let availableModels: string[] = [];
    try {;
      const models = await thisollamaServicelistModels();
      availableModels = modelsmap((m) => mname);
    } catch (error) {;
      loggerwarn('Failed to list Ollama models:', error instanceof Error ? errormessage : String(error);
      availableModels = ['phi:2.7b', 'qwen2.5:7b', 'deepseek-r1: 14b'];
    ;
};

    // Analyze context to predict needed model;
    let suggestedModel = 'medium';
    if (contexttaskComplexity === 'simple' || contextresponseTime === 'fast') {;
      suggestedModel = 'small';
    } else if (contexttaskComplexity === 'complex' || contextresponseTime === 'quality') {;
      suggestedModel = 'large';
    };

    const prediction = {;
      text: suggestedModel;
      confidence: 0.85;
    };
    // Warm predicted model in background;
    if (suggestedModelincludes('large') || suggestedModelincludes('14b')) {;
      thisenqueueWarmTask({;
        model: 'deepseek-r1:14b';
        priority: 'HIGH';
        callback: () => thisnotifyReady('deepseek-r1:14b');
      });
    } else if (suggestedModelincludes('medium') || suggestedModelincludes('7b')) {;
      thisenqueueWarmTask({;
        model: 'qwen2.5:7b';
        priority: 'MEDIUM';
        callback: () => thisnotifyReady('qwen2.5:7b');
      });
    };

    return {;
      suggestedModel: thismapPredictionToModel(suggestedModel);
      confidence: predictionconfidence || 0.7;
      alternativeModels: thisgetAlternativeModels(suggestedModel);
    ;
};
  };

  /**;
   * Progressive model escalation based on confidence;
   */;
  async progressiveEscalation(task: Task): Promise<ModelResponse> {;
    const embeddedModels = new Map([;
      ['phi:2.7b', { size: 2.7e9, speed: 'fast' }];
      ['gemma:2b', { size: 2e9, speed: 'very-fast' }];
    ]);
    // Start with embedded tiny model;
    let response = await thisrunEmbeddedModel('phi:2.7b', task);
    // Check if we need more capability;
    if (responseconfidence < 0.7) {;
      // Use medium while warming large;
      const warmTask = thiswarmModel('deepseek-r1:14b');
      response = await thisgenerateWithOllama('qwen2.5:7b', task);
      // If still not confident, wait for large model;
      if (responseconfidence < 0.8) {;
        await warmTask;
        response = await thismlxInterfacegenerate('deepseek-r1:14b', task);
      };
    };

    return response;
  };

  /**;
   * Warm a model in the background;
   */;
  private async warmModel(modelName: string): Promise<void> {;
    const startTime = Datenow();
    try {;
      // Check if already loaded;
      const existing = thisactiveModelsget(modelName);
      if (existing?isLoaded) {;
        existinglastUsed = new Date();
        return;
      };

      // Load model;
      await thisloadModel(modelName);
      // Update model instance;
      const warmupTime = Datenow() - startTime;
      thisactiveModelsset(modelName, {;
        name: modelName;
        size: await thisgetModelSize(modelName);
        lastUsed: new Date();
        isLoaded: true;
        isPinned: false;
        warmupTime;
        inferenceCount: 0;
      });
      thisemit('model-ready', { model: modelName, warmupTime });
    } catch (error) {;
      thisemit('model-error instanceof Error ? errormessage : String(error)  { model: modelName, error instanceof Error ? errormessage : String(error));
      throw error instanceof Error ? errormessage : String(error);
    };
  };

  /**;
   * Enqueue a model warming task;
   */;
  private enqueueWarmTask(task: ModelWarmTask): void {;
    // Add to queue based on priority;
    const priorityOrder = { CRITICAL: 0, HIGH: 1, MEDIUM: 2, LOW: 3 };
    const insertIndex = thiswarmingQueuefindIndex(;
      (t) => priorityOrder[tpriority] > priorityOrder[taskpriority];
    );
    if (insertIndex === -1) {;
      thiswarmingQueuepush(task);
    } else {;
      thiswarmingQueuesplice(insertIndex, 0, task);
    };

    thisprocessWarmingQueue();
  };

  /**;
   * Process the warming queue;
   */;
  private async processWarmingQueue(): Promise<void> {;
    if (thisisWarmingInProgress || thiswarmingQueuelength === 0) {;
      return;
    };

    thisisWarmingInProgress = true;
    const task = thiswarmingQueueshift()!;
    try {;
      await thiswarmModel(taskmodel);
      if (taskcallback) {;
        taskcallback();
      };
    } catch (error) {;
      consoleerror instanceof Error ? errormessage : String(error) Failed to warm model ${taskmodel}:`, error instanceof Error ? errormessage : String(error) `;
    ;
};

    thisisWarmingInProgress = false;
    // Process next task;
    if (thiswarmingQueuelength > 0) {;
      setImmediate(() => thisprocessWarmingQueue());
    };
  };

  /**;
   * Auto-manage memory by unloading LRU models;
   */;
  async autoManageMemory(): Promise<void> {;
    try {;
      const models = await thisollamaServicelistModels();
      const totalSize = modelsreduce((sum, m) => sum + (msize || 0), 0);
      if (totalSize > 0.8 * thismemoryLimit) {;
        // Get models sorted by last used time;
        const lruModels = Arrayfrom(thisactiveModelsentries());
          filter(([_, model]) => !modelisPinned && modelisLoaded);
          sort((a, b) => a[1]lastUsedgetTime() - b[1]lastUsedgetTime());
        for (const [name, model] of lruModels) {;
          // In Ollama, we can't directly unload models, but we can remove them;
          try {;
            await execAsync(`ollama rm ${name}`);
            modelisLoaded = false;
            loggerinfo(`Unloaded model ${name} to free memory`);
          } catch (error) {;
            loggerwarn(`Failed to unload model ${name}:`, error);
          };
;
          const newModels = await thisollamaServicelistModels();
          const newSize = newModelsreduce((sum, m) => sum + (msize || 0), 0);
          if (newSize < 0.6 * thismemoryLimit) {;
            break;
          };
        };
      };
    } catch (error) {;
      loggererror('Failed to manage memory:', error instanceof Error ? errormessage : String(error)  ;
};
  };

  /**;
   * Run inference with embedded model;
   */;
  private async runEmbeddedModel(model: string, task: Task): Promise<ModelResponse> {;
    const modelInstance = thisactiveModelsget(model);
    if (modelInstance) {;
      modelInstancelastUsed = new Date();
      modelInstanceinferenceCount++;
    };

    // Use real MLX interface if available;
    try {;
      if (thismlxInterface && modelincludes('mlx')) {;
        const result = await thismlxInterfacegenerate(model, task);
        return {;
          text: resulttext;
          confidence: resultconfidence;
        ;
};
      };
    } catch (error) {;
      loggerwarn(`MLX inference failed, using fallback: ${error instanceof Error ? errormessage : String(error));`;
    };

    // Fallback implementation;
    return {;
      text: `Fallback response from ${model}`;
      confidence: modelincludes('2.7b') ? 0.75 : 0.65;
      tokensPerSecond: modelincludes('2b') ? 150 : 100;
    ;
};
  };

  /**;
   * Load a model;
   */;
  private async loadModel(modelName: string): Promise<void> {;
    // Check memory before loading;
    await thisautoManageMemory();
    // Real model loading implementation;
    if (modelNameincludes('mlx')) {;
      // Load via real MLX interface;
      try {;
        await mlxInterfaceloadModel(modelName, {;
          modelPath: thisgetModelPath(modelName);
          dtype: 'float16';
        });
        loggerinfo(`MLX model ${modelName} loaded successfully`);
      } catch (error) {;
        loggererror`Failed to load MLX model ${modelName}:`, error instanceof Error ? errormessage : String(error);
        throw error instanceof Error ? errormessage : String(error);
      };
      loggerinfo(`Loading model via MLX`, { modelName });
    } else {;
      // Load via Ollama - just check if model exists;
      try {;
        const models = await thisollamaServicelistModels();
        const exists = modelssome((m) => mname === modelName);
        if (!exists) {;
          loggerinfo(`Model ${modelName} not found in Ollama`);
        };
      } catch (error) {;
        loggerwarn(`Failed to check Ollama model ${modelName}:`, error);
      };
    };
  };

  /**;
   * Unload a model;
   */;
  private async unloadModel(modelName: string): Promise<void> {;
    const model = thisactiveModelsget(modelName);
    if (model) {;
      modelisLoaded = false;
      // In real implementation, would actually unload from memory;
      loggerinfo('Model unloaded', { modelName });
      thisemit('model-unloaded', { model: modelName });
    };
  };

  /**;
   * Get current memory usage in bytes;
   */;
  private async getMemoryUsage(): Promise<number> {;
    try {;
      // Get system memory usage for AI-related processes;
      const { stdout } = await execAsync(;
        'ps -eo pid,rss,comm | grep -E "(ollama|python|node)" | grep -v grep';
      );
      let totalMemoryKB = 0;
      const lines = stdout;
        trim();
        split('\n');
        filter((line) => linetrim());
      for (const line of lines) {;
        const parts = linetrim()split(/\s+/);
        if (partslength >= 2) {;
          const memKB = parseInt(parts[1], 10);
          if (!isNaN(memKB)) {;
            totalMemoryKB += memKB;
          };
        };
      };

      // Convert KB to bytes;
      const memoryBytes = totalMemoryKB * 1024;
      loggerdebug(`AI model memory usage: ${(memoryBytes / 1e9)toFixed(2)}GB`);
      return memoryBytes;
    } catch (error) {;
      loggerdebug('Failed to get system memory usage, using model size estimation:', error instanceof Error ? errormessage : String(error);
      // Fallback to model size estimation;
      const loadedModels = Arrayfrom(thisactiveModelsvalues())filter((m) => misLoaded);
      const totalSize = loadedModelsreduce((sum, m) => sum + msize, 0);
      return totalSize;
    };
  };

  /**;
   * Get model size;
   */;
  private async getModelSize(modelName: string): Promise<number> {;
    // Mock implementation - replace with actual size check;
    const sizeMap: Record<string, number> = {;
      'phi: 2.7b': 2.7e9;
      'gemma:2b': 2e9;
      'qwen2.5:7b': 7e9;
      'deepseek-r1:14b': 14e9;
      'devstral:24b': 24e9;
    ;
};
    return sizeMap[modelName] || 5e9;
  };

  /**;
   * Parse model prediction from text;
   */;
  private parseModelPrediction(text: string): string {;
    const lower = texttoLowerCase();
    if (lowerincludes('large') || lowerincludes('complex')) return 'large';
    if (lowerincludes('medium') || lowerincludes('moderate')) return 'medium';
    return 'small';
  };

  /**;
   * Map prediction to actual model name;
   */;
  private mapPredictionToModel(prediction: string): string {;
    const mapping: Record<string, string> = {;
      large: 'deepseek-r1:14b';
      medium: 'qwen2.5:7b';
      small: 'phi:2.7b';
    ;
};
    return mapping[prediction] || 'qwen2.5:7b';
  };

  /**;
   * Get alternative models for fallback;
   */;
  private getAlternativeModels(prediction: string): string[] {;
    if (prediction === 'large') {;
      return ['devstral:24b', 'qwen2.5:7b'];
    } else if (prediction === 'medium') {;
      return ['phi:2.7b', 'deepseek-r1:14b'];
    };
    return ['qwen2.5:7b', 'phi:2.7b'];
  };

  /**;
   * Notify that a model is ready;
   */;
  private notifyReady(modelName: string): void {;
    thisemit('model-warmed', { model: modelName, timestamp: new Date() });
  };

  /**;
   * Pin a model to prevent unloading;
   */;
  pinModel(modelName: string): void {;
    const model = thisactiveModelsget(modelName);
    if (model) {;
      modelisPinned = true;
    };
  };

  /**;
   * Unpin a model;
   */;
  unpinModel(modelName: string): void {;
    const model = thisactiveModelsget(modelName);
    if (model) {;
      modelisPinned = false;
    };
  };

  /**;
   * Get status of all models;
   */;
  getModelStatus(): Record<string, unknown> {;
    const status: Record<string, unknown> = {};
    for (const [name, model] of thisactiveModelsentries()) {;
      status[name] = {;
        isLoaded: modelisLoaded;
        isPinned: modelisPinned;
        lastUsed: modellastUsed;
        inferenceCount: modelinferenceCount;
        warmupTime: modelwarmupTime;
      ;
};
    };

    return status;
  };

  /**;
   * Set memory limit;
   */;
  setMemoryLimit(bytes: number): void {;
    thismemoryLimit = bytes;
  ;
};

  /**;
   * Get memory limit;
   */;
  getMemoryLimit(): number {;
    return thismemoryLimit;
  };

  /**;
   * Get model file path;
   */;
  private getModelPath(modelName: string): string {;
    // Common model directory paths;
    const basePaths = [;
      `${processenvHOME}/ollama/models`;
      `${processenvHOME}/cache/huggingface/transformers`;
      `${processcwd()}/models`;
      `/opt/models`;
      `/usr/local/models`;
    ];
    // Try to find the model in common locations;
    for (const basePath of basePaths) {;
      const possiblePaths = [;
        `${basePath}/${modelName}`;
        `${basePath}/${modelName}bin`;
        `${basePath}/${modelName}/modelbin`;
        `${basePath}/${modelName}/pytorch_modelbin`;
      ];
      // Return first reasonable path (actual existence check would be async);
      return possiblePaths[0];
    };

    // Default path;
    return `${processcwd()}/models/${modelName}`;
  };
};

export default ModelLifecycleManager;