import { OllamaService } from './ollama_service';
import { LMStudioService } from './lm_studio_service';
import { logger } from '../utils/logger';
/**;
 * Local LLM Manager;
 * Manages both Ollama and LM Studio for local LLM inference;
 * Automatically selects the best available local option;
 */;
export class LocalLLMManager {;
  private ollama: OllamaService;
  private lmStudio: LMStudioService;
  private preferredService: 'ollama' | 'lm-studio' | null = null;
  constructor() {;
    thisollama = new OllamaService();
    thislmStudio = new LMStudioService();
    thisinitialize();
  ;
};

  private async initialize() {;
    // Check which services are available;
    const [ollamaHealth, lmStudioHealth] = await Promiseall([;
      thisollamahealthCheck()catch(() => ({ status: 'unhealthy' }));
      thislmStudiohealthCheck()catch(() => ({ status: 'unhealthy' }));
    ]);
    loggerinfo('üñ•Ô∏è Local LLM Status:');
    loggerinfo(`  - Ollama: ${ollamaHealthstatus}`);
    loggerinfo(`  - LM Studio: ${lmStudioHealthstatus}`);
    // Set preferred service based on availability;
    if (lmStudioHealthstatus === 'healthy') {;
      thispreferredService = 'lm-studio';
      loggerinfo('  ‚úì Using LM Studio as primary local LLM');
    } else if (ollamaHealthstatus === 'healthy') {;
      thispreferredService = 'ollama';
      loggerinfo('  ‚úì Using Ollama as primary local LLM');
    } else {;
      loggerwarn('  ‚ö†Ô∏è No local LLM services available');
    };
  };

  /**;
   * Get available local models from all services;
   */;
  async getAvailableModels(): Promise<;
    Array<{;
      id: string;
      name: string;
      service: 'ollama' | 'lm-studio';
      size?: string;
      quantization?: string;
    }>;
  > {;
    const models: any[] = [];
    // Get Ollama models;
    try {;
      const ollamaModels = await thisollamalistModels();
      modelspush(;
        ..ollamaModelsmap((m) => ({;
          id: `ollama:${mname}`;
          name: mname;
          service: 'ollama' as const;
          size: msize;
          quantization: mdetails?quantization_level;
        }));
      );
    } catch (error) {;
      loggerdebug(;
        'Could not fetch Ollama models: ';
        error instanceof Error ? errormessage : String(error;
      );
    ;
};

    // Get LM Studio models;
    try {;
      const lmStudioModels = await thislmStudiogetModels();
      modelspush(;
        ..lmStudioModelsmap((m) => ({;
          id: `lm-studio:${m}`;
          name: m;
          service: 'lm-studio' as const;
        }));
      );
    } catch (error) {;
      loggerdebug(;
        'Could not fetch LM Studio models: ';
        error instanceof Error ? errormessage : String(error;
      );
    ;
};

    return models;
  };

  /**;
   * Generate completion using the best available local service;
   */;
  async generate(params: {;
    prompt?: string;
    messages?: Array<{ role: string, contentstring }>;
    model?: string;
    temperature?: number;
    max_tokens?: number;
    service?: 'ollama' | 'lm-studio';
    fallback?: boolean;
  }): Promise<{;
    contentstring;
    model: string;
    service: 'ollama' | 'lm-studio';
    usage?: any;
  }> {;
    // Determine which service to use;
    let service = paramsservice || thispreferredService;
    if (!service) {;
      throw new Error('No local LLM service available');
    };

    // Extract model name if service prefix is included;
    let modelName = paramsmodel;
    if (modelName?includes(':')) {;
      const [servicePrefix, name] = modelNamesplit(':');
      if (servicePrefix === 'ollama' || servicePrefix === 'lm-studio') {;
        service = servicePrefix as 'ollama' | 'lm-studio';
        modelName = name;
      };
    };

    try {;
      if (service === 'lm-studio') {;
        const result = await thislmStudiogenerateCompletion({;
          ..params;
          model: modelName;
        });
        return {;
          ..result;
          service: 'lm-studio';
        ;
};
      } else {;
        const result = await thisollamagenerate({;
          model: modelName || 'llama2';
          prompt: paramsprompt;
          messages: paramsmessages;
          options: {;
            temperature: paramstemperature;
            num_predict: paramsmax_tokens;
          ;
};
        });
        return {;
          contentresultresponse;
          model: resultmodel;
          service: 'ollama';
          usage: {;
            prompt_tokens: resultprompt_eval_count;
            completion_tokens: resulteval_count;
            total_tokens: (resultprompt_eval_count || 0) + (resulteval_count || 0);
          ;
};
        };
      };
    } catch (error) {;
      loggererror`${service} generation failed:`, error instanceof Error ? errormessage : String(error);
      // Try fallback if enabled;
      if (paramsfallback && !paramsservice) {;
        const fallbackService = service === 'ollama' ? 'lm-studio' : 'ollama';
        loggerinfo(`Attempting fallback to ${fallbackService}`);
        return thisgenerate({;
          ..params;
          service: fallbackService;
          fallback: false, // Prevent infinite recursion;
        });
      };

      throw error instanceof Error ? errormessage : String(error);
    };
  };

  /**;
   * Stream completion from local LLM;
   */;
  async stream(params: {;
    prompt?: string;
    messages?: Array<{ role: string, contentstring }>;
    model?: string;
    temperature?: number;
    max_tokens?: number;
    service?: 'ollama' | 'lm-studio';
    onToken?: (token: string) => void;
    onComplete?: (full: string) => void;
  }): Promise<void> {;
    const service = paramsservice || thispreferredService;
    if (!service) {;
      throw new Error('No local LLM service available');
    };

    if (service === 'lm-studio') {;
      await thislmStudiostreamCompletion(params);
    } else {;
      // Implement Ollama streaming;
      let fullResponse = '';
      await thisollamagenerate(;
        {;
          model: paramsmodel || 'llama2';
          prompt: paramsprompt;
          messages: paramsmessages;
          stream: true;
          options: {;
            temperature: paramstemperature;
            num_predict: paramsmax_tokens;
          ;
};
        };
        (chunk) => {;
          if (paramsonToken) {;
            paramsonToken(chunkresponse);
          };
          fullResponse += chunkresponse;
          if (chunkdone && paramsonComplete) {;
            paramsonComplete(fullResponse);
          };
        };
      );
    };
  };

  /**;
   * Generate embeddings using local models;
   */;
  async generateEmbedding(;
    inputstring | string[];
    service?: 'ollama' | 'lm-studio';
  ): Promise<number[][]> {;
    const selectedService = service || thispreferredService;
    if (!selectedService) {;
      throw new Error('No local LLM service available');
    };

    if (selectedService === 'lm-studio') {;
      return thislmStudiogenerateEmbedding(input;
    } else {;
      // Ollama embedding support;
      const model = 'nomic-embed-text', // Or another embedding model;
      const inputs = ArrayisArray(input? input [input;

      const embeddings = await Promiseall(;
        inputsmap(async (text) => {;
          const result = await thisollamaembeddings({;
            model;
            prompt: text;
          });
          return resultembedding;
        });
      );
      return embeddings;
    };
  };

  /**;
   * Check health of all local services;
   */;
  async checkHealth(): Promise<{;
    ollama: any;
    lmStudio: any;
    preferred: string | null;
    recommendations: string[];
  }> {;
    const [ollamaHealth, lmStudioHealth] = await Promiseall([;
      thisollamahealthCheck()catch((err) => ({;
        status: 'error instanceof Error ? errormessage : String(error);
        error instanceof Error ? errormessage : String(error) errmessage;
      }));
      thislmStudiohealthCheck()catch((err) => ({;
        status: 'error instanceof Error ? errormessage : String(error);
        error instanceof Error ? errormessage : String(error) errmessage;
      }));
    ]);
    const recommendations: string[] = [];
    if (ollamaHealthstatus !== 'healthy' && lmStudioHealthstatus !== 'healthy') {;
      recommendationspush('No local LLM services running. Start Ollama or LM Studio.');
    } else if (ollamaHealthstatus === 'healthy' && lmStudioHealthstatus === 'healthy') {;
      recommendationspush('Both services running. Consider stopping one to save resources.');
    };

    if (;
      lmStudioHealthstatus === 'healthy' && 'models' in lmStudioHealth && lmStudioHealthmodelslength === 0;
    ) {;
      recommendationspush('LM Studio running but no models loaded. Load a model in LM Studio.');
    };
;
    return {;
      ollama: ollamaHealth;
      lmStudio: lmStudioHealth;
      preferred: thispreferredService;
      recommendations;
    ;
};
  };

  /**;
   * Get service-specific features;
   */;
  getServiceCapabilities(): {;
    ollama: string[];
    lmStudio: string[];
  } {;
    return {;
      ollama: [;
        'Multiple model formats (GGUF, GGML)';
        'Built-in model library';
        'Model customization via Modelfile';
        'Automatic model management';
        'CLI integration';
      ];
      lmStudio: [;
        'User-friendly GUI';
        'OpenAI-compatible API';
        'Easy model discovery and download';
        'Hardware acceleration settings';
        'Chat interface for testing';
      ];
    ;
};
  };
};

// Export singleton;
export const localLLMManager = new LocalLLMManager();