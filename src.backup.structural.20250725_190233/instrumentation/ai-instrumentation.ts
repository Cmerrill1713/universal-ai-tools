import type { Span } from '@opentelemetry/api';
import { SpanKind, SpanStatusCode, context, trace } from '@opentelemetry/api';
import { telemetryService } from '../services/telemetry-service';
import { logger } from '../utils/logger';
interface AIOperation {;
  service: string;
  model: string;
  operation: string;
  prompt?: string;
  maxTokens?: number;
  temperature?: number;
  stream?: boolean;
;
};

interface AIResponse {;
  content string;
  tokens?: {;
    prompt: number;
    completion: number;
    total: number;
  ;
};
  cost?: {;
    prompt: number;
    completion: number;
    total: number;
  ;
};
  finishReason?: string;
  error instanceof Error ? errormessage : String(error)  Error;
;
};

export class AIInstrumentation {;
  private tracer = telemetryServicegetTracer();
  /**;
   * Wrap an AI service call with tracing;
   */;
  async withAISpan<T>(operation: AIOperation, fn: () => Promise<T>): Promise<T> {;
    const spanName = `ai.${operationservice}.${operationoperation}`;
    const span = thistracerstartSpan(spanName, {;
      kind: SpanKindCLIENT;
      attributes: {;
        'aiservice': operationservice;
        'aimodel': operationmodel;
        'aioperation': operationoperation;
        'airequestmax_tokens': operationmaxTokens;
        'airequesttemperature': operationtemperature;
        'airequeststream': operationstream || false;
        'airequestprompt_preview': operationprompt?substring(0, 100);
        'airequestprompt_length': operationprompt?length || 0;
      ;
};
    });
    const startTime = Datenow();
    try {;
      const result = await contextwith(tracesetSpan(contextactive(), span), fn);
      // Add response metrics;
      const duration = Datenow() - startTime;
      spansetAttribute('aiduration_ms', duration);
      // Extract and record AI-specific metrics from result;
      if (result && typeof result === 'object') {;
        thisrecordAIResponse(span, result as any);
      };

      spansetStatus({ code: SpanStatusCodeOK });
      return result;
    } catch (error) {;
      spanrecordException(erroras Error);
      spansetStatus({;
        code: SpanStatusCodeERROR;
        message: error instanceof Error ? errormessage : 'AI operation failed';
      });
      // Record errordetails;
      if (error instanceof Error) {;
        spansetAttribute('errortype', errorname);
        spansetAttribute('errormessage', errormessage);
        // Check for specific AI service errors;
        if (errormessageincludes('rate limit')) {;
          spansetAttribute('aierrortype', 'rate_limit');
        } else if (errormessageincludes('context length')) {;
          spansetAttribute('aierrortype', 'context_length_exceeded');
        } else if (errormessageincludes('timeout')) {;
          spansetAttribute('aierrortype', 'timeout');
        } else if (errormessageincludes('authentication')) {;
          spansetAttribute('aierrortype', 'authentication');
        };
      };

      loggererror('AI operation failed', {;
        service: operationservice;
        model: operationmodel;
        operation: operationoperation;
        error;
        duration: Datenow() - startTime;
      });
      throw error instanceof Error ? errormessage : String(error);
    } finally {;
      spanend();
    };
  };

  /**;
   * Record AI response metrics;
   */;
  private recordAIResponse(span: Span, response: any): void {;
    // Token usage;
    if (responseusage) {;
      spansetAttribute('airesponsetokensprompt', responseusageprompt_tokens || 0);
      spansetAttribute('airesponsetokenscompletion', responseusagecompletion_tokens || 0);
      spansetAttribute('airesponsetokenstotal', responseusagetotal_tokens || 0);
    };

    // Response content;
    if (responsechoices?.[0]) {;
      const choice = responsechoices[0];
      if (choicemessage?content{;
        spansetAttribute('airesponsecontent_length', choicemessagecontent-length);
        spansetAttribute('airesponsecontent_preview', choicemessagecontentsubstring(0, 100));
      };
      if (choicefinish_reason) {;
        spansetAttribute('airesponsefinish_reason', choicefinish_reason);
      };
    };

    // Model-specific attributes;
    if (responsemodel) {;
      spansetAttribute('airesponsemodel', responsemodel);
    };
    if (responseid) {;
      spansetAttribute('airesponseid', responseid);
    };
  };

  /**;
   * Instrument OpenAI client;
   */;
  instrumentOpenAI(client: any): any {;
    const instrumented = Objectcreate(client);
    const instrumentation = this;
    // Instrument chat completions;
    if (clientchat?completions) {;
      instrumentedchat = {;
        completions: {;
          create: thiswrapAIMethod(;
            clientchatcompletionscreatebind(clientchatcompletions);
            'openai';
            'chatcompletion';
          );
          createStream: thiswrapStreamingAIMethod(;
            clientchatcompletionscreatebind(clientchatcompletions);
            'openai';
            'chatcompletionstream';
          );
        ;
};
      };
    };

    // Instrument completions (legacy);
    if (clientcompletions) {;
      instrumentedcompletions = {;
        create: thiswrapAIMethod(;
          clientcompletionscreatebind(clientcompletions);
          'openai';
          'completion';
        );
      ;
};
    };

    // Instrument embeddings;
    if (clientembeddings) {;
      instrumentedembeddings = {;
        create: thiswrapAIMethod(;
          clientembeddingscreatebind(clientembeddings);
          'openai';
          'embedding';
        );
      ;
};
    };
;
    return instrumented;
  };

  /**;
   * Instrument Anthropic Claude client;
   */;
  instrumentAnthropic(client: any): any {;
    const instrumented = Objectcreate(client);
    // Instrument messages;
    if (clientmessages) {;
      instrumentedmessages = {;
        create: thiswrapAIMethod(;
          clientmessagescreatebind(clientmessages);
          'anthropic';
          'message';
        );
        stream: thiswrapStreamingAIMethod(;
          clientmessagesstreambind(clientmessages);
          'anthropic';
          'messagestream';
        );
      ;
};
    };

    // Instrument completions (legacy);
    if (clientcompletions) {;
      instrumentedcompletions = {;
        create: thiswrapAIMethod(;
          clientcompletionscreatebind(clientcompletions);
          'anthropic';
          'completion';
        );
      ;
};
    };
;
    return instrumented;
  };

  /**;
   * Wrap an AI method with tracing;
   */;
  private wrapAIMethod(method: Function, service: string, operationType: string): Function {;
    const instrumentation = this;
    return async function (params: any) {;
      const operation: AIOperation = {;
        service;
        model: paramsmodel || 'unknown';
        operation: operationType;
        prompt: instrumentationextractPrompt(params);
        maxTokens: paramsmax_tokens || paramsmaxTokens;
        temperature: paramstemperature;
        stream: paramsstream || false;
      ;
};
      return instrumentationwithAISpan(operation, async () => {;
        const startTime = Datenow();
        const result = await method(params);
        // Calculate cost if possible;
        const span = tracegetActiveSpan();
        if (span && resultusage) {;
          const cost = instrumentationcalculateCost(service, paramsmodel, resultusage);
          if (cost) {;
            spansetAttribute('aicostprompt_usd', costprompt);
            spansetAttribute('aicostcompletion_usd', costcompletion);
            spansetAttribute('aicosttotal_usd', costtotal);
          };
        };

        return result;
      });
    };
  };

  /**;
   * Wrap a streaming AI method with tracing;
   */;
  private wrapStreamingAIMethod(;
    method: Function;
    service: string;
    operationType: string;
  ): Function {;
    const instrumentation = this;
    return async function* (params: any) {;
      const operation: AIOperation = {;
        service;
        model: paramsmodel || 'unknown';
        operation: operationType;
        prompt: instrumentationextractPrompt(params);
        maxTokens: paramsmax_tokens || paramsmaxTokens;
        temperature: paramstemperature;
        stream: true;
      ;
};
      const span = instrumentationtracerstartSpan(`ai.${service}.${operationType}`, {;
        kind: SpanKindCLIENT;
        attributes: {;
          'aiservice': service;
          'aimodel': operationmodel;
          'aioperation': operationType;
          'airequeststream': true;
          'airequestmax_tokens': operationmaxTokens;
          'airequesttemperature': operationtemperature;
          'airequestprompt_length': operationprompt?length || 0;
        ;
};
      });
      const startTime = Datenow();
      let totalTokens = 0;
      let content '';
      try {;
        const stream = await method({ ..params, stream: true });
        for await (const chunk of stream) {;
          // Track streaming progress;
          if (chunkchoices?.[0]?delta?content{;
            content= chunkchoices[0]deltacontent;
          };
          if (chunkusage) {;
            totalTokens = chunkusagetotal_tokens || totalTokens;
          };

          yield chunk;
        };

        // Record final metrics;
        spansetAttribute('airesponsecontent_length', content-length);
        spansetAttribute('airesponsetokenstotal', totalTokens);
        spansetAttribute('aiduration_ms', Datenow() - startTime);
        spansetStatus({ code: SpanStatusCodeOK });
      } catch (error) {;
        spanrecordException(erroras Error);
        spansetStatus({;
          code: SpanStatusCodeERROR;
          message: error instanceof Error ? errormessage : 'Streaming failed';
        });
        throw error instanceof Error ? errormessage : String(error);
      } finally {;
        spanend();
      };
    };
  };

  /**;
   * Extract prompt from AI parameters;
   */;
  private extractPrompt(params: any): string | undefined {;
    // OpenAI style;
    if (paramsmessages && ArrayisArray(paramsmessages)) {;
      return paramsmessagesmap((m: any) => `${mrole}: ${mcontent)join('\n');`;
    };

    // Anthropic style;
    if (paramsprompt) {;
      return paramsprompt;
    };

    // Direct prompt;
    if (paramsinput& typeof paramsinput== 'string') {;
      return params._input;
    };

    return undefined;
  };

  /**;
   * Calculate cost based on token usage;
   */;
  private calculateCost(;
    service: string;
    model: string;
    usage: { prompt_tokens?: number; completion_tokens?: number ;
};
  ): { prompt: number; completion: number; total: number } | null {;
    // Pricing per 1K tokens (example rates, should be configurable);
    const pricing: Record<string, Record<string, { prompt: number; completion: number }>> = {;
      openai: {;
        'gpt-4': { prompt: 0.03, completion: 0.06 ;
};
        'gpt-4-turbo': { prompt: 0.01, completion: 0.03 ;
};
        'gpt-3.5-turbo': { prompt: 0.0005, completion: 0.0015 ;
};
      };
      anthropic: {;
        'claude-3-opus': { prompt: 0.015, completion: 0.075 ;
};
        'claude-3-sonnet': { prompt: 0.003, completion: 0.015 ;
};
        'claude-3-haiku': { prompt: 0.00025, completion: 0.00125 ;
};
      };
    };
    const modelPricing = pricing[service]?.[model];
    if (!modelPricing || !usageprompt_tokens || !usagecompletion_tokens) {;
      return null;
    };

    const promptCost = (usageprompt_tokens / 1000) * modelPricingprompt;
    const completionCost = (usagecompletion_tokens / 1000) * modelPricingcompletion;
    return {;
      prompt: promptCost;
      completion: completionCost;
      total: promptCost + completionCost;
    ;
};
  };

  /**;
   * Create a traced AI function;
   */;
  createTracedAIFunction<T extends (..args: any[]) => Promise<unknown>>(;
    fn: T;
    service: string;
    operation: string;
    modelExtractor: (..args: Parameters<T>) => string;
  ): T {;
    const instrumentation = this;
    return async function (..args: Parameters<T>): Promise<ReturnType<T>> {;
      const aiOperation: AIOperation = {;
        service;
        model: modelExtractor(..args);
        operation;
      ;
};
      return instrumentationwithAISpan(aiOperation, () => fn(..args));
    } as T;
  };

  /**;
   * Monitor AI service health;
   */;
  async monitorAIServiceHealth(;
    service: string;
    healthCheckFn: () => Promise<boolean>;
  ): Promise<void> {;
    const span = thistracerstartSpan(`ai.${service}health_check`, {;
      kind: SpanKindCLIENT;
      attributes: {;
        'aiservice': service;
        'aioperation': 'health_check';
      ;
};
    });
    const startTime = Datenow();
    try {;
      const isHealthy = await healthCheckFn();
      spansetAttribute('aihealthstatus', isHealthy ? 'healthy' : 'unhealthy');
      spansetAttribute('aihealthduration_ms', Datenow() - startTime);
      spansetStatus({ code: SpanStatusCodeOK });
    } catch (error) {;
      spanrecordException(erroras Error);
      spansetStatus({;
        code: SpanStatusCodeERROR;
        message: 'Health check failed';
      });
      spansetAttribute('aihealthstatus', 'error instanceof Error ? errormessage : String(error);
    } finally {;
      spanend();
    };
  };
};

// Export singleton instance;
export const aiInstrumentation = new AIInstrumentation();
// Export convenience functions;
export const withAISpan = <T>(operation: AIOperation, fn: () => Promise<T>) =>;
  aiInstrumentationwithAISpan(operation, fn);
export const instrumentOpenAI = (client: any) => aiInstrumentationinstrumentOpenAI(client);
export const instrumentAnthropic = (client: any) => aiInstrumentationinstrumentAnthropic(client);
export const createTracedAIFunction = <T extends (..args: any[]) => Promise<unknown>>(;
  fn: T;
  service: string;
  operation: string;
  modelExtractor: (..args: Parameters<T>) => string;
) => aiInstrumentationcreateTracedAIFunction(fn, service, operation, modelExtractor);