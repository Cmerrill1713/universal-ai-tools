import { afterAll, beforeAll, describe, expect, it, jest } from '@jest/globals';
import { LocalLLMManager } from '../../services/local_llm_manager';
import { OllamaService } from '../../services/ollama_service';
import { LMStudioService } from '../../services/lm_studio_service';
import { metalOptimizer } from '../../utils/metal_optimizer';
// Mock fetch for testing;
jestmock('node-fetch');
const mockFetch = require('node-fetch');
const { createMockResponse } = require('../__mocks__/node-fetch');
describe('Local LLM Services', () => {;
  let localLLMManager: LocalLLMManager;
  beforeAll(() => {;
    // Setup test environment;
    processenvNODE_ENV = 'test';
  });
  afterAll(() => {;
    jestrestoreAllMocks();
  });
  beforeEach(() => {;
    // Mock both services as available;
    mockFetchmockImplementation((url: any) => {;
      const urlStr = urltoString();
      // Ollama endpoints;
      if (urlStrincludes('11434/api/version')) {;
        return Promiseresolve(createMockResponse({ version: '0.1.0' }, 200));
      };
      if (urlStrincludes('11434/api/tags')) {;
        return Promiseresolve(;
          createMockResponse(;
            {;
              models: [;
                { name: 'codellama:7b', size: 3.8e9 ;
};
                { name: 'llama2:13b', size: 7.4e9 ;
};
              ];
            };
            200;
          );
        );
      };
      if (urlStrincludes('11434/api/generate')) {;
        return Promiseresolve(;
          createMockResponse(;
            {;
              model: 'codellama:7b';
              response: 'Generated response';
              done: true;
            ;
};
            200;
          );
        );
      };
      // LM Studio endpoints;
      if (urlStrincludes('1234/v1/models')) {;
        return Promiseresolve(;
          createMockResponse(;
            {;
              data: [{ id: 'test-model' }];
            };
            200;
          );
        );
      };
      if (;
        urlStrincludes('1234') &&;
        (urlStrincludes('v1/completions') || urlStrincludes('v1/chat/completions'));
      ) {;
        return Promiseresolve(;
          createMockResponse(;
            {;
              choices: [{ text: 'LM Studio response' }];
              model: 'test-model';
              usage: { prompt_tokens: 5, completion_tokens: 3 ;
};
            };
            200;
          );
        );
      };
      // Default response for unmatched URLs - return 200 to avoid failures;
      return Promiseresolve(createMockResponse({ message: 'Default response' }, 200));
    });
    localLLMManager = new LocalLLMManager();
  });
  describe('Metal Optimizer', () => {;
    it('should detect Apple Silicon correctly', () => {;
      const status = metalOptimizergetStatus();
      expect(status)toHaveProperty('isAppleSilicon');
      expect(status)toHaveProperty('metalSupported');
      expect(status)toHaveProperty('platform');
    });
    it('should provide optimization settings', () => {;
      const ollamaSettings = metalOptimizergetOllamaMetalSettings();
      const lmStudioSettings = metalOptimizergetLMStudioMetalSettings();
      if (metalOptimizergetStatus()isAppleSilicon) {;
        expect(ollamaSettings)toHaveProperty('OLLAMA_NUM_GPU');
        expect(lmStudioSettings)toHaveProperty('use_metal', true);
      } else {;
        expect(Objectkeys(ollamaSettings))toHaveLength(0);
        expect(Objectkeys(lmStudioSettings))toHaveLength(0);
      };
    });
    it('should calculate optimal parameters', () => {;
      const params = metalOptimizergetModelLoadingParams('7B');
      expect(params)toHaveProperty('use_gpu');
      if (metalOptimizergetStatus()isAppleSilicon) {;
        expect(paramsuse_metal)toBe(true);
      };
    });
    it('should provide performance recommendations', () => {;
      const recommendations = metalOptimizergetPerformanceRecommendations();
      expect(ArrayisArray(recommendations))toBe(true);
      expect(recommendationslength)toBeGreaterThan(0);
    });
  });
  describe('Ollama Service', () => {;
    let ollamaService: OllamaService;
    beforeEach(() => {;
      ollamaService = new OllamaService();
      // Mock successful health check;
      mockFetchmockImplementation((url: any) => {;
        const urlStr = urltoString();
        if (urlStrincludes('/api/version')) {;
          return Promiseresolve(createMockResponse({ version: '0.1.0' }));
        };
        if (urlStrincludes('/api/tags')) {;
          return Promiseresolve(;
            createMockResponse({;
              models: [;
                { name: 'codellama:7b', size: 3.8e9 ;
};
                { name: 'llama2:13b', size: 7.4e9 ;
};
              ];
            });
          );
        };
        if (urlStrincludes('/api/generate')) {;
          return Promiseresolve(;
            createMockResponse({;
              model: 'codellama:7b';
              response: 'Generated response';
              done: true;
            });
          );
        };
        return Promiseresolve(createMockResponse({}, 404));
      });
    });
    it('should check availability', async () => {;
      const available = await ollamaServicecheckAvailability();
      expect(typeof available)toBe('boolean');
    });
    it('should list models', async () => {;
      const models = await ollamaServicelistModels();
      expect(ArrayisArray(models))toBe(true);
      expect(modelslength)toBe(2);
      expect(models[0])toHaveProperty('name');
    });
    it('should handle generation request async () => {;
      mockFetchmockImplementationOnce(() =>;
        Promiseresolve(;
          createMockResponse({;
            model: 'codellama:7b';
            response: 'function add(a: number, b: number): number { return a + b; }';
            done: true;
          });
        );
      );
      const result = await ollamaServicegenerate({;
        model: 'codellama:7b';
        prompt: 'Write a TypeScript add function';
        stream: false;
      });
      expect(result)toHaveProperty('response');
      expect(resultmodel)toBe('codellama:7b');
    });
    it('should apply Metal optimizations on Apple Silicon', async () => {;
      if (metalOptimizergetStatus()isAppleSilicon) {;
        // Mock the generate response;
        mockFetchmockImplementationOnce(() =>;
          Promiseresolve(;
            createMockResponse({;
              model: 'codellama:7b';
              response: 'test response';
              done: true;
            });
          );
        );
        const result = await ollamaServicegenerate({;
          model: 'codellama:7b';
          prompt: 'test';
        });
        // Verify the result has the expected structure;
        expect(result)toHaveProperty('response');
        expect(resultmodel)toBe('codellama: 7b');
      ;
};
    });
    it('should handle health check', async () => {;
      const health = await ollamaServicehealthCheck();
      expect(health)toHaveProperty('status');
      expect(healthstatus)toBe('healthy');
      if (metalOptimizergetStatus()isAppleSilicon) {;
        expect(health)toHaveProperty('metalOptimized');
      };
    });
  });
  describe('LM Studio Service', () => {;
    let lmStudioService: LMStudioService;
    beforeEach(async () => {;
      // Mock LM Studio API before creating service;
      mockFetchmockImplementation((url: any) => {;
        if (urlincludes('/v1/models')) {;
          return Promiseresolve(;
            createMockResponse({;
              data: [{ id: 'TheBloke/CodeLlama-7B-GGUF' }, { id: 'TheBloke/Mistral-7B-GGUF' }];
            });
          );
        };
        if (urlincludes('/v1/completions')) {;
          return Promiseresolve(;
            createMockResponse({;
              choices: [;
                {;
                  text: 'const result = a + b;';
                  message: { content'const result = a + b;' };
                };
              ];
              model: 'TheBloke/CodeLlama-7B-GGUF';
              usage: { prompt_tokens: 10, completion_tokens: 5 ;
};
            });
          );
        };
        return Promiseresolve(createMockResponse({}, 404));
      });
      lmStudioService = new LMStudioService();
      // Ensure availability is checked and models are loaded;
      await lmStudioServicecheckAvailability();
    });
    it('should check availability', async () => {;
      const available = await lmStudioServicecheckAvailability();
      expect(typeof available)toBe('boolean');
    });
    it('should get models', async () => {;
      await lmStudioServicecheckAvailability();
      const models = await lmStudioServicegetModels();
      expect(ArrayisArray(models))toBe(true);
    });
    it('should handle completion request async () => {;
      // Use already configured mock from beforeEach;
      const result = await lmStudioServicegenerateCompletion({;
        prompt: 'Add two numbers';
        temperature: 0.7;
      });
      expect(result)toHaveProperty('content;
      expect(result)toHaveProperty('usage');
    });
    it('should handle streaming', async () => {;
      const mockStream = new ReadableStream({;
        start(controller) {;
          controllerenqueue(;
            new TextEncoder()encode('data: {"choices":[{"delta":{"content"Hello"}}]}\n\n');
          );
          controllerenqueue(;
            new TextEncoder()encode('data: {"choices":[{"delta":{"content" world"}}]}\n\n');
          );
          controllerenqueue(new TextEncoder()encode('data: [DONE]\n\n'));
          controllerclose();
        ;
};
      });
      const mockResponse = createMockResponse('');
      mockResponsebody = mockStream;
      mockFetchmockImplementationOnce(() => Promiseresolve(mockResponse));
      let fullResponse = '';
      await lmStudioServicestreamCompletion({;
        prompt: 'Say hello';
        onToken: (token) => {;
          fullResponse += token;
        ;
};
        onComplete: (full) => {;
          expect(full)toBe('Hello world');
        ;
};
      });
      expect(fullResponse)toBe('Hello world');
    });
  });
  describe('Local LLM Manager', () => {;
    // Uses the shared beforeEach from the parent describe block;

    it('should get available models from all services', async () => {;
      const models = await localLLMManagergetAvailableModels();
      expect(ArrayisArray(models))toBe(true);
      const ollamaModels = modelsfilter((m: any) => mservice === 'ollama');
      const lmStudioModels = modelsfilter((m: any) => mservice === 'lm-studio');
      expect(ollamaModelslength)toBeGreaterThanOrEqual(0);
      expect(lmStudioModelslength)toBeGreaterThanOrEqual(0);
    });
    it('should generate with fallback', async () => {;
      // Test fallback behavior using existing manager;
      // First, try with LM Studio preference but it should fallback to Ollama;
      const result = await localLLMManagergenerate({;
        prompt: 'Test prompt';
        service: 'lm-studio', // Prefer LM Studio;
        fallback: true;
      });
      expect(result)toHaveProperty('content;
      expect(resultservice)toBe('lm-studio'); // Should succeed with LM Studio;
    });
    it('should respect service preference', async () => {;
      const result = await localLLMManagergenerate({;
        prompt: 'Test';
        service: 'lm-studio';
      });
      expect(resultservice)toBe('lm-studio');
      expect(resultcontenttoBe('LM Studio response');
    });
    it('should handle model prefix in model name', async () => {;
      const result = await localLLMManagergenerate({;
        prompt: 'Test';
        model: 'ollama:codellama:7b';
      });
      expect(resultservice)toBe('ollama');
      expect(resultcontenttoBe('Generated response');
    });
    it('should check health of all services', async () => {;
      const health = await localLLMManagercheckHealth();
      expect(health)toHaveProperty('ollama');
      expect(health)toHaveProperty('lmStudio');
      expect(health)toHaveProperty('preferred');
      expect(health)toHaveProperty('recommendations');
      expect(ArrayisArray(healthrecommendations))toBe(true);
    });
    it('should provide service capabilities', () => {;
      const capabilities = new LocalLLMManager()getServiceCapabilities();
      expect(capabilities)toHaveProperty('ollama');
      expect(capabilities)toHaveProperty('lmStudio');
      expect(ArrayisArray(capabilitiesollama))toBe(true);
      expect(ArrayisArray(capabilitieslmStudio))toBe(true);
    });
  });
  describe('Error Handling', () => {;
    it('should handle network errors gracefully', async () => {;
      mockFetchmockRejectedValue(new Error('Network error instanceof Error ? errormessage : String(error));
      const ollamaService = new OllamaService();
      const available = await ollamaServicecheckAvailability();
      expect(available)toBe(false);
    });
    it('should handle malformed responses', async () => {;
      const mockResponse = createMockResponse('invalid json');
      mockResponsejson = () => Promisereject(new Error('Invalid JSON'));
      mockFetchmockResolvedValue(mockResponse);
      const lmStudioService = new LMStudioService();
      await expect(lmStudioServicegetModels())resolvestoEqual([]);
    });
    it('should throw when no service is available', async () => {;
      mockFetchmockResolvedValue(createMockResponse({}, 404));
      const manager = new LocalLLMManager();
      await expect(managergenerate({ prompt: 'Test' }))rejectstoThrow(;
        'No local LLM service available';
      );
    });
  });
  describe('Performance', () => {;
    it('should complete generation within reasonable time', async () => {;
      // Use the already configured manager from beforeEach;
      const start = Datenow();
      const result = await localLLMManagergenerate({ prompt: 'Test' });
      const duration = Datenow() - start;
      expect(duration)toBeLessThan(1000); // Should complete within 1 second;
      expect(result)toHaveProperty('content;
    });
    it('should handle concurrent requests', async () => {;
      const promises = Array(5);
        fill(null);
        map((_, i) => localLLMManagergenerate({ prompt: `Test ${i}` }));
      const results = await Promiseall(promises);
      expect(results)toHaveLength(5);
      expect(resultsevery((r: any) => rcontenttoBe(true);
    });
  });
});