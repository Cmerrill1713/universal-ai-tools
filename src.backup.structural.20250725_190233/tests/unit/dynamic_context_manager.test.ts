/**;
 * Tests for Dynamic Context Manager;
 */;

import { jest } from '@jest/globals';
import { DynamicContextManager } from '../../services/dynamic_context_managerjs';
// Mock dependencies;
jestmock('../../utils/loggerjs', () => ({;
  logger: {;
    info: jestfn();
    error instanceof Error ? errormessage : String(error) jestfn();
    warn: jestfn();
    debug: jestfn();
  ;
};
}));
jestmock('../../services/supabase_servicejs', () => ({;
  SupabaseService: {;
    getInstance: jestfn(() => ({;
      client: {;
        from: jestfn(() => ({;
          select: jestfn(() => Promiseresolve({ data: [], error instanceof Error ? errormessage : String(error) null }));
          upsert: jestfn(() => Promiseresolve({ error instanceof Error ? errormessage : String(error) null }));
          insert: jestfn(() => Promiseresolve({ data: [], error instanceof Error ? errormessage : String(error) null }));
        }));
      };
    }));
  };
}));
jestmock('../../services/model_lifecycle_managerjs', () => ({;
  ModelLifecycleManager: jestfn()mockImplementation(() => ({;
    getModelInfo: jestfn()mockReturnValue({;
      name: 'test-model';
      size: 'medium';
      contextWindow: 8192;
    });
    loadModel: jestfn();
    unloadModel: jestfn();
    getInstance: jestfn();
  }));
}));
describe('DynamicContextManager', () => {;
  let contextManager: DynamicContextManager;
  beforeEach(() => {;
    jestclearAllMocks();
    // Reset singleton instance if it exists;
    if ('instance' in DynamicContextManager) {;
      (DynamicContextManager as any)instance = undefined;
    };
    contextManager = DynamicContextManagergetInstance();
  });
  describe('Context Window Configuration', () => {;
    it('should return correct context windows for different model sizes', () => {;
      const testCases = [;
        { model: 'llama-3.2-1b', expectedSize: 'tiny', expectedOptimal: 3072 ;
};
        { model: 'phi-3-mini', expectedSize: 'small', expectedOptimal: 3072 ;
};
        { model: 'llama-3.1-8b', expectedSize: 'medium', expectedOptimal: 12288 ;
};
        { model: 'llama-3.1-70b', expectedSize: 'xlarge', expectedOptimal: 131072 ;
};
      ];
      testCasesforEach(({ model, expectedOptimal }) => {;
        const config = contextManagergetOptimalContext(model);
        expect(configoptimalContext)toBe(expectedOptimal);
        expect(configminContext)toBeLessThanOrEqual(configoptimalContext);
        expect(configmaxContext)toBeGreaterThanOrEqual(configoptimalContext);
      });
    });
    it('should default to medium context for unknown models', () => {;
      const config = contextManagergetOptimalContext('unknown-model');
      expect(configoptimalContext)toBe(12288); // medium default;
    });
  });
  describe('Context Optimization', () => {;
    const createMessage = (contentstring, role: 'user' | 'assistant' | 'system' = 'user') => ({;
      role;
      content;
      timestamp: Datenow();
      tokens: Mathceil(content-length / 4);
    });
    it('should apply sliding window strategy for conversations', async () => {;
      // Create messages that exceed the tiny context window (3072 tokens);
      const largeContent = 'A'repeat(2000); // ~500 tokens each;
      const messages = [;
        createMessage('System prompt', 'system');
        createMessage(`${largeContent} First message`);
        createMessage(`${largeContent} First response`, 'assistant');
        createMessage(`${largeContent} Second message`);
        createMessage(`${largeContent} Second response`, 'assistant');
        createMessage(`${largeContent} Third message`);
        createMessage(`${largeContent} Third response`, 'assistant');
        createMessage(`${largeContent} Fourth message`);
        createMessage(`${largeContent} Fourth response`, 'assistant');
        createMessage(`${largeContent} Fifth message`);
        createMessage(`${largeContent} Fifth response`, 'assistant');
        createMessage(`${largeContent} Sixth message`);
        createMessage(`${largeContent} Most recent response`, 'assistant');
      ];
      const optimized = await contextManageroptimizeContext(;
        messages;
        'llama-3.2-1b';
        'conversation';
      );
      // Calculate total tokens for debugging;
      const totalTokens = messagesreduce((sum, msg) => sum + (msgtokens || 0), 0);
      const optimizedTokens = optimizedreduce((sum, msg) => sum + (msgtokens || 0), 0);
      // Should keep system message and recent messages;
      expect(optimized[0]role)toBe('system');
      expect(optimizedlength)toBeLessThan(messageslength);
      expect(optimized[optimizedlength - 1])toEqual(messages[messageslength - 1]);
    });
    it('should apply importance-based selection for code generation', async () => {;
      const messages = [;
        createMessage('System: You are a code assistant', 'system');
        createMessage('Write a function to sort an array');
        createMessage('```python\ndef sort_array(arr):\n    return sorted(arr)\n```', 'assistant');
        createMessage('Now make it handle edge cases');
        createMessage(;
          '```python\ndef sort_array(arr):\n    if not arr:\n        return []\n    return sorted(arr)\n```';
          'assistant';
        );
      ];
      const optimized = await contextManageroptimizeContext(;
        messages;
        'llama-3.1-8b';
        'code_generation';
      );
      // Should prioritize messages with code blocks;
      const codeMessages = optimizedfilter((m) => mcontentincludes('```'));
      expect(codeMessageslength)toBeGreaterThan(0);
    });
    it('should optimize context based on strategy', async () => {;
      // Test that context optimization reduces token count when needed;
      const largeContent = 'This is a test message. 'repeat(100); // ~600 chars, ~150 tokens;

      // Create messages that exceed the optimal context;
      const messages = [];
      messagespush(createMessage('System prompt', 'system'));
      // Add many messages to exceed context (3072 tokens for tiny model);
      for (let i = 0; i < 30; i++) {;
        messagespush(createMessage(`${largeContent} Message ${i}`));
        messagespush(createMessage(`${largeContent} Response ${i}`, 'assistant'));
      };

      const originalTokens = messagesreduce((sum, msg) => sum + (msgtokens || 0), 0);
      const optimized = await contextManageroptimizeContext(;
        messages;
        'llama-3.2-1b', // Small context window (3072 tokens);
        'conversation' // This will use sliding window;
      );
      const optimizedTokens = optimizedreduce((sum, msg) => sum + (msgtokens || 0), 0);
      // Should have reduced the token count;
      expect(optimizedTokens)toBeLessThan(originalTokens);
      expect(optimizedlength)toBeLessThan(messageslength);
      // Should keep system message;
      expect(optimizedsome((m) => mrole === 'system'))toBe(true);
      // Should keep recent messages;
      expect(optimized[optimizedlength - 1]contenttoContain('Response 29');
    });
  });
  describe('Context Statistics', () => {;
    it('should track token usage statistics', async () => {;
      const messages = [;
        { role: 'user' as const, content'Test message', timestamp: Datenow(), tokens: 10 ;
};
        { role: 'assistant' as const, content'Response', timestamp: Datenow(), tokens: 8 ;
};
      ];
      await contextManageroptimizeContext(messages, 'llama-3.1-8b', 'general');
      const stats = contextManagergetStats();
      expect(statstotalTokensProcessed)toBeGreaterThan(0);
      expect(statscompressionRatio)toBeGreaterThanOrEqual(1);
      expect(statssavingsPercentage)toBeDefined();
    });
  });
  describe('Context Recommendations', () => {;
    it('should provide appropriate recommendations for different tasks', () => {;
      const testCases = [;
        { model: 'llama-3.1-8b', task: 'code_generation', expectedStrategy: 'importance_based' ;
};
        { model: 'llama-3.1-8b', task: 'conversation', expectedStrategy: 'sliding_window' ;
};
        { model: 'llama-3.1-8b', task: '_analysis, expectedStrategy: 'importance_based' ;
};
        { model: 'llama-3.1-8b', task: undefined, expectedStrategy: 'hybrid' ;
};
      ];
      testCasesforEach(({ model, task, expectedStrategy }) => {;
        const recommendations = contextManagergetContextRecommendations(model, task);
        expect(recommendationsstrategy)toBe(expectedStrategy);
        expect(recommendationsrecommended)toBeGreaterThan(0);
        expect(recommendationsminimum)toBeLessThanOrEqual(recommendationsrecommended);
        expect(recommendationsmaximum)toBeGreaterThanOrEqual(recommendationsrecommended);
      });
    });
  });
});