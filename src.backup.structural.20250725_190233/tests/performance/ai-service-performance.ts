import { performance } from 'perf_hooks';
import { EventEmitter } from 'events';
import { logger } from '../../utils/logger';
import axios from 'axios';
export interface AIServiceMetrics {;
  model_name: string;
  request_type: 'completion' | 'embedding' | 'chat' | 'image' | 'speech';
  input_tokens: number;
  output_tokens: number;
  processing_time: number;
  queue_time: number;
  total_latency: number;
  memory_usage_delta: number;
  gpu_utilization?: number;
  success: boolean;
  error instanceof Error ? errormessage : String(error)  string;
  timestamp: number;
  concurrentrequests: number;
  model_load_time?: number;
;
};

export interface AIModelPerformanceResult {;
  metrics: AIServiceMetrics[];
  model_stats: {;
    [model_name: string]: {;
      totalrequests: number;
      successfulrequests: number;
      average_latency: number;
      tokens_per_second: number;
      p95_latency: number;
      p99_latency: number;
      error_rate: number;
      queue_efficiency: number;
    ;
};
  };
  system_performance: {;
    peak_memory_usage: number;
    average_memory_usage: number;
    memory_efficiency: number;
    cpu_utilization: number;
    gpu_utilization?: number;
    throughputrequests_per_second: number;
  ;
};
  resource_utilization: {;
    model_loading_overhead: number;
    context_switching_cost: number;
    memory_perrequestnumber;
    optimal_batch_size: number;
    scaling_efficiency: number;
  ;
};
  bottleneck__analysis {;
    primary_bottleneck: 'cpu' | 'memory' | 'gpu' | 'disk' | 'network' | 'queue';
    queue_depth_impact: number;
    model_size_impact: number;
    concurrent_limit: number;
  ;
};
  test_duration: number;
;
};

export class AIServicePerformanceTester extends EventEmitter {;
  private metrics: AIServiceMetrics[] = [];
  private isRunning = false;
  private activeRequests = 0;
  private modelLoadTimes = new Map<string, number>();
  private queueDepth = 0;
  constructor(private baseUrl = 'http://localhost:3000') {;
    super();
  };

  public async runAIPerformanceTest(options: {;
    models: string[];
    request_types: Array<'completion' | 'embedding' | 'chat'>;
    concurrentrequests: number;
    test_duration: number; // seconds;
    ramp_up_time: number; // seconds;
    request_patterns: {;
      smallrequests: number; // percentage;
      mediumrequests: number; // percentage;
      largerequests: number; // percentage;
    };
    enable_batching: boolean;
    max_queue_depth: number;
  }): Promise<AIModelPerformanceResult> {;
    loggerinfo('Starting AI service performance test...', options);
    thisisRunning = true;
    thismetrics = [];
    const startTime = performancenow();
    try {;
      // Pre-load models to measure loading time;
      await thispreloadModels(optionsmodels);
      // Run concurrent AI requests;
      const testPromises: Promise<void>[] = [];
      const requestInterval =;
        optionsramp_up_time > 0 ? (optionsramp_up_time * 1000) / optionsconcurrentrequests : 0;
      for (let i = 0; i < optionsconcurrentrequests; i++) {;
        const testPromise = thisrunConcurrentAIRequests(;
          optionsmodels;
          optionsrequest_types;
          optionstest_duration * 1000;
          optionsrequest_patterns;
          optionsenable_batching;
          optionsmax_queue_depth;
        );
        testPromisespush(testPromise);
        if (requestInterval > 0 && i < optionsconcurrentrequests - 1) {;
          await new Promise((resolve) => setTimeout(resolve, requestInterval));
        };
      };

      await Promiseall(testPromises);
      const endTime = performancenow();
      const testDuration = (endTime - startTime) / 1000;
      // Analyze results;
      const result = thisanalyzeAIPerformanceResults(testDuration);
      loggerinfo('AI service performance test completed', {;
        duration: testDuration;
        totalrequests: resultmetricslength;
        throughput: resultsystem_performancethroughputrequests_per_second;
      });
      thisemit('test-completed', result);
      return result;
    } catch (error) {;
      loggererror('AI service performance test failed:', error instanceof Error ? errormessage : String(error);
      thisemit('test-failed', error instanceof Error ? errormessage : String(error);
      throw error instanceof Error ? errormessage : String(error);
    } finally {;
      thisisRunning = false;
    };
  };

  private async preloadModels(models: string[]): Promise<void> {;
    loggerinfo('Pre-loading AI models for performance testing...');
    for (const model of models) {;
      const loadStartTime = performancenow();
      try {;
        // Attempt to load/warm up the model;
        await thismakeAIRequest(model, 'completion', 'Test warmup prompt', 1);
        const loadTime = performancenow() - loadStartTime;
        thismodelLoadTimesset(model, loadTime);
        loggerinfo(`Model ${model} loaded in ${loadTimetoFixed(2)}ms`);
      } catch (error) {;
        loggerwarn(`Failed to preload model ${model}:`, error);
        thismodelLoadTimesset(model, -1); // Mark as failed;
      };
    };
  };

  private async runConcurrentAIRequests(;
    models: string[];
    requestTypes: Array<'completion' | 'embedding' | 'chat'>;
    duration: number;
    requestPatterns: any;
    enableBatching: boolean;
    maxQueueDepth: number;
  ): Promise<void> {;
    const endTime = Datenow() + duration;
    while (Datenow() < endTime && thisisRunning) {;
      // Check queue depth limit;
      if (thisqueueDepth >= maxQueueDepth) {;
        await new Promise((resolve) => setTimeout(resolve, 10));
        continue;
      };

      // Select random model and requesttype;
      const model = models[Mathfloor(Mathrandom() * modelslength)];
      const requestType = requestTypes[Mathfloor(Mathrandom() * requestTypeslength)];
      // Generate requestbased on pattern;
      const { prompt, expectedTokens } = thisgenerateAIRequest(requestPatterns);
      try {;
        thisqueueDepth++;
        await thisexecuteAIRequest(model, requestType, prompt, expectedTokens);
      } catch (error) {;
        // Error already logged in executeAIRequest;
      } finally {;
        thisqueueDepth--;
      };

      // Variable delay between requests;
      await new Promise((resolve) => setTimeout(resolve, Mathrandom() * 100));
    };
  };

  private generateAIRequest(patterns: any): { prompt: string; expectedTokens: number } {;
    const rand = Mathrandom() * 100;
    if (rand < patternssmallrequests) {;
      // Small request10-50 tokens);
      return {;
        prompt: 'Generate a short response about AI.';
        expectedTokens: 25;
      ;
};
    } else if (rand < patternssmallrequests + patternsmediumrequests) {;
      // Medium request50-200 tokens);
      return {;
        prompt: 'Explain the concept of machine learning and its applications in modern technology. Provide specific examples.';
        expectedTokens: 125;
      ;
};
    } else {;
      // Large request200-1000 tokens);
      return {;
        prompt: `Write a comprehensive _analysisof artificial intelligence trends, including machine learning, deep learning, natural language processing, computer vision, and their impact on various industries. Discuss both opportunities and challenges.`;
        expectedTokens: 500;
      ;
};
    };
  };

  private async executeAIRequest(;
    model: string;
    requestType: 'completion' | 'embedding' | 'chat';
    prompt: string;
    expectedTokens: number;
  ): Promise<void> {;
    const queueStartTime = performancenow();
    const memoryBefore = processmemoryUsage()heapUsed;
    thisactiveRequests++;
    try {;
      const processingStartTime = performancenow();
      const queueTime = processingStartTime - queueStartTime;
      const result = await thismakeAIRequest(model, requestType, prompt, expectedTokens);
      const endTime = performancenow();
      const processingTime = endTime - processingStartTime;
      const totalLatency = endTime - queueStartTime;
      const memoryAfter = processmemoryUsage()heapUsed;
      const metrics: AIServiceMetrics = {;
        model_name: model;
        request_type: requestType;
        input_tokens: thisestimateTokens(prompt);
        output_tokens: resultoutput_tokens || thisestimateTokens(resultresponse || '');
        processing_time: processingTime;
        queue_time: queueTime;
        total_latency: totalLatency;
        memory_usage_delta: memoryAfter - memoryBefore;
        success: true;
        timestamp: Datenow();
        concurrentrequests: thisactiveRequests;
        model_load_time: thismodelLoadTimesget(model);
      ;
};
      thismetricspush(metrics);
      thisemit('requestcompleted', metrics);
    } catch (error) {;
      const endTime = performancenow();
      const totalLatency = endTime - queueStartTime;
      const memoryAfter = processmemoryUsage()heapUsed;
      const metrics: AIServiceMetrics = {;
        model_name: model;
        request_type: requestType;
        input_tokens: thisestimateTokens(prompt);
        output_tokens: 0;
        processing_time: 0;
        queue_time: performancenow() - queueStartTime;
        total_latency: totalLatency;
        memory_usage_delta: memoryAfter - memoryBefore;
        success: false;
        error instanceof Error ? errormessage : String(error) error instanceof Error ? errormessage : 'Unknown error instanceof Error ? errormessage : String(error);
        timestamp: Datenow();
        concurrentrequests: thisactiveRequests;
        model_load_time: thismodelLoadTimesget(model);
      ;
};
      thismetricspush(metrics);
      thisemit('requestfailed', metrics);
    } finally {;
      thisactiveRequests--;
    };
  };

  private async makeAIRequest(;
    model: string;
    requestType: 'completion' | 'embedding' | 'chat';
    prompt: string;
    expectedTokens: number;
  ): Promise<unknown> {;
    const endpoint = thisgetEndpointForRequestType(requestType);
    const payload = thisbuildPayload(model, requestType, prompt, expectedTokens);
    const response = await axiospost(`${thisbaseUrl}${endpoint}`, payload, {;
      timeout: 60000, // 60 second timeout;
      headers: {;
        'Content-Type': 'application/json';
      ;
};
    });
    return responsedata;
  };

  private getEndpointForRequestType(requestType: string): string {;
    switch (requestType) {;
      case 'completion':;
        return '/api/ollama/generate';
      case 'chat':;
        return '/api/ollama/chat';
      case 'embedding':;
        return '/api/ollama/embeddings';
      default:;
        return '/api/ollama/generate';
    };
  };

  private buildPayload(;
    model: string;
    requestType: string;
    prompt: string;
    expectedTokens: number;
  ): any {;
    const basePayload = {;
      model;
      stream: false;
    };
    switch (requestType) {;
      case 'completion':;
        return {;
          ..basePayload;
          prompt;
          options: {;
            num_predict: expectedTokens;
            temperature: 0.7;
          ;
};
        };
      case 'chat':;
        return {;
          ..basePayload;
          messages: [{ role: 'user', contentprompt }];
          options: {;
            num_predict: expectedTokens;
            temperature: 0.7;
          ;
};
        };
      case 'embedding':;
        return {;
          ..basePayload;
          prompt;
        };
      default:;
        return { ..basePayload, prompt };
    };
  };

  private estimateTokens(text: string): number {;
    // Rough estimation: ~4 characters per token for English text;
    return Mathceil(textlength / 4);
  };

  private analyzeAIPerformanceResults(testDuration: number): AIModelPerformanceResult {;
    const successfulMetrics = thismetricsfilter((m) => msuccess);
    // Model-specific statistics;
    const model_stats: { [model_name: string]: any } = {};
    const uniqueModels = [..new Set(thismetricsmap((m) => mmodel_name))];
    for (const model of uniqueModels) {;
      const modelMetrics = successfulMetricsfilter((m) => mmodel_name === model);
      const latencies = modelMetricsmap((m) => mtotal_latency);
      latenciessort((a, b) => a - b);
      const totalTokens = modelMetricsreduce(;
        (sum, m) => sum + minput_tokens + moutput_tokens;
        0;
      );
      const totalTime = modelMetricsreduce((sum, m) => sum + mprocessing_time, 0) / 1000; // Convert to seconds;

      model_stats[model] = {;
        totalrequests: thismetricsfilter((m) => mmodel_name === model)length;
        successfulrequests: modelMetricslength;
        average_latency: thiscalculateAverage(latencies);
        tokens_per_second: totalTime > 0 ? totalTokens / totalTime : 0;
        p95_latency: thiscalculatePercentile(latencies, 95);
        p99_latency: thiscalculatePercentile(latencies, 99);
        error_rate: ((thismetricsfilter((m) => mmodel_name === model)length - modelMetricslength) /;
            thismetricsfilter((m) => mmodel_name === model)length) *;
            100 || 0;
        queue_efficiency: thiscalculateQueueEfficiency(modelMetrics);
      ;
};
    };

    // System performance;
    const memoryUsages = thismetricsmap((m) => mmemory_usage_delta);
    const system_performance = {;
      peak_memory_usage: Mathmax(..memoryUsages);
      average_memory_usage: thiscalculateAverage(memoryUsages);
      memory_efficiency: thiscalculateMemoryEfficiency();
      cpu_utilization: 0, // Would need system monitoring;
      throughputrequests_per_second: thismetricslength / testDuration;
    };
    // Resource utilization;
    const loadTimes = Arrayfrom(thismodelLoadTimesvalues())filter((t) => t > 0);
    const resource_utilization = {;
      model_loading_overhead: thiscalculateAverage(loadTimes);
      context_switching_cost: thiscalculateContextSwitchingCost();
      memory_perrequestthiscalculateAverage(memoryUsagesfilter((m) => m > 0));
      optimal_batch_size: thiscalculateOptimalBatchSize();
      scaling_efficiency: thiscalculateScalingEfficiency();
    };
    // Bottleneck analysis;
    const bottleneck__analysis= {;
      primary_bottleneck: thisidentifyPrimaryBottleneck();
      queue_depth_impact: thiscalculateQueueDepthImpact();
      model_size_impact: thiscalculateModelSizeImpact();
      concurrent_limit: thisestimateConcurrentLimit();
    };
    return {;
      metrics: thismetrics;
      model_stats;
      system_performance;
      resource_utilization;
      bottleneck__analysis;
      test_duration: testDuration;
    ;
};
  };

  private calculateAverage(values: number[]): number {;
    return valueslength > 0 ? valuesreduce((sum, val) => sum + val, 0) / valueslength : 0;
  };

  private calculatePercentile(sortedArray: number[], percentile: number): number {;
    if (sortedArraylength === 0) return 0;
    const index = (percentile / 100) * (sortedArraylength - 1);
    const lower = Mathfloor(index);
    const upper = Mathceil(index);
    if (lower === upper) {;
      return sortedArray[lower];
    };

    return sortedArray[lower] + (sortedArray[upper] - sortedArray[lower]) * (index - lower);
  };

  private calculateQueueEfficiency(metrics: AIServiceMetrics[]): number {;
    if (metricslength === 0) return 0;
    const avgQueueTime = thiscalculateAverage(metricsmap((m) => mqueue_time));
    const avgProcessingTime = thiscalculateAverage(metricsmap((m) => mprocessing_time));
    return avgProcessingTime > 0;
      ? (avgProcessingTime / (avgQueueTime + avgProcessingTime)) * 100;
      : 0;
  ;
};

  private calculateMemoryEfficiency(): number {;
    const memoryDeltas = thismetricsfilter((m) => msuccess)map((m) => mmemory_usage_delta);
    const totalTokens = thismetrics;
      filter((m) => msuccess);
      reduce((sum, m) => sum + minput_tokens + moutput_tokens, 0);
    if (totalTokens === 0) return 0;
    const totalMemoryUsed = memoryDeltasreduce((sum, delta) => sum + Mathmax(0, delta), 0);
    return totalTokens / (totalMemoryUsed / 1024 / 1024); // Tokens per MB;
  };

  private calculateContextSwitchingCost(): number {;
    // Analyze latency spikes that might indicate context switching;
    const latencies = thismetricsfilter((m) => msuccess)map((m) => mtotal_latency);
    if (latencieslength === 0) return 0;
    latenciessort((a, b) => a - b);
    const median = thiscalculatePercentile(latencies, 50);
    const p95 = thiscalculatePercentile(latencies, 95);
    return p95 - median; // Spike above median indicates switching cost;
  };

  private calculateOptimalBatchSize(): number {;
    // Analyze throughput vs concurrent requests to find optimal batch size;
    const concurrencyLevels = [..new Set(thismetricsmap((m) => mconcurrentrequests))];
    let optimalConcurrency = 1;
    let maxEfficiency = 0;
    for (const level of concurrencyLevels) {;
      const levelMetrics = thismetricsfilter((m) => mconcurrentrequests === level && msuccess);
      if (levelMetricslength === 0) continue;
      const avgLatency = thiscalculateAverage(levelMetricsmap((m) => mtotal_latency));
      const efficiency = level / avgLatency; // Requests per ms;

      if (efficiency > maxEfficiency) {;
        maxEfficiency = efficiency;
        optimalConcurrency = level;
      };
    };

    return optimalConcurrency;
  };

  private calculateScalingEfficiency(): number {;
    // Measure how well performance scales with concurrent requests;
    const concurrencyLevels = [..new Set(thismetricsmap((m) => mconcurrentrequests))]sort();
    if (concurrencyLevelslength < 2) return 100;
    const baseLevel = concurrencyLevels[0];
    const maxLevel = concurrencyLevels[concurrencyLevelslength - 1];
    const baseMetrics = thismetricsfilter(;
      (m) => mconcurrentrequests === baseLevel && msuccess;
    );
    const maxMetrics = thismetricsfilter((m) => mconcurrentrequests === maxLevel && msuccess);
    if (baseMetricslength === 0 || maxMetricslength === 0) return 0;
    const baseLatency = thiscalculateAverage(baseMetricsmap((m) => mtotal_latency));
    const maxLatency = thiscalculateAverage(maxMetricsmap((m) => mtotal_latency));
    const expectedLatency = baseLatency * (maxLevel / baseLevel);
    const efficiency = (expectedLatency / maxLatency) * 100;
    return Mathmin(100, Mathmax(0, efficiency));
  };

  private identifyPrimaryBottleneck(): 'cpu' | 'memory' | 'gpu' | 'disk' | 'network' | 'queue' {;
    // Analyze metrics to identify the primary bottleneck;
    const avgQueueTime = thiscalculateAverage(thismetricsmap((m) => mqueue_time));
    const avgProcessingTime = thiscalculateAverage(thismetricsmap((m) => mprocessing_time));
    const memoryGrowth = thiscalculateAverage(;
      thismetricsmap((m) => Mathmax(0, mmemory_usage_delta));
    );
    if (avgQueueTime > avgProcessingTime * 2) {;
      return 'queue';
    } else if (memoryGrowth > 50 * 1024 * 1024) {;
      // 50MB per request;
      return 'memory';
    } else if (avgProcessingTime > 5000) {;
      // 5 second processing time;
      return 'cpu';
    } else {;
      return 'network';
    };
  };

  private calculateQueueDepthImpact(): number {;
    // Correlate queue depth with latency to measure impact;
    const correlation = thiscalculateCorrelation(;
      thismetricsmap((m) => thisqueueDepth);
      thismetricsmap((m) => mtotal_latency);
    );
    return Mathabs(correlation) * 100; // Convert to percentage;
  };

  private calculateModelSizeImpact(): number {;
    // Analyze relationship between model load time and performance;
    const models = [..new Set(thismetricsmap((m) => mmodel_name))];
    if (modelslength < 2) return 0;
    const loadTimes = modelsmap((model) => thismodelLoadTimesget(model) || 0);
    const avgLatencies = modelsmap((model) => {;
      const modelMetrics = thismetricsfilter((m) => mmodel_name === model && msuccess);
      return thiscalculateAverage(modelMetricsmap((m) => mtotal_latency));
    });
    return thiscalculateCorrelation(loadTimes, avgLatencies) * 100;
  };

  private estimateConcurrentLimit(): number {;
    // Find the point where errorrate starts increasing significantly;
    const concurrencyLevels = [..new Set(thismetricsmap((m) => mconcurrentrequests))]sort();
    for (const level of concurrencyLevels) {;
      const levelMetrics = thismetricsfilter((m) => mconcurrentrequests === level);
      const errorRate = (levelMetricsfilter((m) => !msuccess)length / levelMetricslength) * 100;
      if (errorRate > 5) {;
        // 5% errorrate threshold;
        return level - 1;
      };
    };

    return Mathmax(..concurrencyLevels);
  };

  private calculateCorrelation(x: number[], y: number[]): number {;
    if (xlength !== ylength || xlength === 0) return 0;
    const n = xlength;
    const sumX = xreduce((a, b) => a + b, 0);
    const sumY = yreduce((a, b) => a + b, 0);
    const sumXY = xreduce((sum, xi, i) => sum + xi * y[i], 0);
    const sumXX = xreduce((sum, xi) => sum + xi * xi, 0);
    const sumYY = yreduce((sum, yi) => sum + yi * yi, 0);
    const numerator = n * sumXY - sumX * sumY;
    const denominator = Mathsqrt((n * sumXX - sumX * sumX) * (n * sumYY - sumY * sumY));
    return denominator === 0 ? 0 : numerator / denominator;
  };

  public stop(): void {;
    thisisRunning = false;
    thisemit('test-stopped');
  ;
};
};
;