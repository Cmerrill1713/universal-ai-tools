/**
 * LFM2 Bridge Service - Fast Local Model Integration
 * Bridges TypeScript services with Python LFM2-1.2B model
 * Optimized for speed and coordination tasks
 */

import type { ChildProcess } from 'child_process';
import { spawn } from 'child_process';
import { LogContext, log } from '@/utils/logger';
import { ollamaService } from '@/services/ollama-service';
import { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';
import { THREE } from '@/utils/constants';

export interface LFM2Request {
  prompt: string;
  systemPrompt?: string;
  maxLength?: number;
  maxTokens?: number;
  temperature?: number;
  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';
}

export interface LFM2Response {
  content: string;
  tokens: number;
  executionTime: number;
  model: string; // Allow different model names for fallback
  confidence?: number;
}

export interface LFM2Metrics {
  avgResponseTime: number;
  totalRequests: number;
  successRate: number;
  tokenThroughput: number;
}

export class LFM2BridgeService {
  private pythonProcess:
    | ChildProcess     | null = null;
  private isInitialized = false;
  private requestQueue: Array<{
    id: string;
    request: LFM2Request;
    resolve: (response: LFM2Response) => void;
    reject: (error: Error) => void;
  }> = [];
  private pendingRequests: Map<
    string,
    {
      resolve: (response: LFM2Response) => void;
      reject: (error: Error) => void;
      startTime: number;
    }
  > = new Map();
  private metrics: LFM2Metrics = {
    avgResponseTime: 0,
    totalRequests: 0,
    successRate: 1.0,
    tokenThroughput: 0,
  };

  constructor() {
    this.initializeLFM2();
  }

  private async initializeLFM2(): Promise<void> {
    try {
      log.info('üöÄ Initializing LFM2-1.2B bridge service', LogContext.AI);

      // Use Ollama directly instead of Python process
      // Test if we can connect to Ollama and get a working model
      const availableModels = await this.getAvailableModels();
      const workingModel = availableModels.find(model => this.isModelWorking(model));
      
      if (!workingModel) {
        throw new Error('No working Ollama models available');
      }

      log.info(`‚úÖ Using Ollama model ${workingModel} for LFM2 bridge`, LogContext.AI);
      this.isInitialized = true;

    } catch (error) {
      log.error('‚ùå Failed to initialize LFM2 bridge service', LogContext.AI, { error });
      // Fall back to mock implementation
      this.initializeMockLFM2();
    }
  }

  private async getAvailableModels(): Promise<string[]> {
    try {
      const response = await fetch('http://localhost:11434/api/tags');
      const data = await response.json();
      return data.models?.map((model: any) => model.name) || [];
    } catch (error) {
      log.warn('‚ö†Ô∏è Failed to fetch available models from Ollama', LogContext.AI, { error });
      return [];
    }
  }

  private isModelWorking(modelName: string): boolean {
    // For now, we know llama3.2:3b works
    return modelName === 'llama3.2:3b';
  }

  private initializeMockLFM2(): void {
    log.warn('‚ö†Ô∏è Using mock LFM2 implementation for testing', LogContext.AI);
    this.isInitialized = true;
  }

  /**
   * Fast routing decision using LFM2
   */
  public async routingDecision(
    userRequest: string,
    context: Record<string, any>
  ): Promise<{
    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';
    confidence: number;
    reasoning: string;
    estimatedTokens: number;
  }> {
    const prompt = this.createRoutingPrompt(userRequest, context);

    const response = await this.generate({
      prompt,
      maxLength: 200,
      temperature: 0.3,
      taskType: 'routing',
    });

    // Parse LFM2 routing response
    return this.parseRoutingResponse(response.content) as {
      targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';
      confidence: number;
      reasoning: string;
      estimatedTokens: number;
    };
  }

  /**
   * Quick classification and simple Q&A
   */
  public async quickResponse(
    userRequest: string,
    taskType: 'classification' | 'simple_qa' = 'simple_qa'
  ): Promise<LFM2Response> {
    const prompt = this.createQuickResponsePrompt(userRequest, taskType);

    return this.generate({
      prompt,
      maxLength: 150,
      temperature: 0.6,
      taskType,
    });
  }

  /**
   * Coordinate multiple agent tasks
   */
  public async coordinateAgents(
    primaryTask: string,
    supportingTasks: string[]
  ): Promise<{
    execution_plan: {
      primary_priority: number;
      supporting_priorities: number[];
      parallel_execution: boolean;
      estimated_total_time: number;
    };
    resource_allocation: {
      primary_service: string;
      supporting_services: string[];
    };
  }> {
    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);

    const response = await this.generate({
      prompt,
      maxLength: 300,
      temperature: 0.4,
      taskType: 'coordination',
    });

    return this.parseCoordinationResponse(response.content) as {
      execution_plan: {
        primary_priority: number;
        supporting_priorities: number[];
        parallel_execution: boolean;
        estimated_total_time: number;
      };
      resource_allocation: {
        primary_service: string;
        supporting_services: string[];
      };
    };
  }

  /**
   * Core generation method
   */
  public async generate(request: LFM2Request): Promise<LFM2Response> {
    if (!this.isInitialized) {
      return this.generateMockResponse(request);
    }

    const startTime = Date.now();

    try {
      // Use Ollama directly instead of Python process
      const ollamaResponse = await ollamaService.generateResponse(
        [{ role: 'user', content: request.prompt }],
        'llama3.2:3b' // Use the working model
      );

      const executionTime = Date.now() - startTime;

      return {
        content: ollamaResponse.message.content,
        tokens: ollamaResponse.eval_count || 0,
        executionTime,
        model: 'llama3.2:3b',
        confidence: 0.9,
      };
    } catch (error) {
      log.error('‚ùå LFM2 generation failed, using mock', LogContext.AI, { error });
      return this.generateMockResponse(request);
    }
  }

  /**
   * Batch processing for efficiency
   */
  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {
    log.info('üì¶ Processing LFM2 batch request', LogContext.AI, { count: requests.length });

    // Process requests in parallel but limit concurrency
    const batchSize = THREE;
    const results: LFM2Response[] = [];

    for (let i = 0; i < requests.length; i += batchSize) {
      const batch = requests.slice(i, i + batchSize);
      const batchResults = await Promise.all(batch.map((request) => this.generate(request)));
      results.push(...batchResults);
    }

    return results;
  }

  private handlePythonResponse(data: string): void {
    const lines = data.trim().split('\n');

    for (const line of lines) {
      if (line === 'INITIALIZED') {
        this.isInitialized = true;
        continue;
      }

      try {
        const response = JSON.parse(line);

        // Handle Python server response format
        if (response.requestId && this.pendingRequests.has(response.requestId)) {
          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;
          this.pendingRequests.delete(response.requestId);

          const executionTime = response.processingTime || Date.now() - startTime;

          if (response.success) {
            const content = response.text || response.strategy || response.category || '';

            // Update metrics
            this.updateMetrics(executionTime, content.length);

            resolve({
              content,
              tokens: Math.ceil(content.length / 4),
              executionTime,
              model: response.model || 'lfm2-1.2b',
              confidence: response.confidence,
            });
          } else {
            reject(new Error(response.error || 'LFM2 processing failed'));
          }
        }
      } catch (error) {
        log.error('‚ùå Failed to parse LFM2 response', LogContext.AI, { error, data: line });
      }
    }
  }

  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {
    return `FAST ROUTING DECISION:

USER REQUEST: "${userRequest}"
CONTEXT: ${JSON.stringify(context)}

ROUTING OPTIONS:
- lfm2: Simple questions, quick responses (<100 tokens)
- ollama: Medium complexity, general purpose (<1000 tokens)
- lm-studio: Code generation, technical tasks (<2000 tokens)
- openai: Complex reasoning, creative tasks (>1000 tokens)
- anthropic: Analysis, research, long-form content

Respond with JSON:
{"service": "...", "confidence": 0.0-1.0, "reasoning": "...", "tokens": number}`;
  }

  private createQuickResponsePrompt(userRequest: string, taskType: string): string {
    const taskInstructions = {
      classification: 'Classify this request into categories and respond briefly.',
      simple_qa: 'Answer this question quickly and concisely.',
    };

    return `${taskInstructions[taskType as keyof typeof taskInstructions]}

REQUEST: "${userRequest}"

Response:`;
  }

  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {
    return `AGENT COORDINATION PLAN:

PRIMARY TASK: "${primaryTask}"
SUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i + 1}. "${task}"`).join(', ')}

Create execution plan with priorities, resource allocation, and timing.

Respond with JSON:
{
  "execution_plan": {
    "primary_priority": 1-5,
    "supporting_priorities": [1-5, ...],
    "parallel_execution": boolean,
    "estimated_total_time": seconds
  },
  "resource_allocation": {
    "primary_service": "service_name",
    "supporting_services": ["service1", "service2", ...]
  }
}`;
  }

  private parseRoutingResponse(content: string): unknown {
    try {
      const jsonMatch = content.match(/{.*}/s);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        return {
          targetService: parsed.service || 'ollama',
          confidence: parsed.confidence || 0.7,
          reasoning: parsed.reasoning || 'Automatic routing decision',
          estimatedTokens: parsed.tokens || 100,
        };
      }
    } catch (error) {
      log.warn('‚ö†Ô∏è Failed to parse LFM2 routing response', LogContext.AI);
    }

    // Fallback parsing
    return {
      targetService: 'ollama' as const,
      confidence: 0.5,
      reasoning: 'Fallback routing due to parsing error',
      estimatedTokens: 100,
    };
  }

  private parseCoordinationResponse(content: string): unknown {
    try {
      const jsonMatch = content.match(/{.*}/s);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0]);
      }
    } catch (error) {
      log.warn('‚ö†Ô∏è Failed to parse LFM2 coordination response', LogContext.AI);
    }

    // Fallback coordination plan
    return {
      execution_plan: {
        primary_priority: 1,
        supporting_priorities: [2, 3],
        parallel_execution: true,
        estimated_total_time: 30,
      },
      resource_allocation: {
        primary_service: 'ollama',
        supporting_services: ['lfm2', 'ollama'],
      },
    };
  }

  private generateMockResponse(request: LFM2Request): LFM2Response {
    // Fast mock response for development/testing
    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;

    return {
      content: mockContent,
      tokens: Math.ceil(mockContent.length / 4),
      executionTime: 50 + Math.random() * 100, // 50-150ms
      model: 'LFM2-1.2B',
      confidence: 0.8,
    };
  }

  private generateRequestId(): string {
    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private updateMetrics(executionTime: number, responseLength: number): void {
    this.metrics.totalRequests++;

    // Exponential moving average
    const alpha = 0.1;
    this.metrics.avgResponseTime =
      alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;

    // Token throughput (tokens per second)
    const tokens = Math.ceil(responseLength / 4);
    const throughput = tokens / (executionTime / 1000);
    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;
  }

  private async restartProcess(): Promise<void> {
    log.info('üîÑ Restarting LFM2 bridge service', LogContext.AI);

    if (this.pythonProcess) {
      this.pythonProcess.kill();
      this.pythonProcess = null;
    }

    // Wait a bit before restarting
    await new Promise((resolve) => setTimeout(resolve, 2000));

    this.initializeLFM2();
  }

  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {
    return {
      ...this.metrics,
      isInitialized: this.isInitialized,
    };
  }

  public isAvailable(): boolean {
    return this.isInitialized && this.pythonProcess !== null;
  }

  public async shutdown(): Promise<void> {
    log.info('üõë Shutting down LFM2 bridge service', LogContext.AI);

    if (this.pythonProcess) {
      this.pythonProcess.kill();
      this.pythonProcess = null;
    }

    this.isInitialized = false;
  }
}

// Create singleton with lazy initialization and circuit breaker protection
class SafeLFM2Bridge {
  private instance: LFM2BridgeService | null = null;
  private initAttempted = false;
  private circuitBreaker: CircuitBreaker<LFM2Response>;

  constructor() {
    // Create circuit breaker with optimized settings
    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {
      failureThreshold: 3,
      successThreshold: 2,
      timeout: 30000, // 30 seconds
      errorThresholdPercentage: 60,
      volumeThreshold: 5,
    });

    // Register for monitoring
    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);
  }

  async quickResponse(
    userRequest: string,
    taskType: 'classification' | 'simple_qa' = 'simple_qa'
  ): Promise<LFM2Response> {
    return this.circuitBreaker.execute(async () => {
      if (!this.initAttempted && !this.instance) {
        this.initAttempted = true;
        try {
          this.instance = new LFM2BridgeService();
          log.info('‚úÖ LFM2 bridge initialized successfully', LogContext.AI);
        } catch (error) {
          log.warn('‚ö†Ô∏è LFM2 bridge initialization failed, using fallback', LogContext.AI, {
            error,
          });
          return this.createFallbackResponse(userRequest);
        }
      }

      if (this.instance) {
        return this.instance.quickResponse(userRequest, taskType);
      } else {
        return this.createFallbackResponse(userRequest);
      }
    });
  }

  async execute(request: LFM2Request): Promise<LFM2Response> {
    // Use circuit breaker for resilient execution
    return this.circuitBreaker.execute(
      async () => {
        // Try to initialize LFM2 if not attempted
        if (!this.initAttempted && !this.instance) {
          this.initAttempted = true;
          try {
            this.instance = new LFM2BridgeService();
            log.info('‚úÖ LFM2 bridge initialized successfully', LogContext.AI);
          } catch (error) {
            log.warn('‚ö†Ô∏è LFM2 bridge initialization failed, using fallback', LogContext.AI, {
              error: error instanceof Error ? error.message : String(error),
            });
          }
        }

        // Try native LFM2 if available
        if (this.instance && this.instance.isAvailable()) {
          return this.instance.generate(request);
        }

        // Use Ollama as primary fallback
        const           messages = [
            ...(request.systemPrompt
              ? [{ role: 'system' as const, content: request.systemPrompt }]
              : []),
            { role: 'user' as const, content: request.prompt },
          ];

        const response = await ollamaService.generateResponse(
          [{ role: 'user', content: request.prompt }],
          'llama3.2:3b',
          {
            temperature: request.temperature || 0.1,
            max_tokens: request.maxTokens || 100,
          }
        );

        return {
          content: response.message.content,
          tokens: response.eval_count || 50,
          executionTime: (response.total_duration || 100000000) / 1000000,
          model: 'llama3.2:3b (LFM2 fallback)',
          confidence: 0.85,
        };
      },
      // Fallback function when circuit is open
      async () => {
        log.warn('‚ö° Circuit breaker active, using emergency fallback', LogContext.AI);
        return {
          content: "I'm currently experiencing high load. Please try again in a moment.",
          tokens: 10,
          executionTime: 1,
          model: 'circuit-breaker-fallback',
          confidence: 0.3,
        };
      }
    );
  }

  private createFallbackResponse(userRequest: string): LFM2Response {
    return {
      content: `I understand you're asking about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,
      tokens: 25,
      executionTime: 1,
      model: 'fallback-response',
      confidence: 0.4,
    };
  }

  isAvailable(): boolean {
    return this.instance?.isAvailable() || true; // Always available with fallback
  }

  getMetrics() {
    if (this.instance) {
      return this.instance.getMetrics();
    }
    return {
      avgResponseTime: 100,
      totalRequests: 0,
      successRate: 1.0,
      tokenThroughput: 500,
    };
  }

  /**
   * Fast routing decision using LFM2 with fallback
   */
  public async routingDecision(
    userRequest: string,
    context: Record<string, any>
  ): Promise<{
    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';
    confidence: number;
    reasoning: string;
    estimatedTokens: number;
  }> {
    try {
      if (this.instance && this.instance.isAvailable()) {
        return this.instance.routingDecision(userRequest, context);
      }

      // Fallback to simple heuristic
      return {
        targetService: 'ollama',
        confidence: 0.5,
        reasoning: 'LFM2 unavailable, falling back to Ollama',
        estimatedTokens: userRequest.length / 4,
      };
    } catch (error) {
      log.error('‚ùå LFM2 routing decision failed', LogContext.AI, { error });
      return {
        targetService: 'ollama',
        confidence: 0.3,
        reasoning: 'Error in routing, defaulting to Ollama',
        estimatedTokens: userRequest.length / 4,
      };
    }
  }

  shutdown() {
    if (this.instance) {
      this.instance.shutdown();
    }
  }

  // Get circuit breaker health status
  getCircuitBreakerMetrics() {
    return this.circuitBreaker.getMetrics();
  }
}

// Export safe singleton that won't crash on startup
export const lfm2Bridge = new SafeLFM2Bridge();
export default lfm2Bridge;
