/**
 * LFM2 Bridge Service - Fast Local Model Integration
 * Bridges TypeScript services with Python LFM2-1.2B model
 * Optimized for speed and coordination tasks
 */

import type { ChildProcess } from 'child_process';
import { spawn } from 'child_process';
// Fallback implementations for better compatibility
const log = {
  info: (...args: any[]) => console.log('[INFO]', ...args),
  warn: (...args: any[]) => console.warn('[WARN]', ...args), 
  error: (...args: any[]) => console.error('[ERROR]', ...args),
  debug: (...args: any[]) => console.debug('[DEBUG]', ...args)
};

const LogContext = {
  LFM2: 'LFM2',
  AI: 'AI'
};

// Mock circuit breaker for standalone operation
class CircuitBreaker<T = any> {
  constructor(private config: any) {}
  async execute<R>(fn: () => Promise<R>): Promise<R> {
    return fn();
  }
  register() {}
  getMetrics() {
    return { 
      totalRequests: 0, 
      failedRequests: 0, 
      successfulRequests: 0,
      state: 'CLOSED',
      nextRetryAt: null 
    };
  }
}

const CircuitBreakerRegistry = {
  get: (name: string) => new CircuitBreaker({ name }),
  register: (name: string, cb: CircuitBreaker) => {}
};

const THREE_SECOND = 3000;

// Import actual services instead of mocks
let ollamaService: any = null;
let LFM2MLXService: any = null;

// Initialize services dynamically
async function initializeServices() {
  try {
    if (!ollamaService) {
      const { ollamaService: importedOllamaService } = await import('./ollama-service');
      ollamaService = importedOllamaService; // It's already a singleton instance
    }
  } catch (error) {
    log.warn('‚ö†Ô∏è Ollama service not available', LogContext.AI, { error });
    ollamaService = {
      isAvailable: async () => false,
      generateResponse: async () => ({ 
        message: { content: 'Ollama not available' },
        eval_count: 0,
        total_duration: 0
      }),
      getInstance: () => ollamaService
    };
  }

  try {
    if (!LFM2MLXService) {
      const { LFM2MLXService: MLXService } = await import('./lfm2-mlx-service');
      LFM2MLXService = MLXService;
    }
  } catch (error) {
    log.warn('‚ö†Ô∏è LFM2 MLX service not available', LogContext.AI, { error });
    LFM2MLXService = class MockLFM2MLXService {
      static getInstance() {
        return new MockLFM2MLXService();
      }
      
      async isAvailable() {
        return false;
      }
      
      async quickResponse() {
        return { 
          content: 'MLX service not available', 
          confidence: 0.1,
          tokens: 10,
          executionTime: 1,
          model: 'fallback'
        };
      }
      
      async routingDecision() {
        return { 
          targetService: 'ollama',
          confidence: 0.1,
          reasoning: 'MLX not available - using fallback'
        };
      }
    };
  }
}

// Initialize services on module load
initializeServices();

export interface LFM2Request {
  prompt: string;
  systemPrompt?: string;
  maxLength?: number;
  maxTokens?: number;
  temperature?: number;
  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';
}

export interface LFM2Response {
  content: string;
  tokens: number;
  executionTime: number;
  model: string; // Allow different model names for fallback
  confidence?: number;
}

export interface LFM2Metrics {
  avgResponseTime: number;
  totalRequests: number;
  successRate: number;
  tokenThroughput: number;
}

export class LFM2BridgeService {
  private pythonProcess:
    | ChildProcess     | null = null;
  private isInitialized = false;
  private requestQueue: Array<{
    id: string;
    request: LFM2Request;
    resolve: (response: LFM2Response) => void;
    reject: (error: Error) => void;
  }> = [];
  private pendingRequests: Map<
    string,
    {
      resolve: (response: LFM2Response) => void;
      reject: (error: Error) => void;
      startTime: number;
    }
  > = new Map();
  private metrics: LFM2Metrics = {
    avgResponseTime: 0,
    totalRequests: 0,
    successRate: 1.0,
    tokenThroughput: 0,
  };

  constructor() {
    this.initializeLFM2();
  }

  private async initializeLFM2(): Promise<void> {
    try {
      // Check if LFM2 is enabled
      if (process.env.ENABLE_LFM2 === 'false') {
        log.info('üö´ LFM2 disabled via environment variable', LogContext.AI);
        return;
      }

      log.info('üöÄ Initializing LFM2-1.2B bridge service', LogContext.AI);

      // Create Python bridge server
      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;

      this.pythonProcess = spawn('python3', [pythonScript], {
        stdio: ['pipe', 'pipe', 'pipe'],
        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' },
      });

      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {
        throw new Error('Failed to create Python process stdio');
      }

      // Handle responses
      this.pythonProcess.stdout.on('data', (data) => {
        this.handlePythonResponse(data.toString());
      });

      // Handle stderr output (includes Python logging)
      this.pythonProcess.stderr.on('data', (data) => {
        const message = data.toString();
        // Python logging outputs to stderr by default
        // Only log as error if it's actually an error-level message
        if (
          message.includes('ERROR') ||
          message.includes('CRITICAL') ||
          message.includes('Traceback')
        ) {
          log.error('‚ùå LFM2 Python error', LogContext.AI, { error: message });
        } else if (message.includes('WARNING')) {
          log.warn('‚ö†Ô∏è LFM2 Python warning', LogContext.AI, { message });
        } else {
          // INFO and DEBUG messages - don't treat as errors
          log.debug('LFM2 Python output', LogContext.AI, { message });
        }
      });

      // Handle process exit
      this.pythonProcess.on('exit', (code) => {
        log.warn(`‚ö†Ô∏è LFM2 Python process exited with code ${code}`, LogContext.AI);
        this.isInitialized = false;
        this.restartProcess();
      });

      // Wait for initialization
      await new Promise((resolve, reject) => {
        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);

        const checkInit = () => {
          if (this.isInitialized) {
            clearTimeout(timeout);
            resolve(true);
          } else {
            setTimeout(checkInit, 100);
          }
        };
        checkInit();
      });

      log.info('‚úÖ LFM2-1.2B bridge service initialized', LogContext.AI);
    } catch (error) {
      log.error('‚ùå Failed to initialize LFM2 bridge service', LogContext.AI, { error });
      // Fall back to mock implementation
      this.initializeMockLFM2();
    }
  }

  private initializeMockLFM2(): void {
    log.warn('‚ö†Ô∏è Using mock LFM2 implementation for testing', LogContext.AI);
    this.isInitialized = true;
  }

  /**
   * Fast routing decision using LFM2
   */
  public async routingDecision(
    userRequest: string,
    context: Record<string, any>
  ): Promise<{
    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';
    confidence: number;
    reasoning: string;
    estimatedTokens: number;
  }> {
    const prompt = this.createRoutingPrompt(userRequest, context);

    const response = await this.generate({
      prompt,
      maxLength: 200,
      temperature: 0.3,
      taskType: 'routing',
    });

    // Parse LFM2 routing response
    return this.parseRoutingResponse(response.content) as {
      targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';
      confidence: number;
      reasoning: string;
      estimatedTokens: number;
    };
  }

  /**
   * Quick classification and simple Q&A
   */
  public async quickResponse(
    userRequest: string,
    taskType: 'classification' | 'simple_qa' = 'simple_qa'
  ): Promise<LFM2Response> {
    const prompt = this.createQuickResponsePrompt(userRequest, taskType);

    return this.generate({
      prompt,
      maxLength: 150,
      temperature: 0.6,
      taskType,
    });
  }

  /**
   * Coordinate multiple agent tasks
   */
  public async coordinateAgents(
    primaryTask: string,
    supportingTasks: string[]
  ): Promise<{
    execution_plan: {
      primary_priority: number;
      supporting_priorities: number[];
      parallel_execution: boolean;
      estimated_total_time: number;
    };
    resource_allocation: {
      primary_service: string;
      supporting_services: string[];
    };
  }> {
    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);

    const response = await this.generate({
      prompt,
      maxLength: 300,
      temperature: 0.4,
      taskType: 'coordination',
    });

    return this.parseCoordinationResponse(response.content) as {
      execution_plan: {
        primary_priority: number;
        supporting_priorities: number[];
        parallel_execution: boolean;
        estimated_total_time: number;
      };
      resource_allocation: {
        primary_service: string;
        supporting_services: string[];
      };
    };
  }

  /**
   * Core generation method
   */
  public async generate(request: LFM2Request): Promise<LFM2Response> {
    if (!this.isInitialized) {
      return this.generateMockResponse(request);
    }

    const requestId = this.generateRequestId();
    const startTime = Date.now();

    return new Promise((resolve, reject) => {
      this.pendingRequests.set(requestId, { resolve, reject, startTime });

      // Send request to Python process with correct format
      const pythonRequest = {
        type:
          request.taskType === 'routing' || request.taskType === 'coordination'
            ? request.taskType
            : 'completion',
        requestId,
        prompt: request.prompt,
        maxTokens: request.maxTokens || request.maxLength || 512,
        temperature: request.temperature || 0.7,
      };

      if (this.pythonProcess && this.pythonProcess.stdin) {
        this.pythonProcess.stdin.write(`${JSON.stringify(pythonRequest)}\n`);
      } else {
        reject(new Error('Python process not available'));
      }

      // Timeout after 10 seconds
      setTimeout(() => {
        if (this.pendingRequests.has(requestId)) {
          this.pendingRequests.delete(requestId);
          reject(new Error('LFM2 request timeout'));
        }
      }, 10000);
    });
  }

  /**
   * Batch processing for efficiency
   */
  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {
    log.info('üì¶ Processing LFM2 batch request', LogContext.AI, { count: requests.length });

    // Process requests in parallel but limit concurrency
    const batchSize = 3;
    const results: LFM2Response[] = [];

    for (let i = 0; i < requests.length; i += batchSize) {
      const batch = requests.slice(i, i + batchSize);
      const batchResults = await Promise.all(batch.map((request) => this.generate(request)));
      results.push(...batchResults);
    }

    return results;
  }

  private handlePythonResponse(data: string): void {
    const lines = data.trim().split('\n');

    for (const line of lines) {
      if (line === 'INITIALIZED') {
        this.isInitialized = true;
        continue;
      }

      try {
        const response = JSON.parse(line);

        // Handle Python server response format
        if (response.requestId && this.pendingRequests.has(response.requestId)) {
          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;
          this.pendingRequests.delete(response.requestId);

          const executionTime = response.processingTime || Date.now() - startTime;

          if (response.success) {
            const content = response.text || response.strategy || response.category || '';

            // Update metrics
            this.updateMetrics(executionTime, content.length);

            resolve({
              content,
              tokens: Math.ceil(content.length / 4),
              executionTime,
              model: response.model || 'lfm2-1.2b',
              confidence: response.confidence,
            });
          } else {
            reject(new Error(response.error || 'LFM2 processing failed'));
          }
        }
      } catch (error) {
        log.error('‚ùå Failed to parse LFM2 response', LogContext.AI, { error, data: line });
      }
    }
  }

  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {
    return `TASK: Route this request to the best service.

REQUEST: "${userRequest}"
CONTEXT: ${JSON.stringify(context)}

SERVICES:
1. lfm2 - Use for: Simple questions, basic math, quick facts
2. lm-studio - Use for: Code writing, programming, technical documentation  
3. ollama - Use for: General conversation, explanations, medium tasks
4. openai - Use for: Creative writing, complex analysis, brainstorming
5. anthropic - Use for: Research, detailed analysis, long documents

RULES:
- If request contains: "code", "function", "programming", "script" ‚Üí choose "lm-studio"  
- If request contains: "creative", "story", "poem", "design" ‚Üí choose "openai"
- If request contains: "research", "analyze", "detailed", "comprehensive" ‚Üí choose "anthropic"
- For simple questions, math, facts ‚Üí choose "lfm2"
- Otherwise ‚Üí choose "ollama"

Respond EXACTLY:
{"service": "SERVICE_NAME", "confidence": 0.9, "reasoning": "brief reason", "tokens": 100}`;
  }

  private createQuickResponsePrompt(userRequest: string, taskType: string): string {
    const taskInstructions = {
      classification: 'Classify this request into categories and respond briefly.',
      simple_qa: 'Answer this question quickly and concisely.',
    };

    return `${taskInstructions[taskType as keyof typeof taskInstructions]}

REQUEST: "${userRequest}"

Response:`;
  }

  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {
    return `AGENT COORDINATION PLAN:

PRIMARY TASK: "${primaryTask}"
SUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i + 1}. "${task}"`).join(', ')}

Create execution plan with priorities, resource allocation, and timing.

Respond with JSON:
{
  "execution_plan": {
    "primary_priority": 1-5,
    "supporting_priorities": [1-5, ...],
    "parallel_execution": boolean,
    "estimated_total_time": seconds
  },
  "resource_allocation": {
    "primary_service": "service_name",
    "supporting_services": ["service1", "service2", ...]
  }
}`;
  }

  private parseRoutingResponse(content: string): unknown {
    try {
      const jsonMatch = content.match(/{.*}/s);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        return {
          targetService: parsed.service || 'lm-studio', // Changed default to lm-studio
          confidence: parsed.confidence || 0.7,
          reasoning: parsed.reasoning || 'Automatic routing decision',
          estimatedTokens: parsed.tokens || 100,
        };
      }
    } catch (error) {
      log.warn('‚ö†Ô∏è Failed to parse LFM2 routing response', LogContext.AI);
    }

    // Fallback parsing - better distribution
    const fallbackServices = ['lm-studio', 'ollama', 'openai', 'anthropic'] as const;
    const randomService = fallbackServices[Math.floor(Math.random() * fallbackServices.length)];
    return {
      targetService: randomService,
      confidence: 0.5,
      reasoning: 'Fallback routing due to parsing error',
      estimatedTokens: 100,
    };
  }

  private parseCoordinationResponse(content: string): unknown {
    try {
      const jsonMatch = content.match(/{.*}/s);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0]);
      }
    } catch (error) {
      log.warn('‚ö†Ô∏è Failed to parse LFM2 coordination response', LogContext.AI);
    }

    // Fallback coordination plan
    return {
      execution_plan: {
        primary_priority: 1,
        supporting_priorities: [2, 3],
        parallel_execution: true,
        estimated_total_time: 30,
      },
      resource_allocation: {
        primary_service: 'ollama',
        supporting_services: ['lfm2', 'ollama'],
      },
    };
  }

  private generateMockResponse(request: LFM2Request): LFM2Response {
    // Fast mock response for development/testing
    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;

    return {
      content: mockContent,
      tokens: Math.ceil(mockContent.length / 4),
      executionTime: 50 + Math.random() * 100, // 50-150ms
      model: 'LFM2-1.2B',
      confidence: 0.8,
    };
  }

  private generateRequestId(): string {
    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private updateMetrics(executionTime: number, responseLength: number): void {
    this.metrics.totalRequests++;

    // Exponential moving average
    const alpha = 0.1;
    this.metrics.avgResponseTime =
      alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;

    // Token throughput (tokens per second)
    const tokens = Math.ceil(responseLength / 4);
    const throughput = tokens / (executionTime / 1000);
    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;
  }

  private async restartProcess(): Promise<void> {
    log.info('üîÑ Restarting LFM2 bridge service', LogContext.AI);

    if (this.pythonProcess) {
      this.pythonProcess.kill();
      this.pythonProcess = null;
    }

    // Wait a bit before restarting
    await new Promise((resolve) => setTimeout(resolve, 2000));

    this.initializeLFM2();
  }

  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {
    return {
      ...this.metrics,
      isInitialized: this.isInitialized,
    };
  }

  public isAvailable(): boolean {
    return this.isInitialized && this.pythonProcess !== null;
  }

  public async shutdown(): Promise<void> {
    log.info('üõë Shutting down LFM2 bridge service', LogContext.AI);

    if (this.pythonProcess) {
      this.pythonProcess.kill();
      this.pythonProcess = null;
    }

    this.isInitialized = false;
  }
}

// Create singleton with lazy initialization and circuit breaker protection
class SafeLFM2Bridge {
  private instance: LFM2BridgeService | null = null;
  private mlxService: any | null = null;
  private initAttempted = false;
  private circuitBreaker: CircuitBreaker;

  constructor() {
    // Create circuit breaker with optimized settings
    this.circuitBreaker = new CircuitBreaker({
      failureThreshold: 3,
      successThreshold: 2,
      timeout: 30000, // 30 seconds
      errorThresholdPercentage: 60,
      volumeThreshold: 5,
    });

    // Register for monitoring
    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);
    
    // Try to initialize MLX service
    this.initializeMLX();
  }
  
  private async initializeMLX(): Promise<void> {
    try {
      // Wait for services to be initialized
      await initializeServices();
      if (LFM2MLXService) {
        this.mlxService = LFM2MLXService.getInstance();
        log.info('üçé MLX service initialized for LFM2', LogContext.AI);
      }
    } catch (error) {
      log.warn('‚ö†Ô∏è MLX service initialization failed', LogContext.AI, { error });
    }
  }

  async quickResponse(
    userRequest: string,
    taskType: 'classification' | 'simple_qa' = 'simple_qa'
  ): Promise<LFM2Response> {
    return this.circuitBreaker.execute(async () => {
      // Try MLX first if available
      if (this.mlxService && await this.mlxService.isAvailable()) {
        try {
          log.info('üçé Using MLX for LFM2 request', LogContext.AI);
          const response = await this.mlxService.quickResponse(userRequest, taskType);
          return response;
        } catch (error) {
          log.warn('‚ö†Ô∏è MLX request failed, falling back', LogContext.AI, { error });
        }
      }
      
      // Fall back to Python bridge
      if (!this.initAttempted && !this.instance) {
        this.initAttempted = true;
        
        // Check if LFM2 is enabled before attempting initialization
        if (process.env.ENABLE_LFM2 === 'false') {
          log.info('üö´ LFM2 disabled, using fallback', LogContext.AI);
          return this.createFallbackResponse(userRequest);
        }
        
        try {
          this.instance = new LFM2BridgeService();
          log.info('‚úÖ LFM2 bridge initialized successfully', LogContext.AI);
        } catch (error) {
          log.warn('‚ö†Ô∏è LFM2 bridge initialization failed, using fallback', LogContext.AI, {
            error,
          });
          return this.createFallbackResponse(userRequest);
        }
      }

      if (this.instance) {
        return this.instance.quickResponse(userRequest, taskType);
      } else {
        return this.createFallbackResponse(userRequest);
      }
    });
  }

  async execute(request: LFM2Request): Promise<LFM2Response> {
    // Use circuit breaker for resilient execution
    return this.circuitBreaker.execute(
      async () => {
        // Try to initialize LFM2 if not attempted
        if (!this.initAttempted && !this.instance) {
          this.initAttempted = true;
          
          // Check if LFM2 is enabled before attempting initialization
          if (process.env.ENABLE_LFM2 === 'false') {
            log.info('üö´ LFM2 disabled, using fallback for execute', LogContext.AI);
            return this.createFallbackResponse(request.prompt);
          }
          
          try {
            this.instance = new LFM2BridgeService();
            log.info('‚úÖ LFM2 bridge initialized successfully', LogContext.AI);
          } catch (error) {
            log.warn('‚ö†Ô∏è LFM2 bridge initialization failed, using fallback', LogContext.AI, {
              error: error instanceof Error ? error.message : String(error),
            });
          }
        }

        // Try native LFM2 if available
        if (this.instance && this.instance.isAvailable()) {
          return this.instance.generate(request);
        }

        // Use Ollama as primary fallback
        const messages = [
            ...(request.systemPrompt
              ? [{ role: 'system' as const, content: request.systemPrompt }]
              : []),
            { role: 'user' as const, content: request.prompt },
          ];

        const response = await ollamaService.generateResponse(
          [{ role: 'user', content: request.prompt }],
          'llama3.2:3b',
          {
            temperature: request.temperature || 0.1,
            max_tokens: request.maxTokens || 100,
          }
        );

        return {
          content: response.message.content,
          tokens: response.eval_count || 50,
          executionTime: (response.total_duration || 100000000) / 1000000,
          model: 'llama3.2:3b (LFM2 fallback)',
          confidence: 0.85,
        };
      }
    );
  }

  private createFallbackResponse(userRequest: string): LFM2Response {
    return {
      content: `I understand you're asking about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,
      tokens: 25,
      executionTime: 1,
      model: 'fallback-response',
      confidence: 0.4,
    };
  }

  isAvailable(): boolean {
    return this.instance?.isAvailable() || true; // Always available with fallback
  }

  getMetrics() {
    if (this.instance) {
      return this.instance.getMetrics();
    }
    return {
      avgResponseTime: 100,
      totalRequests: 0,
      successRate: 1.0,
      tokenThroughput: 500,
    };
  }

  /**
   * Fast routing decision using LFM2 with fallback
   */
  public async routingDecision(
    userRequest: string,
    context: Record<string, any>
  ): Promise<{
    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';
    confidence: number;
    reasoning: string;
    estimatedTokens: number;
  }> {
    try {
      // Try MLX first if available
      if (this.mlxService && await this.mlxService.isAvailable()) {
        try {
          log.info('üçé Using MLX for routing decision', LogContext.AI);
          const mlxResult = await this.mlxService.routingDecision(userRequest, context);
          return {
            targetService: mlxResult.targetService as any,
            confidence: mlxResult.confidence,
            reasoning: mlxResult.reasoning,
            estimatedTokens: userRequest.length / 4
          };
        } catch (error) {
          log.warn('‚ö†Ô∏è MLX routing failed, falling back', LogContext.AI, { error });
        }
      }
      
      // Fall back to Python bridge
      if (this.instance && this.instance.isAvailable()) {
        return this.instance.routingDecision(userRequest, context);
      }

      // Use HRM for complex routing decisions, deterministic for simple ones
      const useHRM = process.env.ENABLE_HRM_ROUTING !== 'false' && userRequest.length > 100;
      
      if (useHRM) {
        log.info('üß† Using HRM for complex routing decision', LogContext.AI);
        return this.hrmRouting(userRequest, context);
      } else {
        log.info('‚ö° Using deterministic routing for fast decision', LogContext.AI);
        return this.deterministicRouting(userRequest, context);
      }
    } catch (error) {
      log.error('‚ùå LFM2 routing decision failed', LogContext.AI, { error });
      return this.deterministicRouting(userRequest, context);
    }
  }

  /**
   * Deterministic routing based on keywords and patterns
   * More reliable than LLM-based routing for service selection
   */
  private deterministicRouting(
    userRequest: string, 
    context: Record<string, any>
  ): {
    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';
    confidence: number;
    reasoning: string;
    estimatedTokens: number;
  } {
    const request = userRequest.toLowerCase();
    const estimatedTokens = Math.max(50, userRequest.length / 4);
    
    // Technical/Programming keywords ‚Üí LM Studio
    const techKeywords = [
      'code', 'function', 'programming', 'script', 'javascript', 'typescript', 'python', 
      'react', 'node', 'api', 'database', 'sql', 'git', 'debug', 'compile', 'algorithm',
      'class', 'method', 'variable', 'array', 'object', 'npm', 'package', 'library',
      'framework', 'backend', 'frontend', 'server', 'client', 'http', 'rest', 'graphql'
    ];
    
    // Creative keywords ‚Üí OpenAI
    const creativeKeywords = [
      'creative', 'story', 'poem', 'design', 'art', 'music', 'writing', 'brainstorm', 
      'imagine', 'idea', 'concept', 'innovative', 'original', 'artistic', 'creative writing',
      'narrative', 'character', 'plot', 'fiction', 'novel', 'essay', 'blog'
    ];
    
    // Research/Analysis keywords ‚Üí Anthropic
    const researchKeywords = [
      'research', 'analyze', 'detailed', 'comprehensive', 'study', 'investigate', 
      'academic', 'scientific', 'evaluation', 'comparison', 'assessment', 'review',
      'thesis', 'paper', 'report', 'analysis', 'examination', 'survey', 'methodology'
    ];
    
    // Simple/Quick keywords ‚Üí LFM2
    const simpleKeywords = [
      'what is', 'who is', 'when is', 'where is', 'how much', 'quick', 'simple',
      'math', 'calculate', 'convert', 'translate', 'define', 'explain briefly'
    ];
    
    // Check for technical content
    if (techKeywords.some(keyword => request.includes(keyword))) {
      return {
        targetService: 'lm-studio',
        confidence: 0.9,
        reasoning: `Technical/programming content detected: ${techKeywords.find(k => request.includes(k))}`,
        estimatedTokens
      };
    }
    
    // Check for creative content
    if (creativeKeywords.some(keyword => request.includes(keyword))) {
      return {
        targetService: 'openai',
        confidence: 0.85,
        reasoning: `Creative content detected: ${creativeKeywords.find(k => request.includes(k))}`,
        estimatedTokens
      };
    }
    
    // Check for research/analysis content
    if (researchKeywords.some(keyword => request.includes(keyword))) {
      return {
        targetService: 'anthropic',
        confidence: 0.88,
        reasoning: `Research/analysis content detected: ${researchKeywords.find(k => request.includes(k))}`,
        estimatedTokens
      };
    }
    
    // Check for simple questions
    if (simpleKeywords.some(keyword => request.includes(keyword)) || request.length < 50) {
      return {
        targetService: 'lfm2',
        confidence: 0.8,
        reasoning: `Simple/quick question detected - suitable for fast local processing`,
        estimatedTokens: Math.min(estimatedTokens, 100)
      };
    }
    
    // Context-based routing
    if (context.taskType === 'code_generation' || context.complexity === 'technical') {
      return {
        targetService: 'lm-studio',
        confidence: 0.75,
        reasoning: 'Context indicates technical task',
        estimatedTokens
      };
    }
    
    // Length-based routing for medium complexity
    if (userRequest.length > 200) {
      return {
        targetService: 'anthropic',
        confidence: 0.7,
        reasoning: 'Long request suggests need for detailed analysis',
        estimatedTokens
      };
    }
    
    // Default to balanced distribution between Ollama and LM Studio
    // Alternate to ensure better distribution
    const timestamp = Date.now();
    const useOllama = (timestamp % 2) === 0;
    
    return {
      targetService: useOllama ? 'ollama' : 'lm-studio',
      confidence: 0.65,
      reasoning: `Default routing with balanced distribution (${useOllama ? 'ollama' : 'lm-studio'})`,
      estimatedTokens
    };
  }

  /**
   * Use HRM model for sophisticated routing decisions
   * Falls back to deterministic routing if HRM is not available
   */
  private async hrmRouting(
    userRequest: string,
    context: Record<string, any>
  ): Promise<{
    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';
    confidence: number;
    reasoning: string;
    estimatedTokens: number;
  }> {
    try {
      // Check if we have access to a high-resolution model for routing
      // This could be a larger model like Llama 3.2:8b or 70b for complex decisions
      
      const hrmPrompt = `You are an expert AI service router. Analyze this request and choose the optimal service.

REQUEST: "${userRequest}"
CONTEXT: ${JSON.stringify(context)}

SERVICE CAPABILITIES:
1. lfm2 - Ultra-fast local model (1.2B params)
   - Best for: Simple Q&A, basic math, quick facts, classifications
   - Speed: <100ms, Tokens: <150
   
2. lm-studio - Code specialist (7B+ params)  
   - Best for: Programming, technical docs, debugging, algorithms
   - Speed: 1-3s, Tokens: unlimited
   
3. ollama - General purpose (3B-8B params)
   - Best for: Conversations, explanations, general knowledge
   - Speed: 500ms-2s, Tokens: 2000+
   
4. openai - Creative powerhouse (GPT models)
   - Best for: Creative writing, brainstorming, complex reasoning
   - Speed: 1-5s, Tokens: 4000+, Cost: High
   
5. anthropic - Analysis expert (Claude models)
   - Best for: Research, detailed analysis, long-form content
   - Speed: 2-8s, Tokens: 8000+, Cost: High

ROUTING CRITERIA:
- Complexity: Simple ‚Üí lfm2, Medium ‚Üí ollama/lm-studio, Complex ‚Üí openai/anthropic  
- Domain: Code ‚Üí lm-studio, Creative ‚Üí openai, Research ‚Üí anthropic
- Speed requirement: Urgent ‚Üí lfm2/ollama, Standard ‚Üí any, Deep analysis ‚Üí anthropic
- Token requirement: Short ‚Üí lfm2, Medium ‚Üí ollama, Long ‚Üí anthropic

Respond with ONLY this JSON format:
{
  "service": "SERVICE_NAME",
  "confidence": 0.XX,  
  "reasoning": "Brief explanation of choice",
  "estimated_tokens": NUMBER
}`;

      // Try to use a larger model for routing decisions
      // This could be routed through the LLM router with a high-capability model
      const { llmRouter } = await import('./llm-router-service');
      
      const routingResponse = await llmRouter.generateResponse('planner-pro', [
        { role: 'user', content: hrmPrompt }
      ]);
      
      // Parse the HRM response
      const content = routingResponse.content;
      const jsonMatch = content.match(/{[\s\S]*}/);
      
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        
        return {
          targetService: parsed.service || 'ollama',
          confidence: Math.max(0.7, parsed.confidence || 0.8),
          reasoning: `HRM routing: ${parsed.reasoning}`,
          estimatedTokens: parsed.estimated_tokens || userRequest.length / 4
        };
      }
      
      throw new Error('Failed to parse HRM routing response');
      
    } catch (error) {
      log.warn('‚ö†Ô∏è HRM routing failed, using deterministic fallback', LogContext.AI, { error });
      return this.deterministicRouting(userRequest, context);
    }
  }

  shutdown() {
    if (this.instance) {
      this.instance.shutdown();
    }
  }

  // Get circuit breaker health status
  getCircuitBreakerMetrics() {
    return this.circuitBreaker.getMetrics();
  }
}

// Export safe singleton that won't crash on startup
export const lfm2Bridge = new SafeLFM2Bridge();
export default lfm2Bridge;
