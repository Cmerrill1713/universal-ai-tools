/**
 * Speculative Decoding Service
 * Implements PEARL (Parallel Speculative Decoding) for 2-4x inference speedup
 * Uses draft models to predict tokens, verified by primary models
 */

import { LogContext, log } from '@/utils/logger';
import { mlxService } from './mlx-service';
import { lfm2Bridge } from './lfm2-bridge';
import { ollamaService } from './ollama-service';
import { modelTierManager, ModelTier, ModelMetadata } from './model-tier-manager';
import { intelligentParameterService } from './intelligent-parameter-service';
import { performanceMonitor } from './performance-monitor';

export interface SpeculativeDecodingConfig {
  draftModelTier: ModelTier;
  targetModelTier: ModelTier;
  draftLength: number; // Number of tokens to speculate
  adaptiveDraftLength: boolean;
  parallelSpeculation: boolean;
  maxSpeculationDepth: number;
  verificationThreshold: number;
  enableFallback: boolean;
}

export interface SpeculationResult {
  tokens: string[];
  acceptanceRate: number;
  speedup: number;
  draftTime: number;
  verificationTime: number;
  totalTime: number;
}

export interface DraftToken {
  token: string;
  logprob: number;
  alternatives: Array<{ token: string; logprob: number }>;
}

export class SpeculativeDecodingService {
  private config: SpeculativeDecodingConfig;
  private draftModel?: ModelMetadata;
  private targetModel?: ModelMetadata;
  private acceptanceHistory: number[] = [];
  private currentDraftLength: number;
  private isInitialized = false;

  constructor(config?: Partial<SpeculativeDecodingConfig>) {
    this.config = {
      draftModelTier: ModelTier.ULTRA_FAST,
      targetModelTier: ModelTier.BALANCED,
      draftLength: 4,
      adaptiveDraftLength: true,
      parallelSpeculation: true,
      maxSpeculationDepth: 8,
      verificationThreshold: 0.9,
      enableFallback: true,
      ...config
    };
    
    this.currentDraftLength = this.config.draftLength;
  }

  /**
   * Initialize speculative decoding with model selection
   */
  async initialize(): Promise<void> {
    try {
      log.info('üöÄ Initializing Speculative Decoding Service', LogContext.AI);

      // Get best models for draft and target tiers
      this.draftModel = modelTierManager.getBestModelForTier(this.config.draftModelTier);
      this.targetModel = modelTierManager.getBestModelForTier(this.config.targetModelTier);

      if (!this.draftModel || !this.targetModel) {
        throw new Error('Required models not available for speculative decoding');
      }

      log.info('‚úÖ Speculative decoding initialized', LogContext.AI, {
        draftModel: this.draftModel.name,
        targetModel: this.targetModel.name,
        draftLength: this.config.draftLength
      });

      this.isInitialized = true;
    } catch (error) {
      log.error('‚ùå Failed to initialize speculative decoding', LogContext.AI, { error });
      throw error;
    }
  }

  /**
   * Generate text using speculative decoding
   */
  async generate(
    prompt: string,
    maxTokens: number = 100,
    temperature: number = 0.7
  ): Promise<SpeculationResult> {
    if (!this.isInitialized) {
      await this.initialize();
    }

    const startTime = Date.now();
    const result: SpeculationResult = {
      tokens: [],
      acceptanceRate: 0,
      speedup: 1,
      draftTime: 0,
      verificationTime: 0,
      totalTime: 0
    };

    try {
      let currentPrompt = prompt;
      let tokensGenerated = 0;
      let acceptedTokens = 0;
      let totalDraftTokens = 0;

      while (tokensGenerated < maxTokens) {
        // Adaptive draft length based on acceptance history
        if (this.config.adaptiveDraftLength) {
          this.currentDraftLength = this.calculateAdaptiveDraftLength();
        }

        // Generate draft tokens
        const draftStartTime = Date.now();
        const draftTokens = await this.generateDraftTokens(
          currentPrompt,
          Math.min(this.currentDraftLength, maxTokens - tokensGenerated),
          temperature
        );
        result.draftTime += Date.now() - draftStartTime;
        totalDraftTokens += draftTokens.length;

        // Verify draft tokens with target model
        const verifyStartTime = Date.now();
        const verifiedTokens = await this.verifyDraftTokens(
          currentPrompt,
          draftTokens,
          temperature
        );
        result.verificationTime += Date.now() - verifyStartTime;

        // Update statistics
        acceptedTokens += verifiedTokens.acceptedCount;
        this.updateAcceptanceHistory(verifiedTokens.acceptanceRate);

        // Add accepted tokens to result
        result.tokens.push(...verifiedTokens.tokens);
        tokensGenerated += verifiedTokens.tokens.length;

        // Update prompt for next iteration
        currentPrompt += verifiedTokens.tokens.join('');

        // If no tokens were accepted, fall back to standard generation
        if (verifiedTokens.acceptedCount === 0 && this.config.enableFallback) {
          const fallbackToken = await this.generateSingleToken(currentPrompt, temperature);
          result.tokens.push(fallbackToken);
          tokensGenerated++;
          currentPrompt += fallbackToken;
        }
      }

      // Calculate final metrics
      result.totalTime = Date.now() - startTime;
      result.acceptanceRate = totalDraftTokens > 0 ? acceptedTokens / totalDraftTokens : 0;
      result.speedup = this.calculateSpeedup(result);

      log.info('‚úÖ Speculative decoding completed', LogContext.AI, {
        tokensGenerated: result.tokens.length,
        acceptanceRate: Math.round(result.acceptanceRate * 100) + '%',
        speedup: result.speedup.toFixed(2) + 'x',
        totalTime: result.totalTime + 'ms'
      });

      return result;
    } catch (error) {
      log.error('‚ùå Speculative decoding failed', LogContext.AI, { error });
      throw error;
    }
  }

  /**
   * Generate draft tokens using the smaller model
   */
  private async generateDraftTokens(
    prompt: string,
    count: number,
    temperature: number
  ): Promise<DraftToken[]> {
    if (!this.draftModel) {
      throw new Error('Draft model not initialized');
    }

    try {
      // Use LFM2 for ultra-fast draft generation if available
      if (this.draftModel.id.includes('lfm2') && lfm2Bridge.isAvailable()) {
        const response = await lfm2Bridge.quickResponse(prompt, 'simple_qa');
        // Parse tokens from response (simplified for now)
        return this.parseTokensFromResponse(response.content, count);
      }

      // Use MLX for draft generation if available
      if (this.draftModel.format === 'mlx' && mlxService.isAvailable()) {
        const response = await mlxService.inference({
          modelPath: this.draftModel.location,
          prompt,
          parameters: {
            maxTokens: count,
            temperature,
            rawPrompt: true
          }
        });
        return this.parseTokensFromResponse(response.text, count);
      }

      // Fallback to Ollama
      const response = await ollamaService.generateResponse(
        [{ role: 'user', content: prompt }],
        this.draftModel.name,
        {
          temperature,
          max_tokens: count,
          stream: false
        }
      );
      return this.parseTokensFromResponse(response.message.content, count);

    } catch (error) {
      log.warn('‚ö†Ô∏è Draft generation failed, using empty draft', LogContext.AI, { error });
      return [];
    }
  }

  /**
   * Verify draft tokens with the target model
   */
  private async verifyDraftTokens(
    prompt: string,
    draftTokens: DraftToken[],
    temperature: number
  ): Promise<{
    tokens: string[];
    acceptedCount: number;
    acceptanceRate: number;
  }> {
    if (!this.targetModel || draftTokens.length === 0) {
      return { tokens: [], acceptedCount: 0, acceptanceRate: 0 };
    }

    const acceptedTokens: string[] = [];
    let currentPrompt = prompt;

    // Parallel verification if enabled
    if (this.config.parallelSpeculation) {
      // Create multiple verification prompts
      const verificationPrompts = draftTokens.map((_, index) => {
        const partialTokens = draftTokens.slice(0, index + 1).map(t => t.token);
        return prompt + partialTokens.join('');
      });

      // Verify all at once (simplified for now)
      const verificationResults = await Promise.all(
        verificationPrompts.map(p => this.verifyToken(p, temperature))
      );

      // Check which tokens match
      for (let i = 0; i < draftTokens.length; i++) {
        if (this.tokensMatch(draftTokens[i].token, verificationResults[i])) {
          acceptedTokens.push(draftTokens[i].token);
        } else {
          // Stop at first mismatch
          break;
        }
      }
    } else {
      // Sequential verification
      for (const draftToken of draftTokens) {
        const verifiedToken = await this.verifyToken(currentPrompt, temperature);
        
        if (this.tokensMatch(draftToken.token, verifiedToken)) {
          acceptedTokens.push(draftToken.token);
          currentPrompt += draftToken.token;
        } else {
          // Add the correct token from target model
          acceptedTokens.push(verifiedToken);
          break;
        }
      }
    }

    return {
      tokens: acceptedTokens,
      acceptedCount: acceptedTokens.filter((t, i) => 
        i < draftTokens.length && t === draftTokens[i].token
      ).length,
      acceptanceRate: acceptedTokens.length > 0 
        ? acceptedTokens.filter((t, i) => 
            i < draftTokens.length && t === draftTokens[i].token
          ).length / draftTokens.length
        : 0
    };
  }

  /**
   * Verify a single token with target model
   */
  private async verifyToken(prompt: string, temperature: number): Promise<string> {
    if (!this.targetModel) {
      throw new Error('Target model not initialized');
    }

    // Use appropriate service based on model format
    if (this.targetModel.format === 'mlx' && mlxService.isAvailable()) {
      const response = await mlxService.inference({
        modelPath: this.targetModel.location,
        prompt,
        parameters: {
          maxTokens: 1,
          temperature,
          rawPrompt: true
        }
      });
      return this.extractFirstToken(response.text);
    }

    // Fallback to Ollama
    const response = await ollamaService.generateResponse(
      [{ role: 'user', content: prompt }],
      this.targetModel.name,
      {
        temperature,
        max_tokens: 1,
        stream: false
      }
    );
    return this.extractFirstToken(response.message.content);
  }

  /**
   * Generate a single token (fallback)
   */
  private async generateSingleToken(prompt: string, temperature: number): Promise<string> {
    return this.verifyToken(prompt, temperature);
  }

  /**
   * Calculate adaptive draft length based on acceptance history
   */
  private calculateAdaptiveDraftLength(): number {
    if (this.acceptanceHistory.length === 0) {
      return this.config.draftLength;
    }

    const recentRate = this.acceptanceHistory.slice(-10).reduce((a, b) => a + b, 0) / 
                       Math.min(this.acceptanceHistory.length, 10);

    // Adjust draft length based on acceptance rate
    if (recentRate > 0.9) {
      // High acceptance, increase draft length
      return Math.min(this.config.maxSpeculationDepth, this.currentDraftLength + 1);
    } else if (recentRate < 0.5) {
      // Low acceptance, decrease draft length
      return Math.max(1, this.currentDraftLength - 1);
    }

    return this.currentDraftLength;
  }

  /**
   * Update acceptance history
   */
  private updateAcceptanceHistory(rate: number): void {
    this.acceptanceHistory.push(rate);
    if (this.acceptanceHistory.length > 100) {
      this.acceptanceHistory.shift();
    }
  }

  /**
   * Calculate speedup from speculative decoding
   */
  private calculateSpeedup(result: SpeculationResult): number {
    // Theoretical speedup based on acceptance rate and draft cost
    const draftCost = 0.2; // Draft model is ~5x faster
    const effectiveSpeedup = 1 / (1 - result.acceptanceRate * (1 - draftCost));
    
    // Actual speedup based on timing
    const baselineTime = result.verificationTime / result.acceptanceRate;
    const actualSpeedup = baselineTime / result.totalTime;
    
    // Return conservative estimate
    return Math.min(effectiveSpeedup, actualSpeedup);
  }

  /**
   * Parse tokens from response text
   */
  private parseTokensFromResponse(text: string, maxCount: number): DraftToken[] {
    // Simple word-based tokenization for now
    const words = text.split(/\s+/).filter(w => w.length > 0).slice(0, maxCount);
    return words.map(word => ({
      token: word + ' ',
      logprob: -1, // Placeholder
      alternatives: []
    }));
  }

  /**
   * Extract first token from text
   */
  private extractFirstToken(text: string): string {
    const match = text.match(/\S+\s*/);
    return match ? match[0] : '';
  }

  /**
   * Check if tokens match (with normalization)
   */
  private tokensMatch(draft: string, verified: string): boolean {
    return draft.trim().toLowerCase() === verified.trim().toLowerCase();
  }

  /**
   * Get current statistics
   */
  getStatistics(): {
    isInitialized: boolean;
    draftModel: string | null;
    targetModel: string | null;
    currentDraftLength: number;
    averageAcceptanceRate: number;
    totalSpeculations: number;
  } {
    return {
      isInitialized: this.isInitialized,
      draftModel: this.draftModel?.name || null,
      targetModel: this.targetModel?.name || null,
      currentDraftLength: this.currentDraftLength,
      averageAcceptanceRate: this.acceptanceHistory.length > 0
        ? this.acceptanceHistory.reduce((a, b) => a + b, 0) / this.acceptanceHistory.length
        : 0,
      totalSpeculations: this.acceptanceHistory.length
    };
  }
}

// Singleton instance
export const speculativeDecodingService = new SpeculativeDecodingService();