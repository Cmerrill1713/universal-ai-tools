import type { Next.Function, Request, Response } from 'express';
import { Log.Context, logger } from './utils/enhanced-logger';
export interface Fallback.Performance.Options {;
  slow.Request.Threshold?: number;
  request.Timeout.Ms?: number;
  max.Metrics.History?: number;
  enable.Request.Timing?: boolean;
};
interface Request.Metric {;
  url: string,;
  method: string,;
  status.Code: number,;
  response.Time: number,;
  timestamp: number,;
  user.Agent?: string;
  ip?: string;
};
interface Rate.Limit.Entry {;
  count: number,;
  reset.Time: number,;
}/**;
 * Lightweight performance middleware that works without Redis* Uses in-memory storage with automatic cleanup*/
export class Fallback.Performance.Middleware {;
  private request.Metrics: Request.Metric[] = [],;
  private rate.Limit.Map = new Map<string, Rate.Limit.Entry>();
  private options: Required<Fallback.Performance.Options>;
  private cleanup.Interval: NodeJ.S.Timeout,;
  private metrics.Cleanup.Interval = 300000// 5 minutes;
  constructor(options: Fallback.Performance.Options = {}) {;
    thisoptions = {;
      slow.Request.Threshold: optionsslow.Request.Threshold ?? 2000,;
      request.Timeout.Ms: optionsrequest.Timeout.Ms ?? 5000, // 5 second max as requested;
      max.Metrics.History: optionsmax.Metrics.History ?? 5000,;
      enable.Request.Timing: optionsenable.Request.Timing ?? true,;
    }// Start cleanup interval;
    thiscleanup.Interval = set.Interval(() => {;
      thiscleanup.Old.Metrics();
      thiscleanup.Rate.Limits()}, this.metrics.Cleanup.Interval);
    loggerinfo('Fallback performance middleware initialized', LogContextPERFORMAN.C.E, {;
      options: thisoptions}),;

  private cleanup.Old.Metrics(): void {;
    const one.Hour.Ago = Date.now() - 3600000;
    const before.Cleanup = thisrequest.Metricslength// Remove metrics older than 1 hour;
    thisrequest.Metrics = thisrequest.Metricsfilter((m) => mtimestamp > one.Hour.Ago)// Keep only the most recent metrics if exceeding max;
    if (thisrequest.Metricslength > thisoptionsmax.Metrics.History) {;
      thisrequest.Metrics = thisrequest.Metricsslice(-thisoptionsmax.Metrics.History);
}    const removed = before.Cleanup - thisrequest.Metricslength;
    if (removed > 0) {;
      loggerdebug(`Cleaned up ${removed} old metrics`, LogContextPERFORMAN.C.E)};

  private cleanup.Rate.Limits(): void {;
    const now = Date.now();
    let cleaned = 0;
    for (const [key, entry] of thisrate.Limit.Mapentries()) {;
      if (now > entryreset.Time) {;
        thisrate.Limit.Mapdelete(key);
        cleaned++};

    if (cleaned > 0) {;
      loggerdebug(`Cleaned up ${cleaned} expired rate limit entries`, LogContextPERFORMAN.C.E)}}/**;
   * Request timing middleware with timeout protection*/
  public request.Timer() {;
    return (req: Request, res: Response, next: Next.Function) => {;
      if (!thisoptionsenable.Request.Timing) {;
        return next();

      const start.Time = processhrtimebigint()// Set requesttimeout;
      const timeout = set.Timeout(() => {;
        if (!resheaders.Sent) {;
          res.status(408)json({;
            error instanceof Error ? error.message : String(error) 'Request timeout';
            message: `Request exceeded ${thisoptionsrequest.Timeout.Ms}ms timeout`}),;
          loggerwarn('Request timeout', LogContextPERFORMAN.C.E, {;
            method: req.method,;
            url: reqoriginal.Url || requrl,;
            timeout: thisoptionsrequest.Timeout.Ms})}}, thisoptionsrequest.Timeout.Ms)// Override resend to capture metrics;
      const original.End = resend;
      const self = this;
      resend = function (this: Response, .args: any[]) {;
        clear.Timeout(timeout);
        const end.Time = processhrtimebigint();
        const response.Time = Number(end.Time - start.Time) / 1000000// Convert to milliseconds// Record metric;
        const metric: Request.Metric = {;
          url: reqoriginal.Url || requrl,;
          method: req.method,;
          status.Code: resstatus.Code,;
          response.Time;
          timestamp: Date.now(),;
          user.Agent: req.headers['user-agent'],;
          ip: req.ip || reqsocketremote.Address,;
}        selfrequest.Metricspush(metric)// Log slow requests;
        if (response.Time > selfoptionsslow.Request.Threshold) {;
          loggerwarn('Slow requestdetected', LogContextPERFORMAN.C.E, {;
            .metric;
            threshold: selfoptionsslow.Request.Threshold})}// Log errors,;
        if (resstatus.Code >= 400) {;
          loggererror('Request error instanceof Error ? error.message : String(error)  LogContextPERFORMAN.C.E, metric);'}// Add performance headers;
        resset('X-Response-Time', `${response.Timeto.Fixed(2)}ms`);
        resset('X-Performance-Mode', 'fallback');
        return original.Endapply(this, args as any);
      next()}}/**;
   * Simple in-memory rate limiter*/
  public rate.Limiter(window.Ms = 900000, max = 1000) {;
    return (req: Request, res: Response, next: Next.Function) => {;
      const identifier = req.ip || reqsocketremote.Address || 'unknown';
      const now = Date.now();
      const user.Requests = thisrate.Limit.Mapget(identifier);
      if (!user.Requests || now > user.Requestsreset.Time) {;
        thisrate.Limit.Mapset(identifier, {;
          count: 1,;
          reset.Time: now + window.Ms}),;
        return next();

      if (user.Requestscount >= max) {;
        return res.status(429)json({;
          error instanceof Error ? error.message : String(error) 'Too many requests';
          retry.After: Mathceil((user.Requestsreset.Time - now) / 1000)}),;

      user.Requestscount++;
      next()}}/**;
   * Generate a simple performance report*/
  public async generate.Performance.Report(): Promise<string> {;
    const metrics = thisget.Metrics();
    const now = new Date()toIS.O.String();
    return ``=== Universal A.I.Tools Performance Report (Fallback Mode) ===;
Generated: ${now}=== System Status ===;
Mode: ${metricsmodeto.Upper.Case(),;
Total Metrics Tracked: ${metricstotal.Metrics,;
Active Rate Limit Entries: ${metricsrate.Limit.Entries}=== Request Statistics (Last 5 Minutes) ===;
Total Requests: ${metricslast5.Minutescount,;
Average Response Time: ${metricslast5MinutesavgResponse.Timeto.Fixed(2)}ms,;
P95 Response Time: ${metricslast5Minutesp95Response.Timeto.Fixed(2)}ms,;
P99 Response Time: ${metricslast5Minutesp99Response.Timeto.Fixed(2)}ms,;
Error Rate: ${metricslast5Minuteserror.Rateto.Fixed(2)}%=== Request Statistics (Last Hour) ===;
Total Requests: ${metricslast1.Hourcount,;
Average Response Time: ${metricslast1HouravgResponse.Timeto.Fixed(2)}ms,;
P95 Response Time: ${metricslast1Hourp95Response.Timeto.Fixed(2)}ms,;
P99 Response Time: ${metricslast1Hourp99Response.Timeto.Fixed(2)}ms,;
Error Rate: ${metricslast1Hourerror.Rateto.Fixed(2)}%=== Performance Issues ===;
Slow Requests (>${thisoptionsslow.Request.Threshold}ms): ${metricsslow.Requests}=== Notes ===;
• Running in fallback mode without Redis;
• Limited to in-memory metrics storage;
• Metrics are cleared on server restart;
• Maximum ${thisoptionsmax.Metrics.History} metrics retained;
• Request timeout protection: ${thisoptionsrequest.Timeout.Ms}ms,;
`;`}/**;
   * Get current metrics summary*/
  public get.Metrics() {;
    const now = Date.now();
    const last5.Minutes = thisrequest.Metricsfilter((m) => mtimestamp > now - 300000);
    const last1.Hour = thisrequest.Metricsfilter((m) => mtimestamp > now - 3600000);
    const calculate.Stats = (metrics: Request.Metric[]) => {;
      if (metricslength === 0) {;
        return {;
          count: 0,;
          avg.Response.Time: 0,;
          error.Rate: 0,;
          p95.Response.Time: 0,;
          p99.Response.Time: 0,;
        };

      const response.Times = metricsmap((m) => mresponse.Time)sort((a, b) => a - b);
      const total.Time = response.Timesreduce((sum, time) => sum + time, 0);
      const errors = metricsfilter((m) => mstatus.Code >= 400)length;
      const p95.Index = Mathfloor(response.Timeslength * 0.95);
      const p99.Index = Mathfloor(response.Timeslength * 0.99);
      return {;
        count: metricslength,;
        avg.Response.Time: total.Time / metricslength,;
        error.Rate: (errors / metricslength) * 100,;
        p95.Response.Time: response.Times[p95.Index] || 0,;
        p99.Response.Time: response.Times[p99.Index] || 0,;
      };
    return {;
      mode: 'fallback',;
      last5.Minutes: calculate.Stats(last5.Minutes),;
      last1.Hour: calculate.Stats(last1.Hour),;
      total.Metrics: thisrequest.Metricslength,;
      rate.Limit.Entries: thisrate.Limit.Mapsize,;
      slow.Requests: thisrequest.Metricsfilter(;
        (m) => mresponse.Time > thisoptionsslow.Request.Threshold)length;
      timestamp: Date.now(),;
    }}/**;
   * Cleanup resources*/
  public close(): void {;
    if (thiscleanup.Interval) {;
      clear.Interval(thiscleanup.Interval);
    thisrequest.Metrics = [];
    thisrate.Limit.Mapclear();
    loggerinfo('Fallback performance middleware closed', LogContextPERFORMAN.C.E)}}/**;
 * Factory function to create fallback middleware instance*/
export function createFallback.Performance.Middleware(options?: Fallback.Performance.Options) {;
  return new Fallback.Performance.Middleware(options);
