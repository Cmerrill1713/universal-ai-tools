#!/usr/bin/env node

/**
 * Comprehensive Model Test for Universal AI Tools
 * Tests all accessible models across LM Studio and Ollama
 */

import { performance } from 'perf_hooks';

const LM_STUDIO_URL = 'http://localhost:5901';
const OLLAMA_URL = 'http://localhost:11434';

// Comprehensive test cases
const testSuite = [
  {
    name: "Math Accuracy",
    prompt: "What is 17 * 23? Show calculation.",
    expectedAnswer: "391",
    weight: 0.3
  },
  {
    name: "Code Quality", 
    prompt: "Write a Python function to find the maximum value in a list.",
    keywords: ["def", "max", "list", "return"],
    weight: 0.3
  },
  {
    name: "Reasoning",
    prompt: "If it takes 5 machines 5 minutes to make 5 widgets, how long does it take 100 machines to make 100 widgets?",
    expectedAnswer: "5",
    weight: 0.4
  }
];

// All available models across both endpoints
const allModels = [
  // LM Studio Models (confirmed working)
  {
    id: "qwen/qwen3-coder-30b",
    name: "Qwen3 Coder 30B â­",
    endpoint: "lm-studio",
    category: "flagship"
  },
  {
    id: "deepseek/deepseek-r1-0528-qwen3-8b", 
    name: "DeepSeek R1 8B â­",
    endpoint: "lm-studio",
    category: "reasoning"
  },
  {
    id: "qwen2.5-coder-14b-instruct-mlx",
    name: "Qwen2.5 Coder 14B MLX",
    endpoint: "lm-studio",
    category: "optimized"
  },
  {
    id: "mistralai/mistral-small-3.2",
    name: "Mistral Small 3.2 24B",
    endpoint: "lm-studio",
    category: "balanced"
  },
  {
    id: "dolphin-mistral-24b-venice-edition-mlx",
    name: "Dolphin Mistral 24B MLX",
    endpoint: "lm-studio",
    category: "conversation"
  },
  {
    id: "google/gemma-3-12b",
    name: "Gemma 3 12B",
    endpoint: "lm-studio",
    category: "balanced"
  },
  
  // Ollama Models (confirmed accessible)
  {
    id: "gpt-oss:20b",
    name: "GPT-OSS 20B (Ollama)",
    endpoint: "ollama",
    category: "reasoning"
  },
  {
    id: "tinyllama:latest",
    name: "TinyLlama 1B (Ollama)",
    endpoint: "ollama",
    category: "lightweight"
  }
];

async function testModel(model, test) {
  const startTime = performance.now();
  
  try {
    let response, data;
    
    if (model.endpoint === 'lm-studio') {
      response = await fetch(`${LM_STUDIO_URL}/v1/chat/completions`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: model.id,
          messages: [{ role: 'user', content: test.prompt }],
          temperature: 0.1,
          max_tokens: 500
        }),
        signal: AbortSignal.timeout(45000)
      });
      
      if (!response.ok) throw new Error(`HTTP ${response.status}`);
      data = await response.json();
      
      return {
        success: true,
        response: data.choices[0].message.content,
        responseTime: performance.now() - startTime,
        tokens: data.usage?.total_tokens || 0
      };
    } else {
      response = await fetch(`${OLLAMA_URL}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: model.id,
          prompt: test.prompt,
          stream: false,
          options: { temperature: 0.1 }
        }),
        signal: AbortSignal.timeout(45000)
      });
      
      if (!response.ok) throw new Error(`HTTP ${response.status}`);
      data = await response.json();
      
      return {
        success: true,
        response: data.response,
        responseTime: performance.now() - startTime,
        tokens: data.eval_count || 0
      };
    }
  } catch (error) {
    return {
      success: false,
      error: error.message,
      responseTime: performance.now() - startTime
    };
  }
}

function scoreTest(response, test) {
  const content = response.toLowerCase();
  let score = 0;
  
  // Check for exact answer
  if (test.expectedAnswer && content.includes(test.expectedAnswer.toLowerCase())) {
    score += 60;
  }
  
  // Check for keywords
  if (test.keywords) {
    const found = test.keywords.filter(k => content.includes(k.toLowerCase()));
    score += (found.length / test.keywords.length) * 30;
  }
  
  // Quality indicators
  if (response.length > 50) score += 10;
  
  return Math.min(100, score);
}

async function runComprehensiveTest() {
  console.log("ðŸš€ Comprehensive Model Evaluation");
  console.log("==================================");
  console.log(`Testing ${allModels.length} models with ${testSuite.length} test cases\n`);
  
  const results = [];
  
  for (const model of allModels) {
    console.log(`\nðŸ§ª Testing ${model.name}`);
    console.log("-".repeat(50));
    
    let weightedScore = 0;
    let totalTime = 0;
    let successfulTests = 0;
    const testResults = [];
    
    for (const test of testSuite) {
      console.log(`\nðŸ“ ${test.name}...`);
      
      const result = await testModel(model, test);
      
      if (result.success) {
        const score = scoreTest(result.response, test);
        weightedScore += score * test.weight;
        totalTime += result.responseTime;
        successfulTests++;
        
        console.log(`â±ï¸  ${(result.responseTime/1000).toFixed(2)}s`);
        console.log(`ðŸŽ¯ ${score.toFixed(0)}/100`);
        console.log(`ðŸ“ ${result.response.substring(0, 80)}...`);
        
        testResults.push({ name: test.name, score, time: result.responseTime });
      } else {
        console.log(`âŒ Error: ${result.error}`);
        testResults.push({ name: test.name, score: 0, error: result.error });
      }
    }
    
    const avgTime = successfulTests > 0 ? totalTime / successfulTests : 0;
    const efficiency = avgTime > 0 ? weightedScore / (avgTime / 1000) : 0;
    
    results.push({
      model: model.name,
      category: model.category,
      endpoint: model.endpoint,
      weightedScore,
      avgTime,
      efficiency,
      successfulTests,
      testResults
    });
    
    console.log(`\nðŸ“Š Overall: ${weightedScore.toFixed(1)}/100 (${(avgTime/1000).toFixed(2)}s avg)`);
  }
  
  // Comprehensive Analysis
  console.log("\n" + "=".repeat(60));
  console.log("ðŸ“Š COMPREHENSIVE ANALYSIS");
  console.log("=".repeat(60));
  
  // Overall Rankings
  console.log("\nðŸ† Overall Performance Rankings:");
  const sortedByScore = [...results].sort((a, b) => b.weightedScore - a.weightedScore);
  sortedByScore.forEach((r, i) => {
    const medal = i === 0 ? "ðŸ¥‡" : i === 1 ? "ðŸ¥ˆ" : i === 2 ? "ðŸ¥‰" : `${i+1}.`;
    console.log(`${medal} ${r.model}: ${r.weightedScore.toFixed(1)}/100`);
  });
  
  // Speed Rankings
  console.log("\nâš¡ Speed Rankings:");
  const sortedBySpeed = [...results].sort((a, b) => a.avgTime - b.avgTime);
  sortedBySpeed.forEach((r, i) => {
    console.log(`${i+1}. ${r.model}: ${(r.avgTime/1000).toFixed(2)}s`);
  });
  
  // Efficiency Rankings (Quality per Second)
  console.log("\nðŸŽ¯âš¡ Efficiency Rankings (Quality/Speed):");
  const sortedByEfficiency = [...results].sort((a, b) => b.efficiency - a.efficiency);
  sortedByEfficiency.forEach((r, i) => {
    const medal = i === 0 ? "ðŸ¥‡" : i === 1 ? "ðŸ¥ˆ" : i === 2 ? "ðŸ¥‰" : `${i+1}.`;
    console.log(`${medal} ${r.model}: ${r.efficiency.toFixed(1)} pts/sec`);
  });
  
  // Category Analysis
  console.log("\nðŸ“‚ Best by Category:");
  const categories = [...new Set(results.map(r => r.category))];
  categories.forEach(category => {
    const categoryModels = results.filter(r => r.category === category);
    const best = categoryModels.reduce((a, b) => a.weightedScore > b.weightedScore ? a : b);
    console.log(`${category}: ${best.model} (${best.weightedScore.toFixed(1)}/100)`);
  });
  
  // Endpoint Comparison
  console.log("\nðŸ”Œ Endpoint Comparison:");
  const endpoints = [...new Set(results.map(r => r.endpoint))];
  endpoints.forEach(endpoint => {
    const endpointModels = results.filter(r => r.endpoint === endpoint);
    const avgScore = endpointModels.reduce((sum, r) => sum + r.weightedScore, 0) / endpointModels.length;
    const avgSpeed = endpointModels.reduce((sum, r) => sum + r.avgTime, 0) / endpointModels.length;
    console.log(`${endpoint}: ${avgScore.toFixed(1)}/100 avg (${(avgSpeed/1000).toFixed(2)}s avg)`);
  });
  
  // R1 RAG Recommendations
  console.log("\nðŸŽ¯ R1 RAG Implementation Recommendations:");
  
  const reasoningModels = results.filter(r => r.category === 'reasoning');
  const topReasoning = reasoningModels.reduce((a, b) => a.weightedScore > b.weightedScore ? a : b);
  console.log(`ðŸ§  Reasoning: ${topReasoning.model}`);
  
  const optimizedModels = results.filter(r => r.category === 'optimized');
  if (optimizedModels.length > 0) {
    const topOptimized = optimizedModels.reduce((a, b) => b.efficiency > a.efficiency ? b : a);
    console.log(`âš¡ Fast Tasks: ${topOptimized.model}`);
  }
  
  const flagshipModels = results.filter(r => r.category === 'flagship');
  if (flagshipModels.length > 0) {
    const topFlagship = flagshipModels.reduce((a, b) => a.weightedScore > b.weightedScore ? a : b);
    console.log(`ðŸ‘‘ Complex Tasks: ${topFlagship.model}`);
  }
  
  // Ollama Availability Note
  console.log("\nâš ï¸  Ollama Model Availability:");
  console.log("Only 3 models accessible via Ollama API (out of models shown in 'ollama list'):");
  results.filter(r => r.endpoint === 'ollama').forEach(r => {
    console.log(`  âœ… ${r.model}`);
  });
  console.log("Other Ollama models may need to be loaded/activated first.");
  
  console.log("\nâœ… Conclusion: Your LM Studio collection provides excellent coverage!");
  console.log("ðŸŽ¯ Perfect for R1 RAG implementation without any additional downloads.");
}

runComprehensiveTest().catch(console.error);