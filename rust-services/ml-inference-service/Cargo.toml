[package]
name = "ml-inference-service"
version = "0.1.0"
edition = "2021"
authors = ["Universal AI Tools Team"]
description = "High-performance ML inference service with ONNX, Candle, and native Rust ML"

[lib]
crate-type = ["cdylib", "rlib"]

[[bin]]
name = "ml-inference-service"
path = "src/main.rs"

# Make this package standalone to avoid workspace conflicts
[workspace]

[dependencies]
# Async runtime
tokio = { version = "1.35", features = ["full"] }
futures = "0.3"
async-trait = "0.1"

# Web framework for API
axum = "0.7"
tower = "0.4"
tower-http = { version = "0.5", features = ["cors", "trace"] }

# ML Frameworks (simplified for initial testing)
# Candle - Rust-native deep learning (similar to PyTorch)
candle-core = "0.6"
candle-nn = "0.6"
candle-transformers = "0.6"

# ONNX Runtime for cross-framework model support (optional for testing)
# ort = { version = "2.0.0-rc.10", features = ["cpu"], optional = true }

# SmartCore for classical ML
smartcore = { version = "0.3", features = ["serde"] }

# Burn - Deep learning framework with WGPU backend  
burn = { version = "0.13", features = ["wgpu", "std"] }
burn-autodiff = "0.13"

# Mathematical operations  
nalgebra = "0.32"
ndarray = { version = "0.15", features = ["rayon"] }
# ndarray-stats = "0.5"  # Optional for testing
statrs = "0.16"

# Image processing for vision models
image = "0.24"
imageproc = "0.23"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
bincode = "1.3"

# Caching
moka = { version = "0.12", features = ["future"] }
redis = { version = "0.24", features = ["tokio-comp"] }

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# DateTime
chrono = { version = "0.4", features = ["serde"] }

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# Metrics
prometheus = "0.13"
metrics = "0.21"
metrics-exporter-prometheus = "0.12"

[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports"] }
proptest = "1.4"