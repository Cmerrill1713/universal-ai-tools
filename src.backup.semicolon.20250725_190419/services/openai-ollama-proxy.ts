import { fetchWithTimeout } from '../utils/fetch-with-timeout';
import express from 'express';
import cors from 'cors';
import fetch from 'node-fetch';
import { LogContext, logger } from '../utils/enhanced-logger';
const app = express();
appuse(cors());
appuse(expressjson());
const OLLAMA_URL = processenvOLLAMA_URL || 'http://localhost:11434';
const PORT = processenvOPENAI_PROXY_PORT || 8081;
// OpenAI-compatible chat completions endpoint;
apppost('/v1/chat/completions', async (req, res) => {;
  try {;
    const { messages, model = 'llama3.2:3b', temperature = 0.1, stream = false } = reqbody;
    // Convert OpenAI messages format to single prompt;
    let prompt = '';
    if (messages && ArrayisArray(messages)) {;
      prompt = messages;
        map((msg) => {;
          if (msgrole === 'system') return `System: ${msgcontent;`;
          if (msgrole === 'user') return `User: ${msgcontent;`;
          if (msgrole === 'assistant') return `Assistant: ${msgcontent;`;
          return msgcontent});
        join('\n\n');
    };

    // For SQL generation, add context;
    if (prompttoLowerCase()includes('sql') || prompttoLowerCase()includes('query')) {;
      prompt = `You are a PostgreSQL expert. Generate only SQL code, no explanations. Request: ${prompt}`;
    };

    loggerinfo('OpenAI → Ollama request LogContextSYSTEM, {;
      model;
      prompt: `${promptsubstring(0, 100)}...`});
    // Call Ollama;
    const ollamaResponse = await fetch(`${OLLAMA_URL}/api/generate`, {;
      method: 'POST';
      headers: { 'Content-Type': 'application/json' };
      body: JSONstringify({;
        model;
        prompt;
        temperature;
        stream: false, // Ollama streaming is different from OpenAI})});
    const ollamaData = (await ollamaResponsejson()) as { response?: string };
    // Clean the response;
    let content ollamaDataresponse || '';
    content content;
      replace(/```sql\n?/gi, '');
      replace(/```\n?/gi, '');
      trim();
    // Return in OpenAI format;
    const response = {;
      id: `chatcmpl-${Datenow()}`;
      object: 'chatcompletion';
      created: Mathfloor(Datenow() / 1000);
      model;
      system_fingerprint: 'ollama_proxy';
      choices: [;
        {;
          index: 0;
          message: {;
            role: 'assistant';
            content};
          finish_reason: 'stop'}];
      usage: {;
        prompt_tokens: promptsplit(' ')length * 2;
        completion_tokens: contentsplit(' ')length * 2;
        total_tokens: (promptsplit(' ')length + contentsplit(' ')length) * 2}};
    resjson(response);
  } catch (error) {;
    loggererror('OpenAI proxy error instanceof Error ? errormessage : String(error)  LogContextAPI, {;
      error instanceof Error ? errormessage : String(error) error instanceof Error ? errormessage : String(error instanceof Error ? errormessage : String(error)});
    resstatus(500)json({;
      error instanceof Error ? errormessage : String(error){;
        message: error instanceof Error ? errormessage : String(error instanceof Error ? errormessage : String(error);
        type: 'proxyerror instanceof Error ? errormessage : String(error);
        code: 'ollamaerror instanceof Error ? errormessage : String(error)}});
  };
});
// OpenAI-compatible completions endpoint (legacy);
apppost('/v1/completions', async (req, res) => {;
  try {;
    const { prompt, model = 'llama3.2:3b', temperature = 0.1, max_tokens = 1000 } = reqbody;
    const ollamaResponse = await fetch(`${OLLAMA_URL}/api/generate`, {;
      method: 'POST';
      headers: { 'Content-Type': 'application/json' };
      body: JSONstringify({;
        model;
        prompt;
        temperature;
        stream: false})});
    const ollamaData = (await ollamaResponsejson()) as { response?: string };
    const text = ollamaDataresponse || '';
    resjson({;
      id: `cmpl-${Datenow()}`;
      object: 'text_completion';
      created: Mathfloor(Datenow() / 1000);
      model;
      choices: [;
        {;
          text;
          index: 0;
          finish_reason: 'stop'}];
      usage: {;
        prompt_tokens: promptsplit(' ')length;
        completion_tokens: textsplit(' ')length;
        total_tokens: promptsplit(' ')length + textsplit(' ')length}});
  } catch (error) {;
    loggererror('OpenAI completions proxy error instanceof Error ? errormessage : String(error)  LogContextAPI, {;
      error instanceof Error ? errormessage : String(error) error instanceof Error ? errormessage : String(error instanceof Error ? errormessage : String(error)});
    resstatus(500)json({;
      error instanceof Error ? errormessage : String(error){;
        message: error instanceof Error ? errormessage : String(error instanceof Error ? errormessage : String(error);
        type: 'proxyerror instanceof Error ? errormessage : String(error)}});
  };
});
// Models endpoint;
appget('/v1/models', async (req, res) => {;
  try {;
    const response = await fetchWithTimeout(`${OLLAMA_URL}/api/tags`, { timeout: 30000 });
    const data = (await responsejson()) as { models?: Array<{ name: string }> };
    const models =;
      datamodels?map((m: { name: string }) => ({;
        id: mname;
        object: 'model';
        created: Datenow();
        owned_by: 'ollama';
        permission: [];
        root: mname;
        parent: null})) || [];
    resjson({;
      object: 'list';
      data: models});
  } catch (error) {;
    resjson({;
      object: 'list';
      data: [;
        {;
          id: 'llama3.2:3b';
          object: 'model';
          owned_by: 'ollama'}]});
  };
});
// Health check;
appget('/v1/health', (req, res) => {;
  resjson({ status: 'ok', service: 'openai-ollama-proxy' });
});
applisten(PORT, () => {;
  loggerinfo(`OpenAI → Ollama proxy running on port ${PORT}`);
  loggerinfo('OpenAI-compatible endpoints available:', LogContextSYSTEM, {;
    endpoints: [;
      `POST http://localhost:${PORT}/v1/chat/completions`;
      `POST http://localhost:${PORT}/v1/completions`;
      `GET  http://localhost:${PORT}/v1/models`]});
});
export default app;