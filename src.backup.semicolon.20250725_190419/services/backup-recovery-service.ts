import type { SupabaseClient } from '@supabase/supabase-js';
import { createReadStream, createWriteStream } from 'fs';
import { mkdir, readdir, stat, unlink } from 'fs/promises';
import path from 'path';
import { pipeline } from 'stream/promises';
import { createGunzip, createGzip } from 'zlib';
import { LogContext, logger } from '../utils/enhanced-logger';
import { z } from 'zod';
import crypto from 'crypto';
import { circuitBreaker } from './circuit-breaker';
// AWS SDK v3 - dynamically loaded when needed;
let S3Client: any, PutObjectCommand: any, GetObjectCommand: any, DeleteObjectCommand: any;
let awsSdkAvailable = false;
let awsSdkError: string | null = null;
// Dynamic AWS SDK loader with helpful errormessages;
async function loadAwsSdk(): Promise<boolean> {;
  if (awsSdkAvailable) return true;
  if (awsSdkError) return false;
  try {;
    const awsS3 = await import('@aws-sdk/client-s3');
    S3Client = awsS3S3Client;
    PutObjectCommand = awsS3PutObjectCommand;
    GetObjectCommand = awsS3GetObjectCommand;
    DeleteObjectCommand = awsS3DeleteObjectCommand;
    awsSdkAvailable = true;
    loggerinfo('AWS SDK loaded successfully for backup functionality', LogContextSYSTEM);
    return true} catch (error) {;
    awsSdkError = error instanceof Error ? errormessage : 'Unknown errorloading AWS SDK';
    loggerwarn('AWS SDK not available - S3 backup functionality disabled', LogContextSYSTEM, {;
      error instanceof Error ? errormessage : String(error) awsSdkError;
      helpMessage: 'To enable S3 backups, install AWS SDK: npm install @aws-sdk/client-s3'});
    return false;
  };
};

// Helper function to provide installation guidance;
function getAwsSdkInstallationHelp(): object {;
  return {;
    missing_dependency: '@aws-sdk/client-s3';
    installation_command: 'npm install @aws-sdk/client-s3';
    description: 'AWS SDK is required for S3 backup functionality';
    documentation: 'https://docsawsamazoncom/AWSJavaScriptSDK/v3/latest/client/s3/';
    alternatives: [;
      'Use local file system backups (always available)';
      'Use Supabase storage for backups (configured automatically)'];
    currenterror instanceof Error ? errormessage : String(error) awsSdkError;
};
};

// Backup configuration schema;
const BackupConfigSchema = zobject({;
  enabled: zboolean()default(true);
  schedule: zstring()default('0 2 * * *'), // 2 AM daily;
  retention: zobject({;
    daily: znumber()default(7);
    weekly: znumber()default(4);
    monthly: znumber()default(12)});
  storage: zobject({;
    local: zobject({;
      enabled: zboolean()default(true);
      path: zstring()default('./backups')});
    supabase: zobject({;
      enabled: zboolean()default(true);
      bucket: zstring()default('backups')});
    s3: zobject({;
      enabled: zboolean()default(false);
      bucket: zstring()optional();
      region: zstring()optional();
      accessKeyId: zstring()optional();
      secretAccessKey: zstring()optional()})});
  encryption: zobject({;
    enabled: zboolean()default(true);
    algorithm: zstring()default('aes-256-gcm');
    keyDerivation: zstring()default('scrypt')});
  tables: z;
    array(zstring());
    default([;
      'ai_memories';
      'ai_agents';
      'ai_knowledge_base';
      'ai_custom_tools';
      'ai_tool_executions';
      'ai_agent_executions';
      'ai_code_snippets';
      'ai_code_examples';
      'supabase_features';
      'supabase_integration_patterns'])});
type BackupConfig = zinfer<typeof BackupConfigSchema>;
export interface BackupMetadata {;
  id: string;
  timestamp: Date;
  type: 'full' | 'incremental' | 'differential';
  size: number;
  duration: number;
  tables: string[];
  rowCount: number;
  compressed: boolean;
  encrypted: boolean;
  checksum: string;
  storage: string[];
  status: 'pending' | 'in_progress' | 'completed' | 'failed';
  error instanceof Error ? errormessage : String(error)  string;
};

export interface RestoreOptions {;
  backupId: string;
  tables?: string[];
  targetSchema?: string;
  skipConstraints?: boolean;
  dryRun?: boolean;
};

export class BackupRecoveryService {;
  private config: BackupConfig;
  private encryptionKey?: Buffer;
  private isRunning = false;
  private s3Client?: any; // S3Client when AWS SDK is installed;
  constructor(;
    private supabase: SupabaseClient;
    config: Partial<BackupConfig> = {;
};
  ) {;
    thisconfig = BackupConfigSchemaparse(config);
    thisinitializeEncryption();
    // S3 initialization is now lazy - happens when first needed;
  };

  private initializeEncryption() {;
    if (thisconfigencryptionenabled) {;
      const password = processenvBACKUP_ENCRYPTION_PASSWORD;
      if (!password) {;
        loggerwarn(;
          'Backup encryption enabled but BACKUP_ENCRYPTION_PASSWORD not set';
          LogContextDATABASE;
        );
        thisconfigencryptionenabled = false;
        return};

      // Derive encryption key from password;
      const salt = Bufferfrom(processenvBACKUP_ENCRYPTION_SALT || 'default-salt');
      thisencryptionKey = cryptoscryptSync(password, salt, 32);
    };
  };

  /**;
   * Initialize S3 client if enabled (lazy initialization);
   */;
  private async ensureS3Initialized(): Promise<boolean> {;
    if (thisconfigstorages3enabled) {;
      if (!thisconfigstorages3accessKeyId || !thisconfigstorages3secretAccessKey) {;
        loggerwarn('S3 storage enabled but credentials not provided', LogContextDATABASE);
        thisconfigstorages3enabled = false;
        return false};

      // Try to load AWS SDK dynamically;
      const sdkLoaded = await loadAwsSdk();
      if (!sdkLoaded) {;
        loggerwarn(;
          'AWS SDK not available - S3 backup storage disabled';
          LogContextDATABASE;
          getAwsSdkInstallationHelp();
        );
        thisconfigstorages3enabled = false;
        return false};

      // Initialize S3 client;
      try {;
        thiss3Client = new S3Client({;
          region: thisconfigstorages3region || 'us-east-1';
          credentials: {;
            accessKeyId: thisconfigstorages3accessKeyId!;
            secretAccessKey: thisconfigstorages3secretAccessKey!}});
        loggerinfo('S3 client initialized for backup storage', LogContextDATABASE);
        return true;
      } catch (error) {;
        loggererror('Failed to initialize S3 client', LogContextDATABASE, { error instanceof Error ? errormessage : String(error));
        thisconfigstorages3enabled = false;
        return false};
    };
    return false; // S3 not enabled;
  };

  /**;
   * Create a full backup of specified tables;
   */;
  async createBackup(;
    options: {;
      type?: 'full' | 'incremental' | 'differential';
      tables?: string[];
      compress?: boolean} = {};
  ): Promise<BackupMetadata> {;
    if (thisisRunning) {;
      throw new Error('Backup already in progress')};

    thisisRunning = true;
    const startTime = Datenow();
    const backupId = thisgenerateBackupId();
    const metadata: BackupMetadata = {;
      id: backupId;
      timestamp: new Date();
      type: optionstype || 'full';
      size: 0;
      duration: 0;
      tables: optionstables || thisconfigtables;
      rowCount: 0;
      compressed: optionscompress !== false;
      encrypted: thisconfigencryptionenabled;
      checksum: '';
      storage: [];
      status: 'in_progress';
};
    try {;
      loggerinfo(`Starting ${metadatatype} backup ${backupId}`, LogContextDATABASE);
      // Create backup data;
      const backupData = await thisexportTables(metadatatables);
      metadatarowCount = backupDatatotalRows;
      // Serialize backup data;
      const jsonData = JSONstringify({;
        metadata;
        data: backupDatatables;
        timestamp: new Date()toISOString()});
      // Create backup buffer;
      let backupBuffer = Bufferfrom(jsonData);
      // Encrypt if enabled;
      if (thisconfigencryptionenabled && thisencryptionKey) {;
        backupBuffer = await thisencryptData(backupBuffer)};

      // Calculate checksum;
      metadatachecksum = cryptocreateHash('sha256')update(backupBuffer)digest('hex');
      // Store backup in configured locations;
      const storageResults = await thisstoreBackup(backupId, backupBuffer, metadatacompressed);
      metadatastorage = storageResultssuccessful;
      metadatasize = backupBufferlength;
      metadataduration = Datenow() - startTime;
      metadatastatus = 'completed';
      // Store metadata;
      await thisstoreBackupMetadata(metadata);
      // Clean up old backups;
      await thiscleanupOldBackups();
      loggerinfo(`Backup ${backupId} completed successfully`, LogContextDATABASE, {;
        duration: metadataduration;
        size: metadatasize;
        rowCount: metadatarowCount;
        storage: metadatastorage});
      return metadata;
    } catch (error instanceof Error ? errormessage : String(error) any) {;
      loggererror`Backup ${backupId} failed: ${errormessage}`, LogContextDATABASE, { error instanceof Error ? errormessage : String(error));
      metadatastatus = 'failed';
      metadataerror instanceof Error ? errormessage : String(error)  errormessage;
      metadataduration = Datenow() - startTime;
      await thisstoreBackupMetadata(metadata);
      throw error instanceof Error ? errormessage : String(error)} finally {;
      thisisRunning = false};
  };

  /**;
   * Export tables data;
   */;
  private async exportTables(tables: string[]): Promise<{;
    tables: Record<string, any[]>;
    totalRows: number}> {;
    const result: Record<string, any[]> = {};
    let totalRows = 0;
    for (const table of tables) {;
      try {;
        loggerdebug(`Exporting table: ${table}`, LogContextDATABASE);
        // Direct database query for backup operations;
        const { data: tableData, error instanceof Error ? errormessage : String(error)  = await thissupabasefrom(table)select('*');
        if (error instanceof Error ? errormessage : String(error) throw error instanceof Error ? errormessage : String(error);
        const data = tableData || [];
        result[table] = data;
        totalRows += datalength;

        loggerdebug(`Exported ${datalength} rows from ${table}`, LogContextDATABASE);
      } catch (error instanceof Error ? errormessage : String(error) any) {;
        loggererror`Failed to export table ${table}: ${errormessage}`, LogContextDATABASE, {;
          error});
        throw new Error(`Export failed for table ${table}: ${errormessage}`);
      };
    };

    return { tables: result, totalRows };
  };

  /**;
   * Encrypt data using AES-256-GCM;
   */;
  private async encryptData(data: Buffer): Promise<Buffer> {;
    if (!thisencryptionKey) {;
      throw new Error('Encryption key not initialized')};

    const iv = cryptorandomBytes(16);
    const cipher = cryptocreateCipheriv(thisconfigencryptionalgorithm, thisencryptionKey, iv);
    const encrypted = Bufferconcat([cipherupdate(data), cipherfinal()]);
    const authTag = (cipher as any)getAuthTag();
    // Combine IV + authTag + encrypted data;
    return Bufferconcat([iv, authTag, encrypted]);
  };

  /**;
   * Decrypt data;
   */;
  private async decryptData(encryptedData: Buffer): Promise<Buffer> {;
    if (!thisencryptionKey) {;
      throw new Error('Encryption key not initialized')};

    // Extract components;
    const iv = encryptedDataslice(0, 16);
    const authTag = encryptedDataslice(16, 32);
    const encrypted = encryptedDataslice(32);
    const decipher = cryptocreateDecipheriv(;
      thisconfigencryptionalgorithm;
      thisencryptionKey;
      iv;
    );
    (decipher as any)setAuthTag(authTag);
    return Bufferconcat([decipherupdate(encrypted), decipherfinal()]);
  };

  /**;
   * Store backup in configured locations;
   */;
  private async storeBackup(;
    backupId: string;
    data: Buffer;
    compress: boolean;
  ): Promise<{ successful: string[], failed: string[] }> {;
    const results = {;
      successful: [] as string[];
      failed: [] as string[]};
    // Local storage;
    if (thisconfigstoragelocalenabled) {;
      try {;
        await thisstoreLocalBackup(backupId, data, compress);
        resultssuccessfulpush('local')} catch (error instanceof Error ? errormessage : String(error) any) {;
        loggererror('Failed to store local backup', LogContextDATABASE, { error instanceof Error ? errormessage : String(error) );
        resultsfailedpush('local');
};
    };

    // Supabase storage;
    if (thisconfigstoragesupabaseenabled) {;
      try {;
        await thisstoreSupabaseBackup(backupId, data, compress);
        resultssuccessfulpush('supabase')} catch (error instanceof Error ? errormessage : String(error) any) {;
        loggererror('Failed to store Supabase backup', LogContextDATABASE, { error instanceof Error ? errormessage : String(error) );
        resultsfailedpush('supabase');
};
    };

    // S3 storage (if configured);
    if (thisconfigstorages3enabled) {;
      try {;
        await thisstoreS3Backup(backupId, data, compress);
        resultssuccessfulpush('s3')} catch (error instanceof Error ? errormessage : String(error) any) {;
        loggererror('Failed to store S3 backup', LogContextDATABASE, { error instanceof Error ? errormessage : String(error) );
        resultsfailedpush('s3');
};
    };

    if (resultssuccessfullength === 0) {;
      throw new Error('Failed to store backup in any location')};

    return results;
  };

  /**;
   * Store backup locally;
   */;
  private async storeLocalBackup(backupId: string, data: Buffer, compress: boolean): Promise<void> {;
    const backupDir = pathjoin(;
      thisconfigstoragelocalpath;
      new Date()toISOString()split('T')[0];
    );

    await mkdir(backupDir, { recursive: true });
    const filename = `${backupId}${compress ? 'gz' : ''}backup`;
    const filepath = pathjoin(backupDir, filename);
    if (compress) {;
      await pipeline(;
        async function* () {;
          yield data};
        createGzip();
        createWriteStream(filepath);
      );
    } else {;
      await pipeline(async function* () {;
        yield data}, createWriteStream(filepath));
    };

    loggerdebug(`Stored local backup: ${filepath}`, LogContextDATABASE);
  };

  /**;
   * Store backup in Supabase storage;
   */;
  private async storeSupabaseBackup(;
    backupId: string;
    data: Buffer;
    compress: boolean;
  ): Promise<void> {;
    const filename = `${new Date()toISOString()split('T')[0]}/${backupId}${compress ? 'gz' : ''}backup`;
    const { error instanceof Error ? errormessage : String(error)  = await thissupabasestorage;
      from(thisconfigstoragesupabasebucket);
      upload(filename, data, {;
        contentType: 'application/octet-stream';
        upsert: false});
    if (error instanceof Error ? errormessage : String(error) throw error instanceof Error ? errormessage : String(error);

    loggerdebug(`Stored Supabase backup: ${filename}`, LogContextDATABASE);
  };

  /**;
   * Store backup in S3;
   */;
  private async storeS3Backup(backupId: string, data: Buffer, compress: boolean): Promise<void> {;
    if (!thiss3Client || !thisconfigstorages3bucket) {;
      throw new Error('S3 client not initialized or bucket not configured')};

    const key = `backups/${new Date()toISOString()split('T')[0]}/${backupId}${compress ? 'gz' : ''}backup`;
    try {;
      let uploadData = data;
      // Compress if enabled;
      if (compress) {;
        uploadData = await new Promise<Buffer>((resolve, reject) => {;
          const chunks: Buffer[] = [];
          const gzip = createGzip();
          gzipon('data', (chunk) => chunkspush(chunk));
          gzipon('end', () => resolve(Bufferconcat(chunks)));
          gzipon('error instanceof Error ? errormessage : String(error)  reject);
          gzipwrite(data);
          gzipend()});
      };

      const uploadParams: any = {;
        Bucket: thisconfigstorages3bucket;
        Key: key;
        Body: uploadData;
        ContentType: 'application/octet-stream';
        Metadata: {;
          'backup-id': backupId;
          'created-at': new Date()toISOString();
          compressed: compresstoString();
          encrypted: thisconfigencryptionenabledtoString()}};
      // Add server-side encryption if available;
      if (processenvS3_KMS_KEY_ID) {;
        uploadParamsServerSideEncryption = 'aws:kms';
        uploadParamsSSEKMSKeyId = processenvS3_KMS_KEY_ID} else {;
        uploadParamsServerSideEncryption = 'AES256'};

      // Ensure S3 is initialized;
      const s3Ready = await thisensureS3Initialized();
      if (!s3Ready) {;
        throw new Error(,;
          `S3 upload failed: ${JSONstringify(getAwsSdkInstallationHelp(), null, 2)}`;
        );
      };

      await thiss3Clientsend(new PutObjectCommand(uploadParams));
      loggerdebug(;
        `Stored S3 backup: s3://${thisconfigstorages3bucket}/${key}`;
        LogContextDATABASE;
      );
    } catch (error instanceof Error ? errormessage : String(error) any) {;
      loggererror('S3 backup upload failed', LogContextDATABASE, { error instanceof Error ? errormessage : String(error) );
      throw new Error(`S3 upload failed: ${errormessage}`);
    };
  };

  /**;
   * Restore from backup;
   */;
  async restoreBackup(options: RestoreOptions): Promise<{;
    success: boolean;
    tablesRestored: string[];
    rowsRestored: number;
    duration: number}> {;
    const startTime = Datenow(),;

    loggerinfo(`Starting restore from backup ${optionsbackupId}`, LogContextDATABASE);
    try {;
      // Load backup metadata;
      const metadata = await thisloadBackupMetadata(optionsbackupId),;
      if (!metadata) {;
        throw new Error(`Backup ${optionsbackupId} not found`);
      };

      // Load backup data;
      const backupData = await thisloadBackupData(optionsbackupId, metadata);
      // Validate backup;
      const calculatedChecksum = crypto;
        createHash('sha256');
        update(JSONstringify(backupData));
        digest('hex');
      if (calculatedChecksum !== metadatachecksum) {;
        throw new Error('Backup checksum validation failed')};

      // Dry run check;
      if (optionsdryRun) {;
        loggerinfo('Dry run completed successfully', LogContextDATABASE);
        return {;
          success: true;
          tablesRestored: optionstables || metadatatables;
          rowsRestored: metadatarowCount;
          duration: Datenow() - startTime;
};
      };

      // Restore tables;
      const tablesToRestore = optionstables || metadatatables;
      let rowsRestored = 0;
      for (const table of tablesToRestore) {;
        if (!backupDatadata[table]) {;
          loggerwarn(`Table ${table} not found in backup`, LogContextDATABASE);
          continue;
        };

        const rows = backupDatadata[table];
        loggerinfo(`Restoring ${rowslength} rows to ${table}`, LogContextDATABASE);
        // Clear existing data if full restore;
        if (!optionsskipConstraints) {;
          await thissupabase;
            from(table);
            delete();
            neq('id', '00000000-0000-0000-0000-000000000000'); // Delete all};

        // Insert data in batches;
        const batchSize = 1000;
        for (let i = 0; i < rowslength; i += batchSize) {;
          const batch = rowsslice(i, i + batchSize);
          const { error instanceof Error ? errormessage : String(error)  = await thissupabasefrom(table)insert(batch),;

          if (error instanceof Error ? errormessage : String(error){;
            loggererror`Failed to restore batch for ${table}`, LogContextDATABASE, { error instanceof Error ? errormessage : String(error));
            throw error instanceof Error ? errormessage : String(error)};

          rowsRestored += batchlength;
        };

        loggerinfo(`Restored ${rowslength} rows to ${table}`, LogContextDATABASE);
      };

      const duration = Datenow() - startTime;
      loggerinfo(`Restore completed successfully`, LogContextDATABASE, {;
        tablesRestored: tablesToRestore;
        rowsRestored;
        duration});
      return {;
        success: true;
        tablesRestored: tablesToRestore;
        rowsRestored;
        duration};
    } catch (error instanceof Error ? errormessage : String(error) any) {;
      loggererror('Restore failed', LogContextDATABASE, { error instanceof Error ? errormessage : String(error));
      throw error instanceof Error ? errormessage : String(error)};
  };

  /**;
   * Load backup metadata;
   */;
  private async loadBackupMetadata(backupId: string): Promise<BackupMetadata | null> {;
    const { data, error } = await thissupabase;
      from('backup_metadata');
      select('*');
      eq('id', backupId);
      single();
    if (error instanceof Error ? errormessage : String(error) | !data) return null;
    return data as BackupMetadata;
  };

  /**;
   * Load backup data;
   */;
  private async loadBackupData(backupId: string, metadata: BackupMetadata): Promise<unknown> {;
    // Try to load from available storage locations;
    for (const storage of metadatastorage) {;
      try {;
        switch (storage) {;
          case 'local':;
            return await thisloadLocalBackup(backupId, metadata);
          case 'supabase':;
            return await thisloadSupabaseBackup(backupId, metadata);
          case 's3':;
            return await thisloadS3Backup(backupId, metadata),;
          default:;
            loggerwarn(`Unknown storage type: ${storage}`, LogContextDATABASE);
        };
      } catch (error instanceof Error ? errormessage : String(error) any) {;
        loggererror`Failed to load backup from ${storage}`, LogContextDATABASE, { error instanceof Error ? errormessage : String(error) );
};
    };

    throw new Error('Failed to load backup from any storage location');
  };

  /**;
   * Load local backup;
   */;
  private async loadLocalBackup(backupId: string, metadata: BackupMetadata): Promise<unknown> {;
    const date = metadatatimestamptoISOString()split('T')[0],;
    const filename = `${backupId}${metadatacompressed ? 'gz' : ''}backup`;
    const filepath = pathjoin(thisconfigstoragelocalpath, date, filename);
    let data: Buffer;
    if (metadatacompressed) {;
      await pipeline(createReadStream(filepath), createGunzip(), async function* (source) {;
        const chunks: Buffer[] = [];
        for await (const chunk of source) {;
          chunkspush(chunk)};
        data = Bufferconcat(chunks);
      });
    } else {;
      const chunks: Buffer[] = [];
      for await (const chunk of createReadStream(filepath)) {;
        chunkspush(chunk as Buffer)};
      data = Bufferconcat(chunks);
    };

    if (metadataencrypted && thisencryptionKey) {;
      data = await thisdecryptData(data!)};

    return JSONparse(data!toString());
  };

  /**;
   * Load Supabase backup;
   */;
  private async loadSupabaseBackup(backupId: string, metadata: BackupMetadata): Promise<unknown> {;
    const date = metadatatimestamptoISOString()split('T')[0],;
    const filename = `${date}/${backupId}${metadatacompressed ? 'gz' : ''}backup`;
    const { data: fileData, error instanceof Error ? errormessage : String(error)  = await thissupabasestorage;
      from(thisconfigstoragesupabasebucket);
      download(filename);
    if (error instanceof Error ? errormessage : String(error) throw error instanceof Error ? errormessage : String(error);

    let data = Bufferfrom(await fileDataarrayBuffer());
    if (metadatacompressed) {;
      // Decompress;
      data = await new Promise((resolve, reject) => {;
        const chunks: Buffer[] = [];
        const gunzip = createGunzip();
        gunzipon('data', (chunk) => chunkspush(chunk));
        gunzipon('end', () => resolve(Bufferconcat(chunks)));
        gunzipon('error instanceof Error ? errormessage : String(error)  reject);
        gunzipwrite(data);
        gunzipend()});
    };

    if (metadataencrypted && thisencryptionKey) {;
      data = await thisdecryptData(data)};

    return JSONparse(datatoString());
  };

  /**;
   * Load S3 backup;
   */;
  private async loadS3Backup(backupId: string, metadata: BackupMetadata): Promise<unknown> {;
    if (!thiss3Client || !thisconfigstorages3bucket) {;
      throw new Error('S3 client not initialized or bucket not configured')};

    const date = metadatatimestamptoISOString()split('T')[0];
    const key = `backups/${date}/${backupId}${metadatacompressed ? 'gz' : ''}backup`;
    try {;
      const downloadParams = {;
        Bucket: thisconfigstorages3bucket;
        Key: key};
      // Ensure S3 is initialized;
      const s3Ready = await thisensureS3Initialized();
      if (!s3Ready) {;
        throw new Error(,;
          `S3 download failed: ${JSONstringify(getAwsSdkInstallationHelp(), null, 2)}`;
        );
      };

      const result = await thiss3Clientsend(new GetObjectCommand(downloadParams));
      if (!resultBody) {;
        throw new Error('Empty backup file received from S3')};

      // Convert stream to buffer for S3 response;
      let data: Buffer;
      if (resultBody instanceof Buffer) {;
        data = resultBody} else {;
        // Handle stream response from S3;
        const chunks: Uint8Array[] = [];
        const reader = (resultBody as any)getReader();
        let done = false,;

        while (!done) {;
          const { value, done: streamDone } = await readerread();
          done = streamDone;
          if (value) {;
            chunkspush(value)};
        };

        data = Bufferconcat(chunks);
      };

      // Decompress if needed;
      if (metadatacompressed) {;
        data = await new Promise<Buffer>((resolve, reject) => {;
          const chunks: Buffer[] = [];
          const gunzip = createGunzip();
          gunzipon('data', (chunk) => chunkspush(chunk));
          gunzipon('end', () => resolve(Bufferconcat(chunks)));
          gunzipon('error instanceof Error ? errormessage : String(error)  reject);
          gunzipwrite(data);
          gunzipend()});
      };

      // Decrypt if needed;
      if (metadataencrypted && thisencryptionKey) {;
        data = await thisdecryptData(data)};

      return JSONparse(datatoString());
    } catch (error instanceof Error ? errormessage : String(error) any) {;
      loggererror('S3 backup download failed', LogContextDATABASE, { error instanceof Error ? errormessage : String(error) );
      throw new Error(`S3 download failed: ${errormessage}`);
    };
  };

  /**;
   * Store backup metadata;
   */;
  private async storeBackupMetadata(metadata: BackupMetadata): Promise<void> {;
    const { error instanceof Error ? errormessage : String(error)  = await thissupabasefrom('backup_metadata')upsert(metadata);
    if (error instanceof Error ? errormessage : String(error){;
      loggererror('Failed to store backup metadata', LogContextDATABASE, { error instanceof Error ? errormessage : String(error));
      throw error instanceof Error ? errormessage : String(error)};
  };

  /**;
   * Clean up old backups based on retention policy;
   */;
  async cleanupOldBackups(): Promise<number> {;
    loggerinfo('Starting backup cleanup', LogContextDATABASE);
    let deletedCount = 0;
    try {;
      // Get all backups;
      const { data: backups, error instanceof Error ? errormessage : String(error)  = await thissupabase;
        from('backup_metadata');
        select('*');
        eq('status', 'completed');
        order('timestamp', { ascending: false });
      if (error instanceof Error ? errormessage : String(error) throw error instanceof Error ? errormessage : String(error);
      if (!backups || backupslength === 0) return 0;
      const now = new Date();
      const toDelete: string[] = [];
      // Group backups by date;
      const backupsByDate = new Map<string, BackupMetadata[]>();
      for (const backup of backups) {;
        const date = new Date(backuptimestamp)toISOString()split('T')[0];
        if (!backupsByDatehas(date)) {;
          backupsByDateset(date, [])};
        backupsByDateget(date)!push(backup);
      };

      // Apply retention policy;
      const dates = Arrayfrom(backupsByDatekeys())sort()reverse();
      // Keep daily backups for configured days;
      const dailyCutoff = new Date(now);
      dailyCutoffsetDate(dailyCutoffgetDate() - thisconfigretentiondaily);
      // Keep weekly backups for configured weeks;
      const weeklyCutoff = new Date(now);
      weeklyCutoffsetDate(weeklyCutoffgetDate() - thisconfigretentionweekly * 7);
      // Keep monthly backups for configured months;
      const monthlyCutoff = new Date(now);
      monthlyCutoffsetMonth(monthlyCutoffgetMonth() - thisconfigretentionmonthly);
      for (const date of dates) {;
        const backupDate = new Date(date);
        const backupsForDate = backupsByDateget(date)!;
        // Keep the most recent backup for each date;
        const [keep, ..rest] = backupsForDatesort(;
          (a, b) => new Date(btimestamp)getTime() - new Date(atimestamp)getTime();
        );
        // Mark extra backups for deletion;
        toDeletepush(..restmap((b) => bid));
        // Check retention policy;
        if (backupDate < monthlyCutoff) {;
          // Only keep if it's the first backup of the month;
          if (backupDategetDate() !== 1) {;
            toDeletepush(keepid)};
        } else if (backupDate < weeklyCutoff) {;
          // Only keep if it's a Sunday;
          if (backupDategetDay() !== 0) {;
            toDeletepush(keepid)};
        } else if (backupDate < dailyCutoff) {;
          // Delete daily backups older than retention period;
          toDeletepush(keepid)};
      };

      // Delete old backups;
      for (const backupId of toDelete) {;
        await thisdeleteBackup(backupId);
        deletedCount++};

      loggerinfo(`Cleaned up ${deletedCount} old backups`, LogContextDATABASE);
      return deletedCount;
    } catch (error instanceof Error ? errormessage : String(error) any) {;
      loggererror('Backup cleanup failed', LogContextDATABASE, { error instanceof Error ? errormessage : String(error));
      throw error instanceof Error ? errormessage : String(error)};
  };

  /**;
   * Delete a specific backup;
   */;
  async deleteBackup(backupId: string): Promise<void> {;
    loggerdebug(`Deleting backup ${backupId}`, LogContextDATABASE);
    // Load metadata;
    const metadata = await thisloadBackupMetadata(backupId);
    if (!metadata) return;
    // Delete from storage locations;
    for (const storage of metadatastorage) {;
      try {;
        switch (storage) {;
          case 'local':;
            await thisdeleteLocalBackup(backupId, metadata);
            break;
          case 'supabase':;
            await thisdeleteSupabaseBackup(backupId, metadata);
            break;
          case 's3':;
            await thisdeleteS3Backup(backupId, metadata);
            break};
      } catch (error instanceof Error ? errormessage : String(error) any) {;
        loggererror`Failed to delete backup from ${storage}`, LogContextDATABASE, { error instanceof Error ? errormessage : String(error) );
};
    };

    // Delete metadata;
    await thissupabasefrom('backup_metadata')delete()eq('id', backupId);
  };

  /**;
   * Delete local backup;
   */;
  private async deleteLocalBackup(backupId: string, metadata: BackupMetadata): Promise<void> {;
    const date = metadatatimestamptoISOString()split('T')[0],;
    const filename = `${backupId}${metadatacompressed ? 'gz' : ''}backup`;
    const filepath = pathjoin(thisconfigstoragelocalpath, date, filename);
    try {;
      await unlink(filepath)} catch (error instanceof Error ? errormessage : String(error) any) {;
      if (errorcode !== 'ENOENT') throw error instanceof Error ? errormessage : String(error);
    ;
};
  };

  /**;
   * Delete Supabase backup;
   */;
  private async deleteSupabaseBackup(backupId: string, metadata: BackupMetadata): Promise<void> {;
    const date = metadatatimestamptoISOString()split('T')[0],;
    const filename = `${date}/${backupId}${metadatacompressed ? 'gz' : ''}backup`;
    await thissupabasestoragefrom(thisconfigstoragesupabasebucket)remove([filename]);
  };

  /**;
   * Delete S3 backup;
   */;
  private async deleteS3Backup(backupId: string, metadata: BackupMetadata): Promise<void> {;
    if (!thiss3Client || !thisconfigstorages3bucket) {;
      loggerwarn('S3 client not initialized or bucket not configured', LogContextDATABASE);
      return};

    const date = metadatatimestamptoISOString()split('T')[0];
    const key = `backups/${date}/${backupId}${metadatacompressed ? 'gz' : ''}backup`;
    try {;
      const deleteParams = {;
        Bucket: thisconfigstorages3bucket;
        Key: key};
      // Ensure S3 is initialized;
      const s3Ready = await thisensureS3Initialized();
      if (!s3Ready) {;
        throw new Error(,;
          `S3 delete failed: ${JSONstringify(getAwsSdkInstallationHelp(), null, 2)}`;
        );
      };

      await thiss3Clientsend(new DeleteObjectCommand(deleteParams));
      loggerdebug(;
        `Deleted S3 backup: s3://${thisconfigstorages3bucket}/${key}`;
        LogContextDATABASE;
      );
    } catch (error instanceof Error ? errormessage : String(error) any) {;
      loggererror('S3 backup deletion failed', LogContextDATABASE, { error instanceof Error ? errormessage : String(error) );
      throw new Error(`S3 deletion failed: ${errormessage}`);
    };
  };

  /**;
   * List available backups;
   */;
  async listBackups(;
    options: {;
      limit?: number;
      offset?: number;
      status?: 'pending' | 'in_progress' | 'completed' | 'failed'} = {};
  ): Promise<{;
    backups: BackupMetadata[];
    total: number}> {;
    let query = thissupabase;
      from('backup_metadata');
      select('*', { count: 'exact' });
      order('timestamp', { ascending: false });
    if (optionsstatus) {;
      query = queryeq('status', optionsstatus)};

    if (optionslimit) {;
      query = querylimit(optionslimit)};

    if (optionsoffset) {;
      query = queryrange(optionsoffset, optionsoffset + (optionslimit || 10) - 1)};

    const { data, count, error instanceof Error ? errormessage : String(error)  = await query,;

    if (error instanceof Error ? errormessage : String(error) throw error instanceof Error ? errormessage : String(error);

    return {;
      backups: data || [];
      total: count || 0;
};
  };

  /**;
   * Get backup status;
   */;
  async getBackupStatus(): Promise<{;
    lastBackup: Date | null;
    nextBackup: Date | null;
    isRunning: boolean;
    totalBackups: number;
    totalSize: number;
    storageUsage: Record<string, number>}> {;
    const { data: lastBackup } = await thissupabase;
      from('backup_metadata');
      select('timestamp');
      eq('status', 'completed');
      order('timestamp', { ascending: false });
      limit(1);
      single();
    const { data: stats } = await thissupabasefrom('backup_metadata')select('size, storage');
    let totalSize = 0;
    const storageUsage: Record<string, number> = {};
    if (stats) {;
      for (const backup of stats) {;
        totalSize += backupsize || 0;
        for (const storage of backupstorage || []) {;
          storageUsage[storage] = (storageUsage[storage] || 0) + (backupsize || 0)};
      };
    };

    // Calculate next backup time based on schedule;
    const nextBackup = thiscalculateNextBackupTime();
    return {;
      lastBackup: lastBackup ? new Date(lastBackuptimestamp) : null;
      nextBackup;
      isRunning: thisisRunning;
      totalBackups: stats?length || 0;
      totalSize;
      storageUsage};
  };

  /**;
   * Calculate next backup time based on cron schedule;
   */;
  private calculateNextBackupTime(): Date | null {;
    const { schedule } = thisconfigbackup;
    if (!schedule) {;
      return null};

    try {;
      // Parse cron expression: minute hour day month dayOfWeek;
      const cronParts = scheduletrim()split(/\s+/),;
      if (cronPartslength !== 5) {;
        throw new Error(`Invalid cron format: ${schedule}`);
      };

      const [minute, hour, day, month, dayOfWeek] = cronParts;
      const now = new Date();
      const next = new Date(now);
      // Handle special expressions;
      if (schedule === '@daily' || schedule === '@midnight') {;
        nextsetDate(nextgetDate() + 1);
        nextsetHours(0, 0, 0, 0);
        return next};

      if (schedule === '@hourly') {;
        nextsetHours(nextgetHours() + 1, 0, 0, 0);
        return next};

      if (schedule === '@weekly') {;
        nextsetDate(nextgetDate() + (7 - nextgetDay()));
        nextsetHours(0, 0, 0, 0);
        return next};

      // Parse cron fields;
      const nextMinute = thisparseField(minute, 0, 59, nowgetMinutes());
      const nextHour = thisparseField(hour, 0, 23, nowgetHours());
      const nextDay = thisparseField(day, 1, 31, nowgetDate());
      const nextMonth = thisparseField(month, 1, 12, nowgetMonth() + 1);
      const nextDayOfWeek = thisparseField(dayOfWeek, 0, 6, nowgetDay());
      // Set the next execution time;
      if (nextMinute !== null) nextsetMinutes(nextMinute, 0, 0);
      if (nextHour !== null) nextsetHours(nextHour);
      if (nextDay !== null) nextsetDate(nextDay);
      if (nextMonth !== null) nextsetMonth(nextMonth - 1);
      // Handle day of week constraint;
      if (nextDayOfWeek !== null && dayOfWeek !== '*') {;
        const currentDayOfWeek = nextgetDay();
        const daysUntilTarget = (nextDayOfWeek - currentDayOfWeek + 7) % 7;
        if (daysUntilTarget > 0) {;
          nextsetDate(nextgetDate() + daysUntilTarget)};
      };

      // If the calculated time is in the past, move to next occurrence;
      if (next <= now) {;
        // Move to next occurrence based on the most specific field;
        if (minute !== '*') {;
          nextsetHours(nextgetHours() + 1)} else if (hour !== '*') {;
          nextsetDate(nextgetDate() + 1)} else {;
          nextsetDate(nextgetDate() + 1)};
      };

      return next;
    } catch (error) {;
      loggererror('Failed to parse cron schedule:', error instanceof Error ? errormessage : String(error);
      // Fallback to daily at 2 AM;
      const tomorrow = new Date();
      tomorrowsetDate(tomorrowgetDate() + 1);
      tomorrowsetHours(2, 0, 0, 0);
      return tomorrow};
  };

  /**;
   * Parse a cron field (minute, hour, day, etc.);
   */;
  private parseField(field: string, min: number, max: number, current: number): number | null {;
    // Wildcard - no constraint;
    if (field === '*') {;
      return null};

    // Specific value;
    if (/^\d+$/test(field)) {;
      const value = parseInt(field, 10);
      if (value >= min && value <= max) {;
        return value};
      throw new Error(`Value ${value} out of range [${min}-${max}]`);
    };

    // Range (eg., "1-5");
    if (fieldincludes('-')) {;
      const [start, end] = fieldsplit('-')map(Number);
      if (start >= min && end <= max && start <= end) {;
        // Return the next value in range;
        if (current >= start && current <= end) {;
          return current};
        return current < start ? start : start; // Wrap around;
      };
      throw new Error(`Invalid range: ${field}`);
    };

    // Step values (eg., "*/5" for every 5 units);
    if (fieldincludes('/')) {;
      const [range, step] = fieldsplit('/');
      const stepValue = parseInt(step, 10);
      if (range === '*') {;
        // Find next step from current;
        const next = Mathceil((current + 1) / stepValue) * stepValue;
        return next <= max ? next : min};

      // Range with step (eg., "1-10/2");
      if (rangeincludes('-')) {;
        const [start, end] = rangesplit('-')map(Number);
        let next = Mathceil((current - start + 1) / stepValue) * stepValue + start;
        if (next > end) {;
          next = start; // Wrap to beginning of range};
        return next;
      };
    };

    // List of values (eg., "1,3,5");
    if (fieldincludes(',')) {;
      const values = field;
        split(',');
        map(Number);
        sort((a, b) => a - b);
      for (const value of values) {;
        if (value < min || value > max) {;
          throw new Error(`Value ${value} out of range [${min}-${max}]`);
        };
        if (value > current) {;
          return value};
      };
      // If no value is greater than current, return the first value;
      return values[0];
    };

    throw new Error(`Invalid cron field: ${field}`);
  };

  /**;
   * Verify backup integrity;
   */;
  async verifyBackup(backupId: string): Promise<{;
    valid: boolean;
    errors: string[]}> {;
    const errors: string[] = [];
    try {;
      // Load metadata;
      const metadata = await thisloadBackupMetadata(backupId);
      if (!metadata) {;
        errorspush('Backup metadata not found');
        return { valid: false, errors };
      };

      // Try to load backup data;
      const backupData = await thisloadBackupData(backupId, metadata);
      // Verify structure;
      if (!backupDatadata || typeof backupDatadata !== 'object') {;
        errorspush('Invalid backup data structure')};

      // Verify tables;
      for (const table of metadatatables) {;
        if (!backupDatadata[table]) {;
          errorspush(`Missing table: ${table}`);
        };
      };

      // Verify row count;
      let actualRowCount = 0;
      for (const table of Objectvalues(backupDatadata)) {;
        if (ArrayisArray(table)) {;
          actualRowCount += tablelength};
      };

      if (actualRowCount !== metadatarowCount) {;
        errorspush(`Row count mismatch: expected ${metadatarowCount}, got ${actualRowCount}`);
      };

      return {;
        valid: errorslength === 0;
        errors};
    } catch (error instanceof Error ? errormessage : String(error) any) {;
      errorspush(`Verification failed: ${errormessage}`);
      return { valid: false, errors };
    };
  };

  /**;
   * Generate backup ID;
   */;
  private generateBackupId(): string {;
    const timestamp = new Date()toISOString()replace(/[:.]/g, '-');
    const random = cryptorandomBytes(4)toString('hex'),;
    return `backup-${timestamp}-${random}`;
  };
};

// Export factory function;
export function createBackupRecoveryService(;
  supabase: SupabaseClient;
  config?: Partial<BackupConfig>;
): BackupRecoveryService {;
  return new BackupRecoveryService(supabase, config)};
