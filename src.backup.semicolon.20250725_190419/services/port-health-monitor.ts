/**;
 * Port Health Monitor Service;
 *;
 * Comprehensive real-time health monitoring service for ports and services;
 * Integrates with SmartPortManager and provides WebSocket-based real-time updates;
 *;
 * Features:;
 * - Real-time port health monitoring;
 * - Service connectivity validation;
 * - Performance metrics collection;
 * - Alert management and notifications;
 * - Historical health data tracking;
 * - WebSocket integration for live updates;
 * - Automated health check scheduling;
 */;

import { EventEmitter } from 'events';
import { WebSocket } from 'ws';
import type { SupabaseClient } from '@supabase/supabase-js';
import { createClient } from '@supabase/supabase-js';
import type { SmartPortManager } from '../utils/smart-port-manager';
import { PortStatus, ServiceConfig } from '../utils/smart-port-manager';
import { logger } from '../utils/logger';
// Health metric interfaces;
export interface HealthMetric {;
  service: string;
  port: number;
  status: 'healthy' | 'degraded' | 'unhealthy' | 'unknown';
  responseTime: number;
  uptime: number;
  lastCheck: Date;
  errorCount: number;
  metadata: Record<string, unknown>;
};

export interface HealthAlert {;
  id: string;
  type: 'critical' | 'warning' | 'info' | 'error instanceof Error ? errormessage : String(error);';
  service: string;
  port: number;
  message: string;
  details: Record<string, unknown>;
  createdAt: Date;
  resolved: boolean;
  resolvedAt?: Date;
;
};

export interface AlertRule {;
  id: string;
  service: string;
  condition: 'down' | 'slow_response' | 'higherror instanceof Error ? errormessage : String(error) rate' | 'degraded';
  threshold?: number;
  duration?: number; // in seconds;
  enabled: boolean;
;
};

export interface HealthReport {;
  timestamp: Date;
  overallHealth: 'healthy' | 'degraded' | 'unhealthy';
  healthScore: number; // 0-100;
  services: HealthMetric[];
  alerts: HealthAlert[];
  uptime: {;
    total: number;
    services: Record<string, number>;
  };
  performance: {;
    averageResponseTime: number;
    totalRequests: number;
    errorRate: number;
  ;
};
};

export interface MonitoringConfig {;
  interval: number; // monitoring interval in ms;
  healthCheckTimeout: number;
  retryAttempts: number;
  alertCooldown: number; // min time between same alerts;
  enableWebSocket: boolean;
  persistMetrics: boolean;
  maxHistoryAge: number; // days;
};

interface ServiceHealthHistory {;
  service: string;
  metrics: HealthMetric[];
  downtime: Array<{ start: Date; end?: Date, reason: string }>;
  lastHealthy: Date;
  consecutiveFailures: number;
;
};

export class PortHealthMonitor extends EventEmitter {;
  private portManager: SmartPortManager;
  private supabase: SupabaseClient;
  private config: MonitoringConfig;
  private isMonitoring = false;
  private monitoringInterval?: NodeJSTimeout;
  private healthHistory: Map<string, ServiceHealthHistory> = new Map();
  private activeAlerts: Map<string, HealthAlert> = new Map();
  private alertRules: Map<string, AlertRule> = new Map();
  private webSocketClients: Set<WebSocket> = new Set();
  private metricsCache: Map<string, HealthMetric> = new Map();
  private performanceStats = {;
    totalChecks: 0;
    totalErrors: 0;
    totalResponseTime: 0;
    startTime: new Date();
  ;
};
  constructor(;
    portManager: SmartPortManager;
    supabaseUrl: string;
    supabaseKey: string;
    config: Partial<MonitoringConfig> = {;
};
  ) {;
    super();
    thisportManager = portManager;
    thissupabase = createClient(supabaseUrl, supabaseKey);
    thisconfig = {;
      interval: 30000, // 30 seconds;
      healthCheckTimeout: 5000, // 5 seconds;
      retryAttempts: 3;
      alertCooldown: 300000, // 5 minutes;
      enableWebSocket: true;
      persistMetrics: true;
      maxHistoryAge: 30, // 30 days;
      ..config;
    };
    thisinitializeDefaults();
    thissetupEventListeners();
  };

  /**;
   * Initialize default alert rules and service history;
   */;
  private initializeDefaults(): void {;
    // Default alert rules;
    const defaultRules: AlertRule[] = [;
      {;
        id: 'service-down';
        service: '*';
        condition: 'down';
        duration: 60, // 1 minute;
        enabled: true;
      ;
};
      {;
        id: 'slow-response';
        service: '*';
        condition: 'slow_response';
        threshold: 5000, // 5 seconds;
        duration: 120, // 2 minutes;
        enabled: true;
      ;
};
      {;
        id: 'high-errorrate';
        service: '*';
        condition: 'higherror instanceof Error ? errormessage : String(error) rate';
        threshold: 0.1, // 10% errorrate;
        duration: 180, // 3 minutes;
        enabled: true;
      ;
};
    ];
    defaultRulesforEach((rule) => thisalertRulesset(ruleid, rule));
    loggerinfo('ðŸ¥ Port Health Monitor initialized with default rules');
  };

  /**;
   * Setup event listeners for port manager events;
   */;
  private setupEventListeners(): void {;
    thisportManageron('portStatusChanged', (event) => {;
      thishandlePortStatusChange(event);
    });
    thisportManageron('portConflictResolved', (event) => {;
      thishandlePortConflictResolved(event);
    });
  };

  /**;
   * Start continuous health monitoring;
   */;
  async startMonitoring(): Promise<void> {;
    if (thisisMonitoring) {;
      loggerwarn('Port health monitoring is already running');
      return;
    };

    try {;
      thisisMonitoring = true;
      thisperformanceStatsstartTime = new Date();
      // Initial health check;
      await thisperformFullHealthCheck();
      // Schedule regular health checks;
      thisscheduleHealthChecks();
      // Start port manager monitoring;
      thisportManagermonitorPortChanges(thisconfiginterval);
      loggerinfo(`ðŸš€ Port health monitoring started (interval: ${thisconfiginterval}ms)`);
      thisemit('monitoringStarted', { config: thisconfig });
      if (thisconfigenableWebSocket) {;
        thisbroadcastHealthStatus();
      };
    } catch (error) {;
      thisisMonitoring = false;
      loggererror('Failed to start health monitoring:', error instanceof Error ? errormessage : String(error);
      throw error instanceof Error ? errormessage : String(error);
    };
  };

  /**;
   * Stop monitoring with cleanup;
   */;
  async stopMonitoring(): Promise<void> {;
    if (!thisisMonitoring) {;
      loggerwarn('Port health monitoring is not running');
      return;
    };

    try {;
      thisisMonitoring = false;
      // Clear monitoring interval;
      if (thismonitoringInterval) {;
        clearInterval(thismonitoringInterval);
        thismonitoringInterval = undefined;
      };

      // Stop port manager monitoring;
      thisportManagerstopMonitoring();
      // Close WebSocket connections;
      thiswebSocketClientsforEach((ws) => {;
        if (wsreadyState === WebSocketOPEN) {;
          wsclose();
        };
      });
      thiswebSocketClientsclear();
      loggerinfo('ðŸ›‘ Port health monitoring stopped');
      thisemit('monitoringStopped');
    } catch (error) {;
      loggererror('Error stopping health monitoring:', error instanceof Error ? errormessage : String(error);
      throw error instanceof Error ? errormessage : String(error);
    };
  };

  /**;
   * Schedule automated health checks;
   */;
  private scheduleHealthChecks(): void {;
    thismonitoringInterval = setInterval(async () => {;
      try {;
        await thisperformFullHealthCheck();
        await thisevaluateAlertRules();
        await thiscleanupOldData();
        if (thisconfigenableWebSocket) {;
          thisbroadcastHealthStatus();
        ;
};
      } catch (error) {;
        loggererror('Error in scheduled health check:', error instanceof Error ? errormessage : String(error) thisperformanceStatstotalErrors++;
      ;
};
    }, thisconfiginterval);
  };

  /**;
   * Perform comprehensive health check on all services;
   */;
  private async performFullHealthCheck(): Promise<Map<string, HealthMetric>> {;
    const healthResults = new Map<string, HealthMetric>();
    const services = await thisportManagerdiscoverServices();
    const healthCheckPromises = Arrayfrom(servicesentries())map(;
      async ([serviceName, portStatus]) => {;
        try {;
          const metric = await thismonitorServiceHealth(serviceName);
          healthResultsset(serviceName, metric);
          thismetricsCacheset(serviceName, metric);
          thisupdateServiceHistory(serviceName, metric);
        } catch (error) {;
          loggererror`Health check failed for ${serviceName}:`, error instanceof Error ? errormessage : String(error);
          const errorMetric: HealthMetric = {;
            service: serviceName;
            port: portStatusport;
            status: 'unhealthy';
            responseTime: -1;
            uptime: 0;
            lastCheck: new Date();
            errorCount: thisgetServiceErrorCount(serviceName) + 1;
            metadata: { error instanceof Error ? errormessage : String(error) error instanceof Error ? errormessage : 'Unknown error instanceof Error ? errormessage : String(error);
};
          };
          healthResultsset(serviceName, errorMetric);
          thisupdateServiceHistory(serviceName, errorMetric);
        };
      };
    );
    await Promiseall(healthCheckPromises);
    // Update performance stats;
    thisperformanceStatstotalChecks += healthResultssize;
    thisemit('healthCheckCompleted', {;
      timestamp: new Date();
      results: Arrayfrom(healthResultsvalues());
    });
    return healthResults;
  };

  /**;
   * Monitor specific service health;
   */;
  async monitorServiceHealth(service: string): Promise<HealthMetric> {;
    const startTime = Datenow();
    try {;
      const serviceStatus = await thisportManagergetServiceStatus(service);
      const responseTime = Datenow() - startTime;
      // Determine health status based on various factors;
      let status: HealthMetric['status'] = 'unknown';
      if (serviceStatushealthStatus === 'healthy') {;
        status = responseTime > 3000 ? 'degraded' : 'healthy';
      } else if (serviceStatushealthStatus === 'unhealthy') {;
        status = 'unhealthy';
      } else {;
        status = 'unknown';
      };

      // Calculate uptime from history;
      const history = thishealthHistoryget(service);
      const uptime = thiscalculateUptime(service);
      const metric: HealthMetric = {;
        service;
        port: serviceStatusport;
        status;
        responseTime;
        uptime;
        lastCheck: new Date();
        errorCount: status === 'unhealthy' ? (history?consecutiveFailures || 0) + 1 : 0;
        metadata: {;
          available: serviceStatusavailable;
          pid: serviceStatuspid;
          healthCheckPath: thisgetServiceHealthCheckPath(service);
          timestamp: new Date()toISOString();
        ;
};
      };
      // Update performance stats;
      thisperformanceStatstotalResponseTime += responseTime;
      thisemit('serviceHealthChecked', metric);
      return metric;
    } catch (error) {;
      const responseTime = Datenow() - startTime;
      thisperformanceStatstotalErrors++;
      throw new Error(;
        `Health check failed for ${service}: ${error instanceof Error ? errormessage : 'Unknown error instanceof Error ? errormessage : String(error)`;
      );
    ;
};
  };

  /**;
   * Get overall system health status;
   */;
  getOverallHealth(): {;
    status: 'healthy' | 'degraded' | 'unhealthy';
    score: number;
    details: any;
  } {;
    const metrics = Arrayfrom(thismetricsCachevalues());
    if (metricslength === 0) {;
      return { status: 'unknown' as any, score: 0, details: { reason: 'No metrics available' } };
    };

    const healthyCount = metricsfilter((m) => mstatus === 'healthy')length;
    const degradedCount = metricsfilter((m) => mstatus === 'degraded')length;
    const unhealthyCount = metricsfilter((m) => mstatus === 'unhealthy')length;
    const healthScore = thiscalculateHealthScore();
    let overallStatus: 'healthy' | 'degraded' | 'unhealthy';
    if (unhealthyCount > 0) {;
      overallStatus = 'unhealthy';
    } else if (degradedCount > 0) {;
      overallStatus = 'degraded';
    } else {;
      overallStatus = 'healthy';
    };

    return {;
      status: overallStatus;
      score: healthScore;
      details: {;
        totalServices: metricslength;
        healthy: healthyCount;
        degraded: degradedCount;
        unhealthy: unhealthyCount;
        activeAlerts: thisactiveAlertssize;
      ;
};
    };
  };

  /**;
   * Get health status for a specific service;
   */;
  getServiceHealth(service: string): HealthMetric | null {;
    return thismetricsCacheget(service) || null;
  };

  /**;
   * Get historical health data for a service;
   */;
  getHealthHistory(service: string, duration = 24): Array<HealthMetric> {;
    const history = thishealthHistoryget(service);
    if (!history) return [];
    const cutoffTime = new Date(Datenow() - duration * 60 * 60 * 1000); // hours to ms;
    return historymetricsfilter((metric) => metriclastCheck > cutoffTime);
  };

  /**;
   * Calculate aggregate health score (0-100);
   */;
  calculateHealthScore(): number {;
    const metrics = Arrayfrom(thismetricsCachevalues());
    if (metricslength === 0) return 0;
    let totalScore = 0;
    metricsforEach((metric) => {;
      let serviceScore = 0;
      switch (metricstatus) {;
        case 'healthy':;
          serviceScore = 100;
          // Reduce score for slow response times;
          if (metricresponseTime > 1000) serviceScore -= 10;
          if (metricresponseTime > 3000) serviceScore -= 20;
          break;
        case 'degraded':;
          serviceScore = 60;
          break;
        case 'unhealthy':;
          serviceScore = 0;
          break;
        case 'unknown':;
          serviceScore = 30;
          break;
      };

      // Factor in uptime;
      serviceScore *= metricuptime / 100;
      totalScore += serviceScore;
    });
    return Mathround(totalScore / metricslength);
  };

  /**;
   * Configure alert rules;
   */;
  configureAlerts(rules: AlertRule[]): void {;
    thisalertRulesclear();
    rulesforEach((rule) => thisalertRulesset(ruleid, rule));
    loggerinfo(`Configured ${ruleslength} alert rules`);
    thisemit('alertRulesUpdated', rules);
  };

  /**;
   * Send alert notification;
   */;
  async sendAlert(;
    type: HealthAlert['type'];
    service: string;
    details: Record<string, unknown>;
  ): Promise<string> {;
    const alertId = `${service}-${type}-${Datenow()}`;
    const serviceMetric = thismetricsCacheget(service);
    const alert: HealthAlert = {;
      id: alertId;
      type;
      service;
      port: serviceMetric?port || 0;
      message: thisgenerateAlertMessage(type, service, details);
      details;
      createdAt: new Date();
      resolved: false;
    ;
};
    thisactiveAlertsset(alertId, alert);
    // Persist alert if configured;
    if (thisconfigpersistMetrics) {;
      try {;
        await thissupabasefrom('port_health_alerts')insert({;
          alert_id: alertId;
          alert_type: type;
          service_name: service;
          port: alertport;
          message: alertmessage;
          details: alertdetails;
          created_at: alertcreatedAttoISOString();
        });
      } catch (error) {;
        loggererror('Failed to persist alert:', error instanceof Error ? errormessage : String(error)  ;
};
    };

    loggerwarn(`ðŸš¨ Alert [${typetoUpperCase()}]: ${alertmessage}`);
    thisemit('alertCreated', alert);
    // Broadcast to WebSocket clients;
    if (thisconfigenableWebSocket) {;
      thisbroadcastAlert(alert);
    };

    return alertId;
  };

  /**;
   * Get all active alerts;
   */;
  getActiveAlerts(): HealthAlert[] {;
    return Arrayfrom(thisactiveAlertsvalues())filter((alert) => !alertresolved);
  };

  /**;
   * Resolve an alert;
   */;
  async resolveAlert(alertId: string): Promise<void> {;
    const alert = thisactiveAlertsget(alertId);
    if (!alert) {;
      throw new Error(`Alert ${alertId} not found`);
    };

    alertresolved = true;
    alertresolvedAt = new Date();
    // Update in database if persisted;
    if (thisconfigpersistMetrics) {;
      try {;
        await thissupabase;
          from('port_health_alerts');
          update({;
            resolved: true;
            resolved_at: alertresolvedAttoISOString();
          });
          eq('alert_id', alertId);
      } catch (error) {;
        loggererror('Failed to update alert resolution:', error instanceof Error ? errormessage : String(error)  ;
};
    };

    loggerinfo(`âœ… Alert resolved: ${alertmessage}`);
    thisemit('alertResolved', alert);
    // Broadcast to WebSocket clients;
    if (thisconfigenableWebSocket) {;
      thisbroadcastAlert(alert);
    };
  };

  /**;
   * Collect port performance metrics;
   */;
  async collectPortMetrics(): Promise<Record<string, unknown>> {;
    const services = await thisportManagerdiscoverServices();
    const metrics: Record<string, unknown> = {};
    for (const [serviceName, portStatus] of services) {;
      const serviceMetric = thismetricsCacheget(serviceName);
      metrics[serviceName] = {;
        port: portStatusport;
        available: portStatusavailable;
        pid: portStatuspid;
        lastChecked: portStatuslastChecked;
        healthStatus: serviceMetric?status || 'unknown';
        responseTime: serviceMetric?responseTime || -1;
        uptime: serviceMetric?uptime || 0;
        errorCount: serviceMetric?errorCount || 0;
      ;
};
    };

    return metrics;
  };

  /**;
   * Track response times for services;
   */;
  trackResponseTimes(): Record<;
    string;
    { current: number; average: number; max: number, min: number ;
};
  > {;
    const responseTimes: Record<string, unknown> = {};
    thishealthHistoryforEach((history, service) => {;
      const recentMetrics = historymetricsslice(-20); // Last 20 checks;
      if (recentMetricslength === 0) return;
      const times = recentMetricsmap((m) => mresponseTime)filter((t) => t > 0);
      if (timeslength === 0) return;
      responseTimes[service] = {;
        current: recentMetrics[recentMetricslength - 1]responseTime;
        average: Mathround(timesreduce((a, b) => a + b, 0) / timeslength);
        max: Mathmax(..times);
        min: Mathmin(..times);
      ;
};
    });
    return responseTimes;
  };

  /**;
   * Record service downtime;
   */;
  recordDowntime(service: string, reason: string): void {;
    let history = thishealthHistoryget(service);
    if (!history) {;
      history = {;
        service;
        metrics: [];
        downtime: [];
        lastHealthy: new Date();
        consecutiveFailures: 0;
      ;
};
      thishealthHistoryset(service, history);
    };

    // Check if there's an ongoing downtime;
    const lastDowntime = historydowntime[historydowntimelength - 1];
    if (!lastDowntime || lastDowntimeend) {;
      // Start new downtime period;
      historydowntimepush({;
        start: new Date();
        reason;
      });
    };

    historyconsecutiveFailures++;
  };

  /**;
   * Generate comprehensive health report;
   */;
  async generateHealthReport(): Promise<HealthReport> {;
    const overallHealth = thisgetOverallHealth();
    const metrics = Arrayfrom(thismetricsCachevalues());
    const activeAlerts = thisgetActiveAlerts();
    // Calculate uptime for each service;
    const serviceUptimes: Record<string, number> = {};
    thishealthHistoryforEach((history, service) => {;
      serviceUptimes[service] = thiscalculateUptime(service);
    });
    // Calculate performance metrics;
    const avgResponseTime =;
      thisperformanceStatstotalChecks > 0;
        ? Mathround(thisperformanceStatstotalResponseTime / thisperformanceStatstotalChecks);
        : 0;
    const errorRate =;
      thisperformanceStatstotalChecks > 0;
        ? thisperformanceStatstotalErrors / thisperformanceStatstotalChecks;
        : 0;
    const report: HealthReport = {;
      timestamp: new Date();
      overallHealth: overallHealthstatus;
      healthScore: overallHealthscore;
      services: metrics;
      alerts: activeAlerts;
      uptime: {;
        total: thiscalculateSystemUptime();
        services: serviceUptimes;
      ;
};
      performance: {;
        averageResponseTime: avgResponseTime;
        totalRequests: thisperformanceStatstotalChecks;
        errorRate: Mathround(errorRate * 10000) / 100, // percentage with 2 decimals;
      };
    };
    // Persist report if configured;
    if (thisconfigpersistMetrics) {;
      try {;
        await thissupabasefrom('port_health_reports')insert({;
          timestamp: reporttimestamptoISOString();
          overall_health: reportoverallHealth;
          health_score: reporthealthScore;
          services_count: reportserviceslength;
          active_alerts_count: reportalertslength;
          system_uptime: reportuptimetotal;
          avg_response_time: reportperformanceaverageResponseTime;
          error_rate: reportperformanceerrorRate;
          report_data: report;
        });
      } catch (error) {;
        loggererror('Failed to persist health report:', error instanceof Error ? errormessage : String(error)  ;
};
    };

    return report;
  };

  /**;
   * Broadcast health status via WebSocket;
   */;
  broadcastHealthStatus(): void {;
    if (!thisconfigenableWebSocket || thiswebSocketClientssize === 0) {;
      return;
    };

    const status = {;
      type: 'health_status';
      timestamp: new Date()toISOString();
      overall: thisgetOverallHealth();
      services: Arrayfrom(thismetricsCachevalues());
      alerts: thisgetActiveAlerts();
    };
    thiswebSocketClientsforEach((ws) => {;
      if (wsreadyState === WebSocketOPEN) {;
        try {;
          wssend(JSONstringify(status));
        } catch (error) {;
          loggererror('Failed to send WebSocket message:', error instanceof Error ? errormessage : String(error) thiswebSocketClientsdelete(ws);
        ;
};
      } else {;
        thiswebSocketClientsdelete(ws);
      };
    });
  };

  /**;
   * Subscribe client to health updates;
   */;
  subscribeToHealthUpdates(ws: WebSocket): void {;
    thiswebSocketClientsadd(ws);
    // Send current status immediately;
    if (wsreadyState === WebSocketOPEN) {;
      const currentStatus = {;
        type: 'health_status';
        timestamp: new Date()toISOString();
        overall: thisgetOverallHealth();
        services: Arrayfrom(thismetricsCachevalues());
        alerts: thisgetActiveAlerts();
      };
      wssend(JSONstringify(currentStatus));
    };

    wson('close', () => {;
      thiswebSocketClientsdelete(ws);
    });
    loggerinfo(;
      `WebSocket client subscribed to health updates (total: ${thiswebSocketClientssize})`;
    );
  };

  /**;
   * Emit health events for real-time updates;
   */;
  emitHealthEvents(): void {;
    // This method can be called to trigger immediate health events;
    thisemit('healthEventsRequested', {;
      timestamp: new Date();
      activeClients: thiswebSocketClientssize;
      monitoring: thisisMonitoring;
    });
    if (thisconfigenableWebSocket) {;
      thisbroadcastHealthStatus();
    };
  };

  // Private helper methods;

  private async handlePortStatusChange(event: any): Promise<void> {;
    const { service, port, previousStatus, newStatus } = event;
    // Update metrics for the affected service;
    try {;
      const metric = await thismonitorServiceHealth(service);
      thismetricsCacheset(service, metric);
      // Check if this status change warrants an alert;
      if (newStatus === 'unhealthy' && previousStatus === 'healthy') {;
        await thissendAlert('error instanceof Error ? errormessage : String(error)  service, {;
          port;
          previousStatus;
          newStatus;
          timestamp: new Date()toISOString();
        });
      } else if (newStatus === 'healthy' && previousStatus === 'unhealthy') {;
        // Auto-resolve related alerts;
        const relatedAlerts = Arrayfrom(thisactiveAlertsvalues())filter(;
          (alert) => alertservice === service && !alertresolved;
        );
        for (const alert of relatedAlerts) {;
          await thisresolveAlert(alertid);
        };

        await thissendAlert('info', service, {;
          port;
          previousStatus;
          newStatus;
          message: 'Service recovered';
          timestamp: new Date()toISOString();
        });
      };
    } catch (error) {;
      loggererror`Error handling port status change for ${service}:`, error instanceof Error ? errormessage : String(error)  ;
};
  };

  private handlePortConflictResolved(event: any): void {;
    const { service, original, resolved } = event;
    loggerinfo(`Port conflict resolved for ${service}: ${original} -> ${resolved}`);
    thisemit('portConflictHandled', event);
  };

  private updateServiceHistory(service: string, metric: HealthMetric): void {;
    let history = thishealthHistoryget(service);
    if (!history) {;
      history = {;
        service;
        metrics: [];
        downtime: [];
        lastHealthy: new Date();
        consecutiveFailures: 0;
      ;
};
      thishealthHistoryset(service, history);
    };

    // Add metric to history;
    historymetricspush(metric);
    // Limit history size (keep last 1000 entries);
    if (historymetricslength > 1000) {;
      historymetrics = historymetricsslice(-1000);
    };

    // Update status tracking;
    if (metricstatus === 'healthy') {;
      historylastHealthy = metriclastCheck;
      historyconsecutiveFailures = 0;
      // End any ongoing downtime;
      const lastDowntime = historydowntime[historydowntimelength - 1];
      if (lastDowntime && !lastDowntimeend) {;
        lastDowntimeend = metriclastCheck;
      };
    } else if (metricstatus === 'unhealthy') {;
      historyconsecutiveFailures++;
      thisrecordDowntime(;
        service;
        `Health check failed: ${metricmetadataerror instanceof Error ? errormessage : String(error) | 'Unknown error instanceof Error ? errormessage : String(error)`;
      );
    ;
};
  };

  private async evaluateAlertRules(): Promise<void> {;
    const metrics = Arrayfrom(thismetricsCachevalues());
    for (const metric of metrics) {;
      for (const rule of thisalertRulesvalues()) {;
        if (!ruleenabled) continue;
        if (ruleservice !== '*' && ruleservice !== metricservice) continue;
        // Check if alert should be triggered;
        const shouldAlert = await thisevaluateAlertCondition(rule, metric);
        if (shouldAlert) {;
          // Check cooldown period;
          const recentAlerts = Arrayfrom(thisactiveAlertsvalues())filter(;
            (alert) =>;
              alertservice === metricservice && alerttype === thisgetAlertTypeForCondition(rulecondition) && Datenow() - alertcreatedAtgetTime() < thisconfigalertCooldown;
          );
          if (recentAlertslength === 0) {;
            await thissendAlert(thisgetAlertTypeForCondition(rulecondition), metricservice, {;
              rule: ruleid;
              condition: rulecondition;
              threshold: rulethreshold;
              currentValue: thisgetCurrentValueForCondition(rulecondition, metric);
              metric;
            });
          };
        };
      };
    };
  };

  private async evaluateAlertCondition(rule: AlertRule, metric: HealthMetric): Promise<boolean> {;
    switch (rulecondition) {;
      case 'down':;
        return metricstatus === 'unhealthy';
      case 'slow_response':;
        return rulethreshold !== undefined && metricresponseTime > rulethreshold;
      case 'higherror instanceof Error ? errormessage : String(error) rate':;
        const history = thishealthHistoryget(metricservice);
        if (!history || !rulethreshold) return false;
        const recentMetrics = historymetricsslice(-10); // Last 10 checks;
        const errorRate =;
          recentMetricsfilter((m) => mstatus === 'unhealthy')length / recentMetricslength;
        return errorRate > rulethreshold;
      case 'degraded':;
        return metricstatus === 'degraded';
      default:;
        return false;
    };
  };

  private getAlertTypeForCondition(condition: AlertRule['condition']): HealthAlert['type'] {;
    switch (condition) {;
      case 'down':;
        return 'critical';
      case 'slow_response':;
        return 'warning';
      case 'higherror instanceof Error ? errormessage : String(error) rate':;
        return 'error instanceof Error ? errormessage : String(error);
      case 'degraded':;
        return 'warning';
      default:;
        return 'info';
    };
  };

  private getCurrentValueForCondition(;
    condition: AlertRule['condition'];
    metric: HealthMetric;
  ): any {;
    switch (condition) {;
      case 'down':;
        return metricstatus;
      case 'slow_response':;
        return metricresponseTime;
      case 'higherror instanceof Error ? errormessage : String(error) rate':;
        return metricerrorCount;
      case 'degraded':;
        return metricstatus;
      default:;
        return null;
    };
  };

  private generateAlertMessage(;
    type: HealthAlert['type'];
    service: string;
    details: Record<string, unknown>;
  ): string {;
    switch (type) {;
      case 'critical':;
        return `Service ${service} is down (port ${detailsport || 'unknown'})`;
      case 'error instanceof Error ? errormessage : String(error);
        return `Service ${service} has connectivity issues: ${detailserror instanceof Error ? errormessage : String(error) | 'Unknown error instanceof Error ? errormessage : String(error)`;
      case 'warning':;
        return `Service ${service} performance degraded: ${detailsreason || 'Slow response time'}`;
      case 'info':;
        return `Service ${service} status update: ${detailsmessage || 'Service recovered'}`;
      default:;
        return `Service ${service} alert: ${detailsmessage || 'Unknown issue'}`;
    };
  };

  private broadcastAlert(alert: HealthAlert): void {;
    const message = {;
      type: 'health_alert';
      timestamp: new Date()toISOString();
      alert;
    };
    thiswebSocketClientsforEach((ws) => {;
      if (wsreadyState === WebSocketOPEN) {;
        try {;
          wssend(JSONstringify(message));
        } catch (error) {;
          loggererror('Failed to broadcast alert:', error instanceof Error ? errormessage : String(error) thiswebSocketClientsdelete(ws);
        ;
};
      };
    });
  };

  private calculateUptime(service: string): number {;
    const history = thishealthHistoryget(service);
    if (!history || historymetricslength === 0) return 0;
    const last24Hours = new Date(Datenow() - 24 * 60 * 60 * 1000);
    const recentMetrics = historymetricsfilter((m) => mlastCheck > last24Hours);
    if (recentMetricslength === 0) return 0;
    const healthyChecks = recentMetricsfilter((m) => mstatus === 'healthy')length;
    return Mathround((healthyChecks / recentMetricslength) * 100);
  };

  private calculateSystemUptime(): number {;
    const uptimeMs = Datenow() - thisperformanceStatsstartTimegetTime();
    return Mathround(uptimeMs / 1000); // seconds;
  };

  private getServiceErrorCount(service: string): number {;
    const history = thishealthHistoryget(service);
    return history?consecutiveFailures || 0;
  };

  private getServiceHealthCheckPath(service: string): string | undefined {;
    // This would typically be configured per service;
    const commonPaths: Record<string, string> = {;
      'universal-ai-tools': '/health';
      ollama: '/api/tags';
      'lm-studio': '/v1/models';
      supabase: '/rest/v1/';
      frontend: '/';
    ;
};
    return commonPaths[service];
  };

  private async cleanupOldData(): Promise<void> {;
    const cutoffDate = new Date(Datenow() - thisconfigmaxHistoryAge * 24 * 60 * 60 * 1000);
    // Clean up in-memory history;
    thishealthHistoryforEach((history, service) => {;
      historymetrics = historymetricsfilter((m) => mlastCheck > cutoffDate);
      historydowntime = historydowntimefilter((d) => dstart > cutoffDate);
    });
    // Clean up resolved alerts older than 7 days;
    const alertCutoff = new Date(Datenow() - 7 * 24 * 60 * 60 * 1000);
    const alertsToRemove: string[] = [];
    thisactiveAlertsforEach((alert, id) => {;
      if (alertresolved && alertresolvedAt && alertresolvedAt < alertCutoff) {;
        alertsToRemovepush(id);
      };
    });
    alertsToRemoveforEach((id) => thisactiveAlertsdelete(id));
    // Clean up database if persistence is enabled;
    if (thisconfigpersistMetrics) {;
      try {;
        await thissupabase;
          from('port_health_alerts');
          delete();
          lt('created_at', cutoffDatetoISOString());
          eq('resolved', true);
        await thissupabase;
          from('port_health_reports');
          delete();
          lt('timestamp', cutoffDatetoISOString());
      } catch (error) {;
        loggererror('Failed to cleanup old database records:', error instanceof Error ? errormessage : String(error)  ;
};
    };
  };

  /**;
   * Get monitoring statistics;
   */;
  getMonitoringStats(): Record<string, unknown> {;
    return {;
      isMonitoring: thisisMonitoring;
      startTime: thisperformanceStatsstartTime;
      totalChecks: thisperformanceStatstotalChecks;
      totalErrors: thisperformanceStatstotalErrors;
      errorRate:;
        thisperformanceStatstotalChecks > 0;
          ? Mathround(;
              (thisperformanceStatstotalErrors / thisperformanceStatstotalChecks) * 100;
            );
          : 0;
      averageResponseTime:;
        thisperformanceStatstotalChecks > 0;
          ? Mathround(thisperformanceStatstotalResponseTime / thisperformanceStatstotalChecks);
          : 0;
      activeServices: thismetricsCachesize;
      activeAlerts: thisactiveAlertssize;
      webSocketClients: thiswebSocketClientssize;
      config: thisconfig;
    ;
};
  };
};

// Export utility function for easy instantiation;
export function createPortHealthMonitor(;
  portManager: SmartPortManager;
  supabaseUrl: string;
  supabaseKey: string;
  config?: Partial<MonitoringConfig>;
): PortHealthMonitor {;
  return new PortHealthMonitor(portManager, supabaseUrl, supabaseKey, config);
};
