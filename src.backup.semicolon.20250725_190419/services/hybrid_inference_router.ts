/**;
 * Hybrid Inference Router;
 * Intelligently routes inference requests between MLX and Ollama based on requirements;
 */;

import { EventEmitter } from 'events';
import { EmbeddedModelManager } from './embedded_model_manager';
import { ModelLifecycleManager } from './model_lifecycle_manager';
import { exec } from 'child_process';
import { promisify } from 'util';
const execAsync = promisify(exec);
interface InferenceRequest {;
  prompt: string;
  modelPreference?: string;
  maxTokens?: number;
  temperature?: number;
  streaming?: boolean;
  timeout?: number;
  priority?: 'low' | 'medium' | 'high' | 'critical';
};

interface InferenceResponse {;
  text: string;
  model: string;
  engine: 'mlx' | 'ollama' | 'hybrid';
  tokensPerSecond?: number;
  totalTokens?: number;
  latencyMs: number;
  confidence?: number;
};

interface RoutingDecision {;
  engine: 'mlx' | 'ollama' | 'hybrid';
  model: string;
  reasoning: string;
  complexity: number;
  needsSpeed: boolean;
  needsStreaming: boolean;
  isMultimodal: boolean;
  modelSize: number;
};

interface PerformanceStats {;
  mlx: {;
    totalRequests: number;
    averageLatency: number;
    successRate: number;
};
  ollama: {;
    totalRequests: number;
    averageLatency: number;
    successRate: number;
};
};

export class HybridInferenceRouter extends EventEmitter {;
  private embeddedManager: EmbeddedModelManager;
  private lifecycleManager: ModelLifecycleManager;
  private performanceStats: PerformanceStats;
  private routingCache: Map<string, RoutingDecision> = new Map();
  constructor(embeddedManager?: EmbeddedModelManager, lifecycleManager?: ModelLifecycleManager) {;
    super();
    thisembeddedManager = embeddedManager || new EmbeddedModelManager();
    thislifecycleManager = lifecycleManager || new ModelLifecycleManager();

    thisperformanceStats = {;
      mlx: { totalRequests: 0, averageLatency: 0, successRate: 1.0 };
      ollama: { totalRequests: 0, averageLatency: 0, successRate: 1.0 }};
  };

  /**;
   * Route inference request to optimal engine;
   */;
  async route(requestInferenceRequest): Promise<InferenceResponse> {;
    const startTime = Datenow(),;

    try {;
      // Analyze request to determine routing;
      const routing = await thisanalyzeRequest(request;

      // Log routing decision;
      thisemit('routing-decision', {;
        requestrequestpromptsubstring(0, 100);
        decision: routing});
      let response: InferenceResponse;
      // Execute based on routing decision;
      switch (routingengine) {;
        case 'mlx':;
          response = await thismlxInference(requestrouting);
          break;
        case 'ollama':;
          response = await thisollamaInference(requestrouting);
          break;
        case 'hybrid':;
          response = await thishybridInference(requestrouting);
          break;
        default:;
          response = await thisselectOptimalEngine(requestrouting)};

      // Update stats;
      thisupdatePerformanceStats(routingengine, Datenow() - startTime, true);
      return response;
    } catch (error) {;
      const latency = Datenow() - startTime,;
      thisemit('routing-error instanceof Error ? errormessage : String(error)  { error instanceof Error ? errormessage : String(error)latency });
      throw error instanceof Error ? errormessage : String(error);
    };
  };

  /**;
   * Analyze request to determine optimal routing;
   */;
  private async analyzeRequest(requestInferenceRequest): Promise<RoutingDecision> {;
    // Check cache first;
    const cacheKey = thisgenerateCacheKey(request;
    const cached = thisroutingCacheget(cacheKey);
    if (cached) {;
      return cached};

    // Analyze requestcharacteristics;
    const complexity = thisassessComplexity(requestprompt);
    const needsSpeed =;
      requestpriority === 'critical' || (requesttimeout !== undefined && requesttimeout < 5000);
    const needsStreaming = requeststreaming || false;
    const isMultimodal = thisdetectMultimodal(requestprompt);
    const modelSize = thisestimateRequiredModelSize(complexity, requestprompt);
    // Determine optimal engine;
    let engine: 'mlx' | 'ollama' | 'hybrid';
    let model: string;
    let reasoning: string;
    if (needsSpeed && modelSize < 4e9) {;
      engine = 'mlx';
      model = thisselectMLXModel(modelSize);
      reasoning = 'Fast response needed with small model'} else if (needsStreaming || isMultimodal) {;
      engine = 'ollama';
      model = thisselectOllamaModel(modelSize, isMultimodal);
      reasoning = 'Streaming or multimodal capabilities required'} else if (complexity > 8) {;
      engine = 'hybrid';
      model = 'deepseek-r1:14b';
      reasoning = 'Complex task requiring multi-stage processing'} else {;
      // Default: choose based on performance stats;
      engine = thisselectOptimalEngineByStats();
      model = thisselectModelBySize(modelSize, engine);
      reasoning = 'Selected based on performance history'};

    const decision: RoutingDecision = {;
      engine;
      model;
      reasoning;
      complexity;
      needsSpeed;
      needsStreaming;
      isMultimodal;
      modelSize};
    // Cache decision;
    thisroutingCacheset(cacheKey, decision);
    // Clear old cache entries;
    if (thisroutingCachesize > 1000) {;
      const firstKey = thisroutingCachekeys()next()value;
      if (firstKey !== undefined) {;
        thisroutingCachedelete(firstKey)};
    };

    return decision;
  };

  /**;
   * MLX inference;
   */;
  private async mlxInference(;
    requestInferenceRequest;
    routing: RoutingDecision;
  ): Promise<InferenceResponse> {;
    const startTime = Datenow();
    // Ensure model is embedded;
    if (!(await thisisModelEmbedded(routingmodel))) {;
      await thisembeddedManagerembedModel(routingmodel)};

    const text = await thisembeddedManagergenerate(;
      routingmodel;
      requestprompt;
      requestmaxTokens || 100;
    );
    return {;
      text;
      model: routingmodel;
      engine: 'mlx';
      latencyMs: Datenow() - startTime;
      tokensPerSecond: thiscalculateTokensPerSecond(text, Datenow() - startTime)};
  };

  /**;
   * Ollama inference;
   */;
  private async ollamaInference(;
    requestInferenceRequest;
    routing: RoutingDecision;
  ): Promise<InferenceResponse> {;
    const startTime = Datenow(),;

    // Use lifecycle manager to ensure model is ready;
    await thislifecycleManagerpredictAndWarm({ userRequest: requestprompt });
    const command = thisbuildOllamaCommand(routingmodel, request;
    const { stdout } = await execAsync(command);
    return {;
      text: stdouttrim();
      model: routingmodel;
      engine: 'ollama';
      latencyMs: Datenow() - startTime;
      tokensPerSecond: thiscalculateTokensPerSecond(stdout, Datenow() - startTime)};
  };

  /**;
   * Hybrid inference using multiple models;
   */;
  private async hybridInference(;
    requestInferenceRequest;
    routing: RoutingDecision;
  ): Promise<InferenceResponse> {;
    const startTime = Datenow();
    // Step 1: Use small MLX model for planning;
    const planningModel = 'phi:2.7b';
    await thisembeddedManagerembedModel(planningModel);

    const plan = await thisembeddedManagergenerate(;
      planningModel;
      `Plan approach for: ${requestprompt}`;
      50;
    );
    // Step 2: Determine execution engine based on plan;
    const executionComplexity = thisassessComplexity(plan);
    const executionEngine = executionComplexity > 7 ? 'ollama' : 'mlx';
    // Step 3: Execute with appropriate engine;
    let finalResponse: string;
    if (executionEngine === 'ollama') {;
      const { stdout } = await execAsync(;
        thisbuildOllamaCommand(routingmodel, {;
          ..request;
          prompt: `${plan}\n\nNow execute: ${requestprompt}`});
      );
      finalResponse = stdouttrim();
    } else {;
      finalResponse = await thisembeddedManagergenerate(;
        'qwen2.5: 7b';
        `${plan}\n\nNow execute: ${requestprompt}`;
        requestmaxTokens || 100;
      );
    };

    return {;
      text: finalResponse;
      model: `${planningModel}+${routingmodel}`;
      engine: 'hybrid';
      latencyMs: Datenow() - startTime;
      confidence: 0.9, // Higher confidence due to multi-stage processing;
    };
  };

  /**;
   * Select optimal engine based on current conditions;
   */;
  private async selectOptimalEngine(;
    requestInferenceRequest;
    routing: RoutingDecision;
  ): Promise<InferenceResponse> {;
    // Compare current performance stats;
    const mlxScore = thiscalculateEngineScore('mlx');
    const ollamaScore = thiscalculateEngineScore('ollama');
    if (mlxScore > ollamaScore && routingmodelSize < 8e9) {;
      return thismlxInference(requestrouting)} else {;
      return thisollamaInference(requestrouting)};
  };

  /**;
   * Assess prompt complexity;
   */;
  private assessComplexity(prompt: string): number {;
    let complexity = 0;
    // Length factor;
    complexity += Mathmin(promptlength / 100, 3);
    // Technical terms;
    const technicalTerms = ['algorithm', 'implement', 'analyze', 'optimize', 'architecture'];
    complexity += technicalTermsfilter((term) => prompttoLowerCase()includes(term))length * 0.5;
    // Multi-step indicators;
    const multiStepIndicators = ['first', 'then', 'finally', 'step', 'phase'];
    complexity += multiStepIndicatorsfilter((term) => prompttoLowerCase()includes(term))length;
    // Code detection;
    if (promptincludes('```') || promptincludes('function') || promptincludes('class')) {;
      complexity += 2};

    return Mathmin(complexity, 10);
  };

  /**;
   * Detect if requestneeds multimodal capabilities;
   */;
  private detectMultimodal(prompt: string): boolean {;
    const multimodalIndicators = ['image', 'picture', 'photo', 'diagram', 'chart', 'video'];
    return multimodalIndicatorssome((indicator) => prompttoLowerCase()includes(indicator))};

  /**;
   * Estimate required model size based on task;
   */;
  private estimateRequiredModelSize(complexity: number, prompt: string): number {;
    if (complexity < 3) return 2e9; // 2B;
    if (complexity < 5) return 7e9; // 7B;
    if (complexity < 8) return 14e9; // 14B;
    return 24e9; // 24B+};

  /**;
   * Select MLX model based on size requirements;
   */;
  private selectMLXModel(size: number): string {;
    if (size <= 2e9) return 'gemma:2b';
    if (size <= 3e9) return 'phi:2.7b';
    return 'qwen2.5:7b'; // Largest we'll embed};

  /**;
   * Select Ollama model based on requirements;
   */;
  private selectOllamaModel(size: number, isMultimodal: boolean): string {;
    if (isMultimodal) return 'llava:7b';
    if (size <= 7e9) return 'qwen2.5:7b';
    if (size <= 14e9) return 'deepseek-r1:14b';
    return 'devstral:24b'};

  /**;
   * Select optimal engine based on performance stats;
   */;
  private selectOptimalEngineByStats(): 'mlx' | 'ollama' {;
    const mlxScore = thiscalculateEngineScore('mlx');
    const ollamaScore = thiscalculateEngineScore('ollama');
    return mlxScore > ollamaScore ? 'mlx' : 'ollama'};

  /**;
   * Calculate engine performance score;
   */;
  private calculateEngineScore(engine: 'mlx' | 'ollama'): number {;
    const stats = thisperformanceStats[engine];
    if (statstotalRequests === 0) return 0.5;
    // Weighted score: success rate (60%) + speed (40%);
    const speedScore = Mathmax(0, 1 - statsaverageLatency / 10000); // 10s max;
    return statssuccessRate * 0.6 + speedScore * 0.4};

  /**;
   * Select model by size and engine;
   */;
  private selectModelBySize(size: number, engine: 'mlx' | 'ollama'): string {;
    if (engine === 'mlx') {;
      return thisselectMLXModel(size)} else {;
      return thisselectOllamaModel(size, false)};
  };

  /**;
   * Check if model is embedded;
   */;
  private async isModelEmbedded(model: string): Promise<boolean> {;
    const status = thisembeddedManagergetModelStatus();
    return model in status};

  /**;
   * Build Ollama command;
   */;
  private buildOllamaCommand(model: string, requestInferenceRequest): string {;
    const args = [;
      `ollama run ${model}`;
      requestmaxTokens ? `--max-tokens ${requestmaxTokens}` : '';
      requesttemperature ? `--temperature ${requesttemperature}` : ''];
      filter(Boolean);
      join(' ');
    return `echo "${requestpromptreplace(/"/g, '\\"')}" | ${args}`;
  };

  /**;
   * Calculate tokens per second;
   */;
  private calculateTokensPerSecond(text: string, latencyMs: number): number {;
    const tokens = textsplit(/\s+/)length;
    const seconds = latencyMs / 1000;
    return tokens / seconds};

  /**;
   * Generate cache key for routing decisions;
   */;
  private generateCacheKey(requestInferenceRequest): string {;
    const key = `${requestpromptsubstring(0, 50)}_${requestmaxTokens}_${requeststreaming}`;
    return Bufferfrom(key)toString('base64');
  };

  /**;
   * Update performance statistics;
   */;
  private updatePerformanceStats(;
    engine: 'mlx' | 'ollama' | 'hybrid';
    latencyMs: number;
    success: boolean;
  ): void {;
    if (engine === 'hybrid') return; // Don't track hybrid separately;

    const realEngine = engine as 'mlx' | 'ollama';
    const stats = thisperformanceStats[realEngine];
    statstotalRequests++;
    statsaverageLatency =;
      (statsaverageLatency * (statstotalRequests - 1) + latencyMs) / statstotalRequests;
    if (!success) {;
      statssuccessRate = (statssuccessRate * (statstotalRequests - 1) + 0) / statstotalRequests};
  };

  /**;
   * Get routing statistics;
   */;
  getStats(): any {;
    return {;
      performance: thisperformanceStats;
      cacheSize: thisroutingCachesize;
      embeddedModels: Objectkeys(thisembeddedManagergetModelStatus());
      mlxAvailable: thisembeddedManagerisAvailable();
};
  };

  /**;
   * Clear routing cache;
   */;
  clearCache(): void {;
    thisroutingCacheclear();
};

  /**;
   * Preload models based on expected usage;
   */;
  async preloadModels(models: string[]): Promise<void> {;
    const embedPromises = models;
      filter((m) => mincludes('2b') || mincludes('2.7b'));
      map((m) => thisembeddedManagerembedModel(m));
    const warmPromises = models;
      filter((m) => !mincludes('2b') && !mincludes('2.7b'));
      map((m) => thislifecycleManagerpredictAndWarm({ userRequest: `load ${m}` }));
    await Promiseall([..embedPromises, ..warmPromises]);
  };
};

export default HybridInferenceRouter;