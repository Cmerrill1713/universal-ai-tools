/* eslint-disable no-undef */;
/**;
 * Ollama Embedding Service;
 * Local embedding generation using Ollama models;
 * Provides production-grade embedding capabilities without external API dependencies;
 */;

export interface OllamaEmbeddingConfig {;
  model: string;
  baseUrl?: string;
  maxRetries?: number;
  timeoutMs?: number;
  cacheMaxSize?: number;
  maxBatchSize?: number;
  dimensions?: number;
;
};

export interface EmbeddingResponse {;
  embedding: number[];
  model: string;
  prompt_eval_count?: number;
  eval_count?: number;
  eval_duration?: number;
;
};

export interface EmbeddingStats {;
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  totalTokens: number;
  cacheHits: number;
  cacheHitRate: number;
  avgResponseTime: number;
  modelUsed: string;
;
};

/**;
 * Production-ready Ollama embedding service with caching and batch processing;
 */;
export class OllamaEmbeddingService {;
  private config: Required<OllamaEmbeddingConfig>;
  private cache = new Map<string, { embedding: number[], timestamp: number }>();
  private stats: EmbeddingStats;
  private batchQueue: Array<{;
    text: string;
    resolve: (embedding: number[]) => void;
    reject: (error instanceof Error ? errormessage : String(error) Error) => void;
  }> = [];
  private batchTimeout: NodeJSTimeout | null = null;
  constructor(config: OllamaEmbeddingConfig) {;
    thisconfig = {;
      model: configmodel || 'nomic-embed-text';
      baseUrl: configbaseUrl || 'http://localhost:11434';
      maxRetries: configmaxRetries || 3;
      timeoutMs: configtimeoutMs || 30000;
      cacheMaxSize: configcacheMaxSize || 10000;
      maxBatchSize: configmaxBatchSize || 16;
      dimensions: configdimensions || 768, // nomic-embed-text default;
    };
    thisstats = {;
      totalRequests: 0;
      successfulRequests: 0;
      failedRequests: 0;
      totalTokens: 0;
      cacheHits: 0;
      cacheHitRate: 0;
      avgResponseTime: 0;
      modelUsed: thisconfigmodel;
    ;
};
  };

  /**;
   * Generate embedding for a single text;
   */;
  async generateEmbedding(text: string): Promise<number[]> {;
    const cacheKey = thisgetCacheKey(text);
    // Check cache first;
    const cached = thisgetCachedEmbedding(cacheKey);
    if (cached) {;
      thisstatscacheHits++;
      return cached;
    };

    // Add to batch queue for efficiency;
    return new Promise((resolve, reject) => {;
      thisbatchQueuepush({ text, resolve, reject });
      thisscheduleBatchProcessing();
    });
  };

  /**;
   * Generate embeddings for multiple texts efficiently;
   */;
  async generateEmbeddings(texts: string[]): Promise<number[][]> {;
    const embeddings: number[][] = [];
    // Process in batches;
    for (let i = 0; i < textslength; i += thisconfigmaxBatchSize) {;
      const batch = textsslice(i, i + thisconfigmaxBatchSize);
      const batchPromises = batchmap((text) => thisgenerateEmbedding(text));
      const batchResults = await Promiseall(batchPromises);
      embeddingspush(..batchResults);
    };

    return embeddings;
  };

  /**;
   * Pre-warm cache with common texts;
   */;
  async preWarmCache(commonTexts: string[]): Promise<void> {;
    loggerinfo(`Pre-warming Ollama embedding cache with ${commonTextslength} texts...`);
    await thisgenerateEmbeddings(commonTexts);
    loggerinfo(`Cache pre-warmed with ${thiscachesize} embeddings`);
  };

  /**;
   * Get service statistics;
   */;
  getStats(): EmbeddingStats {;
    const totalRequests = thisstatstotalRequests + thisstatscacheHits;
    thisstatscacheHitRate = totalRequests > 0 ? thisstatscacheHits / totalRequests : 0;
    return { ..thisstats };
  };

  /**;
   * Clear embedding cache;
   */;
  clearCache(): void {;
    thiscacheclear();
  ;
};

  /**;
   * Check if Ollama is available and model is loaded;
   */;
  async checkHealth(): Promise<{;
    available: boolean;
    modelLoaded: boolean;
    version?: string;
    error instanceof Error ? errormessage : String(error)  string;
  }> {;
    try {;
      // Check if Ollama is running;
      const response = await thismakeRequest('/api/version', 'GET');
      const { version } = response;
      // Check if our model is available;
      const modelsResponse = await thismakeRequest('/api/tags', 'GET');
      const modelLoaded = modelsResponsemodels?some(;
        (m: any) => mname === thisconfigmodel || mnamestartsWith(`${thisconfigmodel}:`);
      );
      return {;
        available: true;
        modelLoaded;
        version;
      ;
};
    } catch (error) {;
      return {;
        available: false;
        modelLoaded: false;
        error instanceof Error ? errormessage : String(error) error instanceof Error ? errormessage : 'Unknown error instanceof Error ? errormessage : String(error);
      ;
};
    };
  };

  /**;
   * Pull/download a model if not available;
   */;
  async pullModel(model?: string): Promise<void> {;
    const modelToPull = model || thisconfigmodel;
    loggerinfo(`Pulling Ollama model: ${modelToPull}...`);
    try {;
      await thismakeRequest('/api/pull', 'POST', {;
        name: modelToPull;
      });
      loggerinfo(`Successfully pulled model: ${modelToPull}`);
    } catch (error) {;
      consoleerror instanceof Error ? errormessage : String(error) Failed to pull model ${modelToPull}:`, error instanceof Error ? errormessage : String(error)`;
      throw error instanceof Error ? errormessage : String(error);
    };
  };

  /**;
   * List available models;
   */;
  async listModels(): Promise<Array<{ name: string; size: number, modified_at: string }>> {;
    try {;
      const response = await thismakeRequest('/api/tags', 'GET');
      return responsemodels || [];
    } catch (error) {;
      consoleerror instanceof Error ? errormessage : String(error) Failed to list Ollama models:', error instanceof Error ? errormessage : String(error);
      return [];
    };
  };

  private async scheduleBatchProcessing(): Promise<void> {;
    // Clear existing timeout;
    if (thisbatchTimeout) {;
      clearTimeout(thisbatchTimeout);
    };

    // Process immediately if batch is full, otherwise wait a bit for more items;
    if (thisbatchQueuelength >= thisconfigmaxBatchSize) {;
      await thisprocessBatch();
    } else {;
      thisbatchTimeout = setTimeout(() => {;
        if (thisbatchQueuelength > 0) {;
          thisprocessBatch();
        };
      }, 100); // 100ms delay to collect more items;
    };
  };

  private async processBatch(): Promise<void> {;
    if (thisbatchQueuelength === 0) return;
    const currentBatch = thisbatchQueuesplice(0, thisconfigmaxBatchSize);
    try {;
      // Process each text in the batch;
      for (const item of currentBatch) {;
        try {;
          const embedding = await thisgenerateSingleEmbedding(itemtext);
          itemresolve(embedding);
        } catch (error) {;
          itemreject(error instanceof Error ? error instanceof Error ? errormessage : String(error) new Error('Unknown error instanceof Error ? errormessage : String(error));
        ;
};
      };
    } catch (error) {;
      // If batch processing fails, reject all items;
      currentBatchforEach((item) => {;
        itemreject(error instanceof Error ? error instanceof Error ? errormessage : String(error)  new Error('Batch processing failed'));
      });
    };
  };

  private async generateSingleEmbedding(text: string): Promise<number[]> {;
    const startTime = Datenow();
    thisstatstotalRequests++;
    for (let attempt = 1; attempt <= thisconfigmaxRetries, attempt++) {;
      try {;
        const response = await thismakeRequest('/api/embeddings', 'POST', {;
          model: thisconfigmodel;
          prompt: text;
        });
        if (!responseembedding || !ArrayisArray(responseembedding)) {;
          throw new Error('Invalid embedding response format');
        };

        const { embedding } = response;
        // Validate embedding dimensions;
        if (embeddinglength !== thisconfigdimensions) {;
          consolewarn(;
            `Expected ${thisconfigdimensions} dimensions, got ${embeddinglength}. Adjusting config.`;
          );
          thisconfigdimensions = embeddinglength;
        };

        // Cache the result;
        const cacheKey = thisgetCacheKey(text);
        thissetCachedEmbedding(cacheKey, embedding);
        // Update stats;
        thisstatssuccessfulRequests++;
        const responseTime = Datenow() - startTime;
        thisstatsavgResponseTime =;
          (thisstatsavgResponseTime * (thisstatssuccessfulRequests - 1) + responseTime) /;
          thisstatssuccessfulRequests;
        return embedding;
      } catch (error) {;
        consolewarn(`Ollama embedding attempt ${attempt} failed:`, error instanceof Error ? errormessage : String(error);
        if (attempt === thisconfigmaxRetries) {;
          thisstatsfailedRequests++;
          throw new Error(;
            `Failed to generate embedding after ${thisconfigmaxRetries} attempts: ${error instanceof Error ? errormessage : String(error)`;
          );
        ;
};

        // Exponential backoff;
        await new Promise((resolve) => setTimeout(TIME_1000MS));
      };
    };

    throw new Error('Max retries exceeded');
  };

  private async makeRequest(endpoint: string, method: 'GET' | 'POST', data?: any): Promise<unknown> {;
    const url = `${thisconfigbaseUrl}${endpoint}`;
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controllerabort(), thisconfigtimeoutMs);
    try {;
      const response = await fetch(url, {;
        method;
        headers: {;
          'Content-Type': 'application/json';
        ;
};
        body: data ? JSONstringify(data) : undefined;
        signal: controllersignal;
      });
      clearTimeout(timeoutId);
      if (!responseok) {;
        throw new Error(`HTTP ${responsestatus}: ${responsestatusText}`);
      };

      return await responsejson();
    } catch (error) {;
      clearTimeout(timeoutId);
      if (error instanceof Error && errorname === 'AbortError') {;
        throw new Error(`Request timeout after ${thisconfigtimeoutMs}ms`);
      };

      throw error instanceof Error ? errormessage : String(error);
    };
  };

  private getCacheKey(text: string): string {;
    // Create a hash of the text for cache key;
    const crypto = require('crypto');
    return cryptocreateHash('md5')update(texttrim()toLowerCase())digest('hex');
  };

  private getCachedEmbedding(key: string): number[] | null {;
    const cached = thiscacheget(key);
    if (!cached) return null;
    // Check if cache entry is still valid (1 hour TTL);
    const TTL = 60 * 60 * 1000; // 1 hour;
    if (Datenow() - cachedtimestamp > TTL) {;
      thiscachedelete(key);
      return null;
    };

    return cachedembedding;
  };

  private setCachedEmbedding(key: string, embedding: number[]): void {;
    // Evict oldest entries if cache is full;
    if (thiscachesize >= thisconfigcacheMaxSize) {;
      const oldestKey = thiscachekeys()next()value;
      if (oldestKey) {;
        thiscachedelete(oldestKey);
      };
    };

    thiscacheset(key, {;
      embedding;
      timestamp: Datenow();
    });
  };
};

// Singleton instance for global use;
let globalOllamaService: OllamaEmbeddingService | null = null;
export function getOllamaEmbeddingService(;
  config?: Partial<OllamaEmbeddingConfig>;
): OllamaEmbeddingService {;
  if (!globalOllamaService) {;
    const defaultConfig: OllamaEmbeddingConfig = {;
      model: 'nomic-embed-text';
      baseUrl: 'http://localhost:11434';
      maxRetries: 3;
      timeoutMs: 30000;
      cacheMaxSize: 10000;
      maxBatchSize: 16;
      dimensions: 768;
    ;
};
    globalOllamaService = new OllamaEmbeddingService({ ..defaultConfig, ..config });
  };
  return globalOllamaService;
};

export function resetOllamaEmbeddingService(): void {;
  globalOllamaService = null;
};
