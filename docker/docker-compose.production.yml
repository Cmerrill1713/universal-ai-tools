version: '3.9'

services:
  # Main application with optimized Rust services
  app:
    build:
      context: ..
      dockerfile: docker/rust-services.dockerfile
      target: production
    image: universal-ai-tools:rust-optimized
    container_name: universal-ai-tools
    restart: unless-stopped
    ports:
      - "9999:9999"
      - "8080:8080"  # WebSocket port
    environment:
      NODE_ENV: production
      PORT: 9999
      DATABASE_URL: ${DATABASE_URL}
      REDIS_URL: redis://redis:6379
      SUPABASE_URL: ${SUPABASE_URL}
      SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SUPABASE_SERVICE_KEY}
      # Optimized Rust service configuration
      ENABLE_NATIVE_OPTIMIZATION: "true"
      RUST_SERVICES_PATH: /app/rust-services/libs
      RUST_LOG: info
      RUST_BACKTRACE: 1
      # Advanced AI configuration
      ENABLE_MLX_FINE_TUNING: "true"
      ENABLE_INTELLIGENT_PARAMETERS: "true"
      ENABLE_AB_MCTS_ORCHESTRATION: "true"
      ENABLE_MULTIMODAL_FUSION: "true"
      MLX_MODELS_PATH: /app/models
      # LLM configuration
      OLLAMA_URL: http://ollama:11434
      LM_STUDIO_URL: ${LM_STUDIO_URL:-http://host.docker.internal:1234}
      # High-performance tuning
      UV_THREADPOOL_SIZE: 256
      NODE_OPTIONS: "--max-old-space-size=8192 --optimize-for-size"
    volumes:
      - app-data:/app/data
      - app-logs:/app/logs
      - app-cache:/app/cache
      - mlx-models:/app/models
    depends_on:
      - redis
      - ollama
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9999/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Redis for caching and pub/sub
  redis:
    image: redis:7-alpine
    container_name: ai-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: >
      redis-server
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Ollama for local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: ai-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_MODELS: /root/.ollama/models
      OLLAMA_NUM_PARALLEL: 4
      OLLAMA_MAX_LOADED_MODELS: 2
      OLLAMA_KEEP_ALIVE: 30m
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
        reservations:
          cpus: '4'
          memory: 8G
    # GPU support (uncomment if using NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Nginx reverse proxy and load balancer
  nginx:
    image: nginx:alpine
    container_name: ai-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      - nginx-cache:/var/cache/nginx
    depends_on:
      - app
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    container_name: ai-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: ai-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: redis-datasource
    depends_on:
      - prometheus
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Vector for log aggregation
  vector:
    image: timberio/vector:latest-alpine
    container_name: ai-vector
    restart: unless-stopped
    ports:
      - "8686:8686"
    volumes:
      - ./vector.toml:/etc/vector/vector.toml:ro
      - app-logs:/logs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M

networks:
  ai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  app-data:
    driver: local
  app-logs:
    driver: local
  app-cache:
    driver: local
  mlx-models:
    driver: local
  redis-data:
    driver: local
  ollama-data:
    driver: local
  nginx-cache:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local