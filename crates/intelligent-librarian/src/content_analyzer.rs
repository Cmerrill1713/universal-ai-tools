//! Content analysis for the Intelligent Librarian

use crate::models::*;
use anyhow::Result;
use regex::Regex;
use std::collections::HashMap;

/// Content analyzer for multi-modal analysis
pub struct ContentAnalyzer {
    entity_patterns: HashMap<EntityType, Vec<Regex>>,
}

impl ContentAnalyzer {
    pub async fn new() -> Result<Self> {
        let mut analyzer = Self {
            entity_patterns: HashMap::new(),
        };

        analyzer.initialize_patterns();
        Ok(analyzer)
    }

    fn initialize_patterns(&mut self) {
        // Initialize entity patterns
        self.entity_patterns.insert(EntityType::Person, vec![
            Regex::new(r"\b[A-Z][a-z]+ [A-Z][a-z]+\b").unwrap(),
        ]);

        self.entity_patterns.insert(EntityType::Organization, vec![
            Regex::new(r"\b[A-Z][a-z]+ (Inc|Corp|LLC|Ltd|Company)\b").unwrap(),
        ]);

        self.entity_patterns.insert(EntityType::Technology, vec![
            Regex::new(r"\b(Rust|Python|JavaScript|TypeScript|Go|Java|C\+\+|C#)\b").unwrap(),
        ]);

        self.entity_patterns.insert(EntityType::Url, vec![
            Regex::new(r"https?://[^\s]+").unwrap(),
        ]);

        self.entity_patterns.insert(EntityType::Email, vec![
            Regex::new(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b").unwrap(),
        ]);
    }

    /// Analyze content for entities, topics, and metadata
    pub async fn analyze(&self, document: &Document) -> Result<ContentAnalysis> {
        let content = &document.content;

        // Extract entities
        let entities = self.extract_entities(content).await?;

        // Extract topics (simplified)
        let topics = self.extract_topics(content).await?;

        // Extract keywords
        let keywords = self.extract_keywords(content).await?;

        // Calculate complexity score
        let complexity_score = self.calculate_complexity(content).await?;

        // Calculate readability score
        let readability_score = self.calculate_readability(content).await?;

        // Detect technical level
        let technical_level = self.detect_technical_level(content).await?;

        // Detect language
        let language_detected = self.detect_language(content).await?;

        // Extract code languages
        let code_languages = self.extract_code_languages(content).await?;

        // Extract dependencies
        let dependencies = self.extract_dependencies(content).await?;

        // Extract references
        let references = self.extract_references(content).await?;

        // Analyze sentiment
        let sentiment = self.analyze_sentiment(content).await?;

        // Generate summary
        let summary = self.generate_summary(content).await?;

        Ok(ContentAnalysis {
            content_type: ContentType::Text, // Default for now
            category: DocumentCategory::Technical, // Default for now
            topics,
            entities,
            keywords,
            complexity_score,
            readability_score,
            technical_level,
            language_detected,
            code_languages,
            dependencies,
            references,
            sentiment,
            summary,
            embeddings: None, // Would be generated by embedding service
        })
    }

    async fn extract_entities(&self, content: &str) -> Result<Vec<Entity>> {
        let mut entities = Vec::new();

        // Extract person names
        if let Some(patterns) = self.entity_patterns.get(&EntityType::Person) {
            for pattern in patterns {
                for mat in pattern.find_iter(content) {
                    entities.push(Entity {
                        text: mat.as_str().to_string(),
                        entity_type: EntityType::Person,
                        confidence: 0.8,
                        start_pos: mat.start(),
                        end_pos: mat.end(),
                    });
                }
            }
        }

        // Extract organizations
        if let Some(patterns) = self.entity_patterns.get(&EntityType::Organization) {
            for pattern in patterns {
                for mat in pattern.find_iter(content) {
                    entities.push(Entity {
                        text: mat.as_str().to_string(),
                        entity_type: EntityType::Organization,
                        confidence: 0.7,
                        start_pos: mat.start(),
                        end_pos: mat.end(),
                    });
                }
            }
        }

        // Extract technologies
        if let Some(patterns) = self.entity_patterns.get(&EntityType::Technology) {
            for pattern in patterns {
                for mat in pattern.find_iter(content) {
                    entities.push(Entity {
                        text: mat.as_str().to_string(),
                        entity_type: EntityType::Technology,
                        confidence: 0.9,
                        start_pos: mat.start(),
                        end_pos: mat.end(),
                    });
                }
            }
        }

        // Extract URLs
        if let Some(patterns) = self.entity_patterns.get(&EntityType::Url) {
            for pattern in patterns {
                for mat in pattern.find_iter(content) {
                    entities.push(Entity {
                        text: mat.as_str().to_string(),
                        entity_type: EntityType::Url,
                        confidence: 0.95,
                        start_pos: mat.start(),
                        end_pos: mat.end(),
                    });
                }
            }
        }

        // Extract emails
        if let Some(patterns) = self.entity_patterns.get(&EntityType::Email) {
            for pattern in patterns {
                for mat in pattern.find_iter(content) {
                    entities.push(Entity {
                        text: mat.as_str().to_string(),
                        entity_type: EntityType::Email,
                        confidence: 0.95,
                        start_pos: mat.start(),
                        end_pos: mat.end(),
                    });
                }
            }
        }

        Ok(entities)
    }

    async fn extract_topics(&self, content: &str) -> Result<Vec<String>> {
        // Simple topic extraction based on common technical terms
        let technical_terms = vec![
            "machine learning", "artificial intelligence", "optimization",
            "distributed systems", "microservices", "database",
            "web development", "mobile development", "cloud computing",
            "security", "performance", "scalability", "reliability",
        ];

        let mut topics = Vec::new();
        let content_lower = content.to_lowercase();

        for term in technical_terms {
            if content_lower.contains(term) {
                topics.push(term.to_string());
            }
        }

        Ok(topics)
    }

    async fn extract_keywords(&self, content: &str) -> Result<Vec<String>> {
        // Simple keyword extraction
        let words: Vec<&str> = content.split_whitespace()
            .filter(|word| word.len() > 3)
            .collect();

        let mut word_count: HashMap<String, usize> = HashMap::new();
        for word in words {
            let word = word.to_lowercase();
            *word_count.entry(word).or_insert(0) += 1;
        }

        let mut keywords: Vec<(String, usize)> = word_count.into_iter().collect();
        keywords.sort_by(|a, b| b.1.cmp(&a.1));

        Ok(keywords.into_iter().take(10).map(|(word, _)| word).collect())
    }

    async fn calculate_complexity(&self, content: &str) -> Result<f64> {
        // Simple complexity calculation based on sentence length and technical terms
        let sentences: Vec<&str> = content.split('.').collect();
        let avg_sentence_length = sentences.iter().map(|s| s.len()).sum::<usize>() as f64 / sentences.len() as f64;

        let technical_terms = vec!["algorithm", "complexity", "optimization", "architecture", "implementation"];
        let technical_count = technical_terms.iter()
            .map(|term| content.to_lowercase().matches(term).count())
            .sum::<usize>() as f64;

        let complexity = (avg_sentence_length / 100.0) + (technical_count / 10.0);
        Ok(complexity.min(1.0))
    }

    async fn calculate_readability(&self, content: &str) -> Result<f64> {
        // Simple readability score
        let words: Vec<&str> = content.split_whitespace().collect();
        let sentences: Vec<&str> = content.split('.').collect();

        if sentences.is_empty() || words.is_empty() {
            return Ok(0.5);
        }

        let avg_words_per_sentence = words.len() as f64 / sentences.len() as f64;
        let readability = 1.0 - (avg_words_per_sentence / 50.0).min(1.0);
        Ok(readability.max(0.0))
    }

    async fn detect_technical_level(&self, content: &str) -> Result<TechnicalLevel> {
        let content_lower = content.to_lowercase();

        let expert_terms = vec!["algorithm", "complexity", "optimization", "architecture"];
        let advanced_terms = vec!["implementation", "design", "pattern", "framework"];
        let intermediate_terms = vec!["function", "method", "class", "module"];

        let expert_count = expert_terms.iter().map(|term| content_lower.matches(term).count()).sum::<usize>();
        let advanced_count = advanced_terms.iter().map(|term| content_lower.matches(term).count()).sum::<usize>();
        let intermediate_count = intermediate_terms.iter().map(|term| content_lower.matches(term).count()).sum::<usize>();

        if expert_count > advanced_count && expert_count > intermediate_count {
            Ok(TechnicalLevel::Expert)
        } else if advanced_count > intermediate_count {
            Ok(TechnicalLevel::Advanced)
        } else if intermediate_count > 0 {
            Ok(TechnicalLevel::Intermediate)
        } else {
            Ok(TechnicalLevel::Beginner)
        }
    }

    async fn detect_language(&self, content: &str) -> Result<Option<String>> {
        // Simple language detection based on common words
        let english_words = vec!["the", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"];
        let english_count = english_words.iter().map(|word| content.to_lowercase().matches(word).count()).sum::<usize>();

        if english_count > 5 {
            Ok(Some("en".to_string()))
        } else {
            Ok(None)
        }
    }

    async fn extract_code_languages(&self, content: &str) -> Result<Vec<String>> {
        let languages = vec!["rust", "python", "javascript", "typescript", "go", "java", "c++", "c#"];
        let mut found_languages = Vec::new();

        for lang in languages {
            if content.to_lowercase().contains(lang) {
                found_languages.push(lang.to_string());
            }
        }

        Ok(found_languages)
    }

    async fn extract_dependencies(&self, content: &str) -> Result<Vec<String>> {
        // Simple dependency extraction
        let mut dependencies = Vec::new();

        // Look for common dependency patterns
        if content.contains("Cargo.toml") {
            dependencies.push("cargo".to_string());
        }
        if content.contains("package.json") {
            dependencies.push("npm".to_string());
        }
        if content.contains("requirements.txt") {
            dependencies.push("pip".to_string());
        }

        Ok(dependencies)
    }

    async fn extract_references(&self, content: &str) -> Result<Vec<String>> {
        // Extract URLs as references
        let url_pattern = Regex::new(r"https?://[^\s]+")?;
        let mut references = Vec::new();

        for mat in url_pattern.find_iter(content) {
            references.push(mat.as_str().to_string());
        }

        Ok(references)
    }

    async fn analyze_sentiment(&self, content: &str) -> Result<Option<Sentiment>> {
        // Simple sentiment analysis
        let positive_words = vec!["good", "great", "excellent", "amazing", "wonderful", "fantastic"];
        let negative_words = vec!["bad", "terrible", "awful", "horrible", "disappointing", "frustrating"];

        let content_lower = content.to_lowercase();
        let positive_count = positive_words.iter().map(|word| content_lower.matches(word).count()).sum::<usize>() as f64;
        let negative_count = negative_words.iter().map(|word| content_lower.matches(word).count()).sum::<usize>() as f64;

        if positive_count > 0.0 || negative_count > 0.0 {
            let polarity = (positive_count - negative_count) / (positive_count + negative_count + 1.0);
            let confidence = (positive_count + negative_count) / (positive_count + negative_count + 1.0);

            Ok(Some(Sentiment {
                polarity,
                confidence,
                emotions: HashMap::new(),
            }))
        } else {
            Ok(None)
        }
    }

    async fn generate_summary(&self, content: &str) -> Result<Option<String>> {
        // Simple summary generation (first 200 characters)
        if content.len() > 200 {
            Ok(Some(format!("{}...", &content[..200])))
        } else {
            Ok(Some(content.to_string()))
        }
    }
}
