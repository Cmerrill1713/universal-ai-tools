{
  "totalFiles": 10,
  "totalFixes": 3181,
  "fixesByPattern": {
    "missing_comma_object_literal": 220,
    "unterminated_string_single": 730,
    "semicolon_missing_statements": 230,
    "import_spacing": 34,
    "object_property_spacing": 1560,
    "missing_closing_parenthesis": 187,
    "type_annotation_spacing": 81,
    "arrow_function_spacing": 68,
    "unterminated_string_double": 50,
    "missing_parentheses_logger": 6,
    "template_literal_backslash": 8,
    "content_type_header": 7
  },
  "fixesByFile": {
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/environment.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/environment.ts",
        "pattern": "missing_comma_object_literal",
        "count": 7,
        "originalContent": "import dotenv from 'dotenv';\nimport { ServiceConfig } from '@/types';\nimport { ports } from './ports';\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development',\n  \n  database: {\n    url: process.env.DATABASE_URL || '',\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)\n  },\n  \n  redis: process.env.REDIS_URL ? {\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)\n  } : undefined,\n  \n  supabase: {\n    url: process.env.SUPABASE_URL || '',\n    anonKey: process.env.SUPABASE_ANON_KEY || '',\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || ''\n  },\n  \n  jwt: {\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production',\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h'\n  },\n  \n  llm: {\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434'\n  },\n  \n  vision: {\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true',\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf',\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto',\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false'\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [\n    'SUPABASE_URL',\n    'SUPABASE_ANON_KEY', \n    'SUPABASE_SERVICE_KEY'\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {\n    throw new Error('JWT_SECRET must be set in production');\n  }\n}\n\nexport default config;",
        "fixedContent": "import dotenv from 'dotenv';\nimport { ServiceConfig } from '@/types';\nimport { ports } from './ports';\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development',\n  \n  database: {,\n    url: process.env.DATABASE_URL || '',\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '',\n    anonKey: process.env.SUPABASE_ANON_KEY || '',\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || ''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production',\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h'\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434'\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true',\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf',\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto',\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false'\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [\n    'SUPABASE_URL',\n    'SUPABASE_ANON_KEY', \n    'SUPABASE_SERVICE_KEY'\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {\n    throw new Error('JWT_SECRET must be set in production');\n  }\n}\n\nexport default config;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/environment.ts",
        "pattern": "unterminated_string_single",
        "count": 24,
        "originalContent": "import dotenv from 'dotenv';\nimport { ServiceConfig } from '@/types';\nimport { ports } from './ports';\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development',\n  \n  database: {,\n    url: process.env.DATABASE_URL || '',\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '',\n    anonKey: process.env.SUPABASE_ANON_KEY || '',\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || ''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production',\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h'\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434'\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true',\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf',\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto',\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false'\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [\n    'SUPABASE_URL',\n    'SUPABASE_ANON_KEY', \n    'SUPABASE_SERVICE_KEY'\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {\n    throw new Error('JWT_SECRET must be set in production');\n  }\n}\n\nexport default config;",
        "fixedContent": "import dotenv from 'dotenv';'\nimport { ServiceConfig } from '@/types';'\nimport { ports } from './ports';'\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development','\n  \n  database: {,\n    url: process.env.DATABASE_URL || '','\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)'\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)'\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '','\n    anonKey: process.env.SUPABASE_ANON_KEY || '','\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || '''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production','\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h''\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434''\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true','\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto','\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),'\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false''\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [\n    'SUPABASE_URL','\n    'SUPABASE_ANON_KEY', '\n    'SUPABASE_SERVICE_KEY''\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);'\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {'\n    throw new Error('JWT_SECRET must be set in production');'\n  }\n}\n\nexport default config;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/environment.ts",
        "pattern": "semicolon_missing_statements",
        "count": 7,
        "originalContent": "import dotenv from 'dotenv';'\nimport { ServiceConfig } from '@/types';'\nimport { ports } from './ports';'\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development','\n  \n  database: {,\n    url: process.env.DATABASE_URL || '','\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)'\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)'\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '','\n    anonKey: process.env.SUPABASE_ANON_KEY || '','\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || '''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production','\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h''\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434''\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true','\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto','\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),'\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false''\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [\n    'SUPABASE_URL','\n    'SUPABASE_ANON_KEY', '\n    'SUPABASE_SERVICE_KEY''\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);'\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {'\n    throw new Error('JWT_SECRET must be set in production');'\n  }\n}\n\nexport default config;",
        "fixedContent": "import dotenv from 'dotenv';';\nimport { ServiceConfig } from '@/types';';\nimport { ports } from './ports';';\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,;\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development','\n  \n  database: {,\n    url: process.env.DATABASE_URL || '','\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)'\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)'\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '','\n    anonKey: process.env.SUPABASE_ANON_KEY || '','\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || '''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production','\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h''\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434''\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true','\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto','\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),'\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false''\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [;\n    'SUPABASE_URL','\n    'SUPABASE_ANON_KEY', '\n    'SUPABASE_SERVICE_KEY''\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);';\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {'\n    throw new Error('JWT_SECRET must be set in production');';\n  }\n}\n\nexport default config;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/environment.ts",
        "pattern": "import_spacing",
        "count": 2,
        "originalContent": "import dotenv from 'dotenv';';\nimport { ServiceConfig } from '@/types';';\nimport { ports } from './ports';';\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,;\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development','\n  \n  database: {,\n    url: process.env.DATABASE_URL || '','\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)'\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)'\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '','\n    anonKey: process.env.SUPABASE_ANON_KEY || '','\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || '''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production','\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h''\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434''\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true','\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto','\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),'\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false''\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [;\n    'SUPABASE_URL','\n    'SUPABASE_ANON_KEY', '\n    'SUPABASE_SERVICE_KEY''\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);';\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {'\n    throw new Error('JWT_SECRET must be set in production');';\n  }\n}\n\nexport default config;",
        "fixedContent": "import dotenv from 'dotenv';';\nimport { ServiceConfig  } from '@/types';';\nimport { ports  } from './ports';';\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,;\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development','\n  \n  database: {,\n    url: process.env.DATABASE_URL || '','\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)'\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)'\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '','\n    anonKey: process.env.SUPABASE_ANON_KEY || '','\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || '''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production','\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h''\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434''\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true','\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto','\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),'\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false''\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [;\n    'SUPABASE_URL','\n    'SUPABASE_ANON_KEY', '\n    'SUPABASE_SERVICE_KEY''\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);';\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {'\n    throw new Error('JWT_SECRET must be set in production');';\n  }\n}\n\nexport default config;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/environment.ts",
        "pattern": "object_property_spacing",
        "count": 27,
        "originalContent": "import dotenv from 'dotenv';';\nimport { ServiceConfig  } from '@/types';';\nimport { ports  } from './ports';';\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,;\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development','\n  \n  database: {,\n    url: process.env.DATABASE_URL || '','\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)'\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)'\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '','\n    anonKey: process.env.SUPABASE_ANON_KEY || '','\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || '''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production','\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h''\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434''\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true','\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto','\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),'\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false''\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [;\n    'SUPABASE_URL','\n    'SUPABASE_ANON_KEY', '\n    'SUPABASE_SERVICE_KEY''\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);';\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {'\n    throw new Error('JWT_SECRET must be set in production');';\n  }\n}\n\nexport default config;",
        "fixedContent": "import dotenv from 'dotenv';';\nimport { ServiceConfig  } from '@/types';';\nimport { ports  } from './ports';';\n\n// Load environment variables\ndotenv.config();\n\nexport const config: ServiceConfig = {,;\n  port: ports.mainServer,\n  environment: process.env.NODE_ENV || 'development','\n  \n  database: {,\n    url: process.env.DATABASE_URL || '','\n    poolSize: parseInt(process.env.DB_POOL_SIZE || '10', 10)'\n  },\n  \n  redis: process.env.REDIS_URL ? {,\n    url: process.env.REDIS_URL,\n    retryAttempts: parseInt(process.env.REDIS_RETRY_ATTEMPTS || '3', 10)'\n  } : undefined,\n  \n  supabase: {,\n    url: process.env.SUPABASE_URL || '','\n    anonKey: process.env.SUPABASE_ANON_KEY || '','\n    serviceKey: process.env.SUPABASE_SERVICE_KEY || '''\n  },\n  \n  jwt: {,\n    secret: process.env.JWT_SECRET || 'fallback-secret-change-in-production','\n    expiresIn: process.env.JWT_EXPIRES_IN || '24h''\n  },\n  \n  llm: {,\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    anthropicApiKey: process.env.ANTHROPIC_API_KEY,\n    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434''\n  },\n  \n  vision: {,\n    enableSdxlRefiner: process.env.ENABLE_SDXL_REFINER === 'true','\n    sdxlRefinerPath: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    preferredBackend: (process.env.VISION_BACKEND as 'mlx' | 'gguf' | 'auto') || 'auto','\n    maxVram: parseInt(process.env.VISION_MAX_VRAM || '20', 10),'\n    enableCaching: process.env.VISION_ENABLE_CACHING !== 'false''\n  }\n};\n\n// Validation\nexport function validateConfig(): void {\n  const required = [;\n    'SUPABASE_URL','\n    'SUPABASE_ANON_KEY', '\n    'SUPABASE_SERVICE_KEY''\n  ];\n  \n  const missing = required.filter(key => !process.env[key]);\n  \n  if (missing.length > 0) {\n    throw new Error(`Missing required environment variables: ${missing.join(', ')}`);';\n  }\n  \n  if (config.environment === 'production' && config.jwt.secret.includes('fallback')) {'\n    throw new Error('JWT_SECRET must be set in production');';\n  }\n}\n\nexport default config;"
      }
    ],
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/models.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/models.ts",
        "pattern": "missing_comma_object_literal",
        "count": 8,
        "originalContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b',\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest',\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest'\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b',\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b',\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b',\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b',\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b',\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b'\n  },\n  \n  // Embedding models\n  embedding: {\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest',\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest'\n  },\n  \n  // Image generation models\n  imageGeneration: {\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf',\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf',\n    flux: process.env.FLUX_MODEL || 'flux-schnell'\n  },\n  \n  // Specialized models\n  specialized: {\n    math: process.env.MATH_MODEL || 'mathstral:7b',\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b',\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b'\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false',\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct',\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1',\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct',\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2'\n    }\n  },\n  \n  // Model routing preferences\n  routing: {\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true',\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false',\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {\n    enabled: process.env.ENABLE_LFM2 !== 'false',\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf',\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false',\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false'\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {\n  speed?: 'fast' | 'balanced' | 'quality';\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':\n      return ModelConfig.text.code;\n    case 'reasoning':\n      return ModelConfig.text.reasoning;\n    case 'quick_response':\n      return ModelConfig.text.small;\n    case 'embedding':\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':\n      return ModelConfig.embedding.vision;\n    case 'math':\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;\n      if (speed === 'quality') return ModelConfig.text.large;\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}",
        "fixedContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b',\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest',\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest'\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b',\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b',\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b',\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b',\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b',\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b'\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest',\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest'\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf',\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf',\n    flux: process.env.FLUX_MODEL || 'flux-schnell'\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b',\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b',\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b'\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false',\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct',\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1',\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct',\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2'\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true',\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false',\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false',\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf',\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false',\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false'\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {\n  speed?: 'fast' | 'balanced' | 'quality';\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':\n      return ModelConfig.text.code;\n    case 'reasoning':\n      return ModelConfig.text.reasoning;\n    case 'quick_response':\n      return ModelConfig.text.small;\n    case 'embedding':\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':\n      return ModelConfig.embedding.vision;\n    case 'math':\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;\n      if (speed === 'quality') return ModelConfig.text.large;\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/models.ts",
        "pattern": "unterminated_string_single",
        "count": 45,
        "originalContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b',\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest',\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest'\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b',\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b',\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b',\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b',\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b',\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b'\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest',\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest'\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf',\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf',\n    flux: process.env.FLUX_MODEL || 'flux-schnell'\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b',\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b',\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b'\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false',\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct',\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1',\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct',\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2'\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true',\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false',\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false',\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf',\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false',\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false'\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {\n  speed?: 'fast' | 'balanced' | 'quality';\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':\n      return ModelConfig.text.code;\n    case 'reasoning':\n      return ModelConfig.text.reasoning;\n    case 'quick_response':\n      return ModelConfig.text.small;\n    case 'embedding':\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':\n      return ModelConfig.embedding.vision;\n    case 'math':\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;\n      if (speed === 'quality') return ModelConfig.text.large;\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}",
        "fixedContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';'\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};'\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b''\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');'\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/models.ts",
        "pattern": "missing_closing_parenthesis",
        "count": 1,
        "originalContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';'\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};'\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b''\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');'\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}",
        "fixedContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';'\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {)\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};'\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b''\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');'\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/models.ts",
        "pattern": "semicolon_missing_statements",
        "count": 5,
        "originalContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';'\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {)\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};'\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b''\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');'\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}",
        "fixedContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {);\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};';\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'';\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');';\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/models.ts",
        "pattern": "type_annotation_spacing",
        "count": 1,
        "originalContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {);\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};';\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'';\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');';\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}",
        "fixedContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {);\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};';\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'';\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');';\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/models.ts",
        "pattern": "import_spacing",
        "count": 1,
        "originalContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls } from './ports';';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {);\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};';\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'';\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');';\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}",
        "fixedContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls  } from './ports';';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {);\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};';\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'';\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');';\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/models.ts",
        "pattern": "object_property_spacing",
        "count": 47,
        "originalContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls  } from './ports';';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {);\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};';\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2:1.2b'';\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default:\n      // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http://localhost:11434/api/tags');';\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}",
        "fixedContent": "/**\n * Model Configuration\n * Centralizes all model references to allow easy switching\n */\n\nimport { ports, getServiceUrls  } from './ports';';\n\n// Get service URLs with proper ports\nconst serviceUrls = getServiceUrls(ports);\n\nexport const ModelConfig = {\n  // Vision models\n  vision: {,\n    multimodal: process.env.VISION_MULTIMODAL_MODEL || 'llava:13b','\n    analysis: process.env.VISION_ANALYSIS_MODEL || 'llama3.2-vision:latest','\n    embedding: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Text generation models\n  text: {\n    // LFM2 as default for ultra-fast routing and simple tasks\n    routing: process.env.ROUTING_MODEL || 'lfm2:1.2b','\n    small: process.env.TEXT_SMALL_MODEL || 'lfm2:1.2b','\n    medium: process.env.TEXT_MEDIUM_MODEL || 'llama3.2:8b','\n    large: process.env.TEXT_LARGE_MODEL || 'llama3.2:70b','\n    code: process.env.CODE_MODEL || 'qwen2.5-coder:7b','\n    reasoning: process.env.REASONING_MODEL || 'deepseek-r1:8b''\n  },\n  \n  // Embedding models\n  embedding: {,\n    text: process.env.TEXT_EMBEDDING_MODEL || 'nomic-embed-text:latest','\n    vision: process.env.VISION_EMBEDDING_MODEL || 'nomic-embed-vision:latest''\n  },\n  \n  // Image generation models\n  imageGeneration: {,\n    sdxl: process.env.SDXL_MODEL_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-base-1.0-Q4_1.gguf','\n    sdxlRefiner: process.env.SDXL_REFINER_PATH || '/Users/christianmerrill/Downloads/stable-diffusion-xl-refiner-1.0-Q4_1.gguf','\n    flux: process.env.FLUX_MODEL || 'flux-schnell''\n  },\n  \n  // Specialized models\n  specialized: {,\n    math: process.env.MATH_MODEL || 'mathstral:7b','\n    medical: process.env.MEDICAL_MODEL || 'meditron:7b','\n    finance: process.env.FINANCE_MODEL || 'nous-hermes:13b''\n  },\n  \n  // LM Studio configuration (replaces HuggingFace)\n  lmStudio: {,\n    enabled: process.env.ENABLE_LM_STUDIO !== 'false','\n    url: process.env.LM_STUDIO_URL || serviceUrls.lmStudio,\n    // Models available in LM Studio for HuggingFace-like tasks\n    models: {,\n      textGeneration: process.env.LM_STUDIO_TEXT_MODEL || 'meta-llama-3-8b-instruct','\n      embedding: process.env.LM_STUDIO_EMBEDDING_MODEL || 'nomic-embed-text-v1','\n      summarization: process.env.LM_STUDIO_SUMMARY_MODEL || 'mistral-7b-instruct','\n      sentiment: process.env.LM_STUDIO_SENTIMENT_MODEL || 'phi-2''\n    }\n  },\n  \n  // Model routing preferences\n  routing: {,\n    preferLocal: process.env.PREFER_LOCAL_MODELS === 'true','\n    fallbackEnabled: process.env.MODEL_FALLBACK_ENABLED !== 'false','\n    maxRetries: parseInt(process.env.MODEL_MAX_RETRIES || '3'),'\n    // LFM2 server endpoint\n    lfm2Endpoint: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server\n  },\n  \n  // LFM2 specific configuration\n  lfm2: {,\n    enabled: process.env.ENABLE_LFM2 !== 'false','\n    serverUrl: process.env.LFM2_SERVER_URL || serviceUrls.lfm2Server,\n    modelPath: process.env.LFM2_MODEL_PATH || '/Users/christianmerrill/Downloads/hf/LFM-1.2B-GGUF-Q5_1/lfm-1.2b_q5_1.gguf','\n    // Use LFM2 for ultra-fast routing decisions\n    useForRouting: process.env.LFM2_FOR_ROUTING !== 'false','\n    // Use LFM2 for simple queries\n    useForSimpleQueries: process.env.LFM2_FOR_SIMPLE !== 'false''\n  }\n};\n\n/**\n * Get the appropriate model based on task requirements\n */\nexport function getModelForTask(taskType: string, requirements?: {);\n  speed?: 'fast' | 'balanced' | 'quality';'\n  capabilities?: string[];\n  maxTokens?: number;\n}): string {\n  const { speed = 'balanced' } = requirements || {};';\n  \n  // Use LFM2 for routing and simple tasks when enabled\n  if (ModelConfig.lfm2.enabled) {\n    if (taskType === 'routing' || (taskType === 'quick_response' && ModelConfig.lfm2.useForSimpleQueries)) {'\n      return ModelConfig.text.routing; // Returns 'lfm2: 1.2b'';\n    }\n  }\n  \n  switch (taskType) {\n    case 'vision_analysis':'\n      return ModelConfig.vision.analysis;\n    case 'multimodal_reasoning':'\n      return ModelConfig.vision.multimodal;\n    case 'code_generation':'\n      return ModelConfig.text.code;\n    case 'reasoning':'\n      return ModelConfig.text.reasoning;\n    case 'quick_response':'\n      return ModelConfig.text.small;\n    case 'embedding':'\n      return ModelConfig.embedding.text;\n    case 'vision_embedding':'\n      return ModelConfig.embedding.vision;\n    case 'math':'\n      return ModelConfig.specialized.math;\n    default: // Choose based on speed preference\n      if (speed === 'fast') return ModelConfig.text.small;'\n      if (speed === 'quality') return ModelConfig.text.large;'\n      return ModelConfig.text.medium;\n  }\n}\n\n/**\n * Check if a model is available locally\n */\nexport async function isModelAvailable(model: string): Promise<boolean> {\n  try {\n    // Check with Ollama\n    const response = await fetch('http: //localhost:11434/api/tags');';\n    const data = await response.json();\n    return data.models?.some((m: any) => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}"
      }
    ],
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts",
        "pattern": "missing_comma_object_literal",
        "count": 10,
        "originalContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';\nimport net from 'net';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;\n  storybook: number;\n  \n  // Database\n  postgres: number;\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {\n    mainServer: ports.mainServer,\n    mlServices: {\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;",
        "fixedContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';\nimport net from 'net';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts",
        "pattern": "unterminated_string_single",
        "count": 19,
        "originalContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';\nimport net from 'net';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;",
        "fixedContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport net from 'net';'\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';'\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {'\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts",
        "pattern": "missing_closing_parenthesis",
        "count": 1,
        "originalContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport net from 'net';'\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';'\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {'\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;",
        "fixedContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport net from 'net';'\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';'\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts",
        "pattern": "semicolon_missing_statements",
        "count": 3,
        "originalContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport net from 'net';'\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';'\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;",
        "fixedContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts",
        "pattern": "type_annotation_spacing",
        "count": 4,
        "originalContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;",
        "fixedContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts",
        "pattern": "import_spacing",
        "count": 1,
        "originalContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;",
        "fixedContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext  } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts",
        "pattern": "object_property_spacing",
        "count": 69,
        "originalContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext  } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration:', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;",
        "fixedContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext  } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration: ', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/config/ports.ts",
        "pattern": "arrow_function_spacing",
        "count": 3,
        "originalContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext  } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration: ', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;",
        "fixedContent": "/**\n * Centralized Port Configuration\n * Manages all service ports to avoid conflicts\n */\n\nimport { log, LogContext  } from '../utils/logger';';\nimport net from 'net';';\n\nexport interface PortConfig {\n  // Main services\n  mainServer: number;\n  \n  // ML/AI services\n  lfm2Server: number;,\n  lmStudio: number;\n  ollama: number;\n  \n  // Python bridges\n  dspyOrchestrator: number;,\n  mlxBridge: number;\n  pyVisionBridge: number;\n  \n  // Infrastructure\n  redis: number;,\n  prometheus: number;\n  grafana: number;\n  \n  // Development tools\n  frontend: number;,\n  storybook: number;\n  \n  // Database\n  postgres: number;,\n  supabaseStudio: number;\n}\n\n// Default port configuration\nconst DEFAULT_PORTS: PortConfig = {\n  // Main services\n  mainServer: parseInt(process.env.PORT || '9999'),'\n  \n  // ML/AI services\n  lfm2Server: parseInt(process.env.LFM2_PORT || '3031'),'\n  lmStudio: parseInt(process.env.LM_STUDIO_PORT || '1234'),'\n  ollama: parseInt(process.env.OLLAMA_PORT || '11434'),'\n  \n  // Python bridges\n  dspyOrchestrator: parseInt(process.env.DSPY_PORT || '8001'),'\n  mlxBridge: parseInt(process.env.MLX_BRIDGE_PORT || '8002'),'\n  pyVisionBridge: parseInt(process.env.PYVISION_PORT || '8003'),'\n  \n  // Infrastructure\n  redis: parseInt(process.env.REDIS_PORT || '6379'),'\n  prometheus: parseInt(process.env.PROMETHEUS_PORT || '9090'),'\n  grafana: parseInt(process.env.GRAFANA_PORT || '3001'),'\n  \n  // Development tools\n  frontend: parseInt(process.env.FRONTEND_PORT || '3000'),'\n  storybook: parseInt(process.env.STORYBOOK_PORT || '6006'),'\n  \n  // Database\n  postgres: parseInt(process.env.POSTGRES_PORT || '5432'),'\n  supabaseStudio: parseInt(process.env.SUPABASE_STUDIO_PORT || '54323')'\n};\n\n/**\n * Check if a port is available\n */\nasync function isPortAvailable(port: number): Promise<boolean> {\n  return new Promise((resolve) => {\n    const server = net.createServer();\n    \n    server.listen(port, () => {\n      server.close();\n      resolve(true);\n    });\n    \n    server.on('error', () => {'\n      resolve(false);\n    });\n  });\n}\n\n/**\n * Find an available port starting from the given port\n */\nasync function findAvailablePort(startPort: number, maxAttempts: number = 10): Promise<number> {\n  for (let i = 0; i < maxAttempts; i++) {\n    const port = startPort + i;\n    if (await isPortAvailable(port)) {\n      return port;\n    }\n  }\n  throw new Error(`No available port found starting from ${startPort}`);\n}\n\n/**\n * Auto-configure ports with conflict detection\n */\nexport async function autoConfigurePorts(): Promise<PortConfig> {\n  const ports = { ...DEFAULT_PORTS };\n  const usedPorts = new Set<number>();\n  \n  // Check and auto-assign ports if conflicts exist\n  for (const [service, port] of Object.entries(ports)) {\n    if (usedPorts.has(port)) {\n      // Port conflict detected\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port conflict detected for ${service} on ${port}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else if (!(await isPortAvailable(port))) {\n      // Port already in use by another process\n      const newPort = await findAvailablePort(port + 1);\n      log.warn(`Port ${port} already in use for ${service}, using ${newPort}`, LogContext.CONFIG);\n      (ports as any)[service] = newPort;\n      usedPorts.add(newPort);\n    } else {\n      usedPorts.add(port);\n    }\n  }\n  \n  return ports;\n}\n\n/**\n * Get service URLs based on port configuration\n */\nexport function getServiceUrls(ports: PortConfig) {\n  const host = process.env.HOST || 'localhost';';\n  \n  return {\n    // Main services\n    mainServer: `http://${host}:${ports.mainServer}`,\n    \n    // ML/AI services\n    lfm2Server: `http://${host}:${ports.lfm2Server}`,\n    lmStudio: `http://${host}:${ports.lmStudio}`,\n    ollama: `http://${host}:${ports.ollama}`,\n    \n    // Python bridges\n    dspyOrchestrator: `http://${host}:${ports.dspyOrchestrator}`,\n    mlxBridge: `http://${host}:${ports.mlxBridge}`,\n    pyVisionBridge: `http://${host}:${ports.pyVisionBridge}`,\n    \n    // Infrastructure\n    redis: `redis://${host}:${ports.redis}`,\n    prometheus: `http://${host}:${ports.prometheus}`,\n    grafana: `http://${host}:${ports.grafana}`,\n    \n    // Development tools\n    frontend: `http://${host}:${ports.frontend}`,\n    storybook: `http://${host}:${ports.storybook}`,\n    \n    // Database\n    postgres: `postgresql://${host}:${ports.postgres}`,\n    supabaseStudio: `http://${host}:${ports.supabaseStudio}`\n  };\n}\n\n/**\n * Log port configuration\n */\nexport function logPortConfiguration(ports: PortConfig): void {\n  log.info('🌐 Port Configuration: ', LogContext.CONFIG, {')\n    mainServer: ports.mainServer,\n    mlServices: {,\n      lfm2: ports.lfm2Server,\n      lmStudio: ports.lmStudio,\n      ollama: ports.ollama\n    },\n    pythonBridges: {,\n      dspy: ports.dspyOrchestrator,\n      mlx: ports.mlxBridge,\n      pyVision: ports.pyVisionBridge\n    },\n    infrastructure: {,\n      redis: ports.redis,\n      prometheus: ports.prometheus,\n      grafana: ports.grafana\n    },\n    development: {,\n      frontend: ports.frontend,\n      storybook: ports.storybook\n    },\n    database: {,\n      postgres: ports.postgres,\n      supabaseStudio: ports.supabaseStudio\n    }\n  });\n}\n\n// Export singleton configuration\nlet cachedPorts: PortConfig | null = null;\n\nexport async function getPorts(): Promise<PortConfig> {\n  if (!cachedPorts) {\n    cachedPorts = await autoConfigurePorts();\n    logPortConfiguration(cachedPorts);\n  }\n  return cachedPorts;\n}\n\n// Export default configuration for immediate use\nexport const ports = DEFAULT_PORTS;"
      }
    ],
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/routers/huggingface.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/routers/huggingface.ts",
        "pattern": "missing_comma_object_literal",
        "count": 9,
        "originalContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';\nimport { log, LogContext } from '../utils/logger';\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)',\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Health check failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to get metrics'\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to list models'\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', \n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {\n      model: model || 'default',\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Text generation failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {\n      model: model || 'default',\n      inputType: Array.isArray(inputs) ? 'batch' : 'single'\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Embedding generation failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', \n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required fields: question and context'\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {\n      model: model || 'default',\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Question answering failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {\n      model: model || 'default',\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Summarization failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required field: text'\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {\n      model: model || 'default',\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Sentiment analysis failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required field: requests (must be array)'\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled(\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');\n        }\n        \n        switch (type) {\n          case 'generate':\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })\n    }));\n\n    return res.json({\n      success: true,\n      data: {\n        results: processedResults,\n        summary: {\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,\n          failed: results.filter(r => r.status === 'rejected').length\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Batch processing failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;",
        "fixedContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';\nimport { log, LogContext } from '../utils/logger';\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)',\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Health check failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to get metrics'\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to list models'\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', \n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {\n      model: model || 'default',\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Text generation failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {\n      model: model || 'default',\n      inputType: Array.isArray(inputs) ? 'batch' : 'single'\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Embedding generation failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', \n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, fields: question and context'\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {\n      model: model || 'default',\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Question answering failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {\n      model: model || 'default',\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Summarization failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: text'\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {\n      model: model || 'default',\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Sentiment analysis failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: requests (must be array)'\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled(\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');\n        }\n        \n        switch (type) {\n          case 'generate':\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })\n    }));\n\n    return res.json({\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,\n          failed: results.filter(r => r.status === 'rejected').length\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Batch processing failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/routers/huggingface.ts",
        "pattern": "unterminated_string_single",
        "count": 69,
        "originalContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';\nimport { log, LogContext } from '../utils/logger';\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)',\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Health check failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to get metrics'\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to list models'\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', \n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {\n      model: model || 'default',\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Text generation failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {\n      model: model || 'default',\n      inputType: Array.isArray(inputs) ? 'batch' : 'single'\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Embedding generation failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', \n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, fields: question and context'\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {\n      model: model || 'default',\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Question answering failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs'\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {\n      model: model || 'default',\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Summarization failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: text'\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {\n      model: model || 'default',\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Sentiment analysis failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured'\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: requests (must be array)'\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled(\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');\n        }\n        \n        switch (type) {\n          case 'generate':\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })\n    }));\n\n    return res.json({\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,\n          failed: results.filter(r => r.status === 'rejected').length\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });\n    return res.status(500).json({\n      success: false,\n      error: 'Batch processing failed',\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;",
        "fixedContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';'\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';'\nimport { log, LogContext } from '../utils/logger';'\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';'\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', '\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {'\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {'\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', '\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {'\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {'\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {'\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {'\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled(\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');'\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/routers/huggingface.ts",
        "pattern": "missing_closing_parenthesis",
        "count": 40,
        "originalContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';'\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';'\nimport { log, LogContext } from '../utils/logger';'\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';'\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', '\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {'\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {'\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', '\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {'\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {'\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {'\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {'\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled(\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');'\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;",
        "fixedContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';'\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';'\nimport { log, LogContext } from '../utils/logger';'\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';'\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({)\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({)\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({)\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({)\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({)\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({)\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled()\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');'\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({)\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/routers/huggingface.ts",
        "pattern": "semicolon_missing_statements",
        "count": 37,
        "originalContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';'\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';'\nimport { log, LogContext } from '../utils/logger';'\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';'\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({)\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({)\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({)\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({)\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({)\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({)\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({)\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({)\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled()\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');'\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({)\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({)\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;",
        "fixedContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';';\nimport { log, LogContext } from '../utils/logger';';\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';';\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({);\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({);\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({);\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({);\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({);\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({);\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled();\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');';\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({);\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/routers/huggingface.ts",
        "pattern": "import_spacing",
        "count": 5,
        "originalContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router } from 'express';';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService } from '../services/huggingface-to-lmstudio';';\nimport { log, LogContext } from '../utils/logger';';\nimport { createRateLimiter } from '../middleware/rate-limiter-enhanced';';\nimport { \n  intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n} from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({);\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({);\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({);\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({);\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({);\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({);\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled();\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');';\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({);\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;",
        "fixedContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router  } from 'express';';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService  } from '../services/huggingface-to-lmstudio';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { createRateLimiter  } from '../middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n } from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({);\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({);\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({);\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({);\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({);\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({);\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled();\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');';\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({);\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/routers/huggingface.ts",
        "pattern": "object_property_spacing",
        "count": 103,
        "originalContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router  } from 'express';';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService  } from '../services/huggingface-to-lmstudio';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { createRateLimiter  } from '../middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n } from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({);\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({);\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({);\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({);\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({);\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({);\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled();\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');';\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default:\n            throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({);\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;",
        "fixedContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router  } from 'express';';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService  } from '../services/huggingface-to-lmstudio';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { createRateLimiter  } from '../middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n } from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({);\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({);\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({);\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({);\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({);\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({);\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled();\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');';\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default: throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({);\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/routers/huggingface.ts",
        "pattern": "arrow_function_spacing",
        "count": 10,
        "originalContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router  } from 'express';';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService  } from '../services/huggingface-to-lmstudio';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { createRateLimiter  } from '../middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n } from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({);\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({);\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({);\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({);\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({);\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({);\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled();\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');';\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default: throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({);\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;",
        "fixedContent": "/**\n * HuggingFace API Router\n * Provides REST endpoints for HuggingFace model interactions\n */\n\nimport { Router  } from 'express';';\n// Use LM Studio adapter instead of native HuggingFace service\nimport { huggingFaceService  } from '../services/huggingface-to-lmstudio';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { createRateLimiter  } from '../middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, \n  creativeParametersMiddleware,\n  analysisParametersMiddleware,\n  parameterEffectivenessLogger \n } from '../middleware/intelligent-parameters';'\n\nconst router = Router();\n\n// Apply rate limiting to HuggingFace endpoints\nrouter.use(createRateLimiter());\n\n// Apply parameter effectiveness logging\nrouter.use(parameterEffectivenessLogger());\n\n/**\n * Health check endpoint\n */\nrouter.get('/health', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured (missing API key)','\n        timestamp: new Date().toISOString()\n      });\n    }\n\n    const healthStatus = await huggingFaceService.healthCheck();\n    \n    return res.json({);\n      success: true,\n      data: healthStatus,\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace health check failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Health check failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Get service metrics\n */\nrouter.get('/metrics', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const metrics = huggingFaceService.getMetrics();\n    \n    return res.json({);\n      success: true,\n      data: {\n        ...metrics,\n        modelsUsed: Array.from(metrics.modelsUsed)\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('Failed to get HuggingFace metrics', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to get metrics''\n    });\n  }\n});\n\n/**\n * List available models\n */\nrouter.get('/models', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { task } = req.query;\n    const result = await huggingFaceService.listModels(task as string);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('Failed to list HuggingFace models', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Failed to list models''\n    });\n  }\n});\n\n/**\n * Generate text\n */\nrouter.post('/generate', ')\n  creativeParametersMiddleware(), // Apply intelligent parameters for creative tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing text generation request with intelligent parameters', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length,\n      optimizedTemperature: req.body.temperature,\n      optimizedMaxTokens: req.body.maxTokens\n    });\n\n    // Use optimized parameters from middleware\n    const optimizedParameters = {\n      max_new_tokens: req.body.maxTokens || parameters?.max_new_tokens,\n      temperature: req.body.temperature || parameters?.temperature,\n      top_p: req.body.topP || parameters?.top_p,\n      do_sample: req.body.temperature > 0.1, // Enable sampling for creative tasks\n      ...parameters\n    };\n\n    const result = await huggingFaceService.generateText({);\n      inputs: req.body.enhancedPrompt ? req.body.prompt.replace('{user_input}', inputs) : inputs,'\n      parameters: optimizedParameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace text generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Text generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Generate embeddings\n */\nrouter.post('/embeddings', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing embedding generation request', LogContext.AI, {')\n      model: model || 'default','\n      inputType: Array.isArray(inputs) ? 'batch' : 'single''\n    });\n\n    const result = await huggingFaceService.generateEmbeddings({);\n      inputs,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace embedding generation failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Embedding generation failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Question answering\n */\nrouter.post('/qa', ')\n  analysisParametersMiddleware(), // Apply intelligent parameters for analysis tasks\n  async (req, res) => {\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { question, context, model } = req.body;\n\n    if (!question || !context) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, fields: question and context''\n      });\n    }\n\n    log.info('🤗 Processing question answering request', LogContext.AI, {')\n      model: model || 'default','\n      questionLength: question.length,\n      contextLength: context.length\n    });\n\n    const result = await huggingFaceService.answerQuestion({);\n      question,\n      context,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace question answering failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Question answering failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Text summarization\n */\nrouter.post('/summarize', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { inputs, parameters, model } = req.body;\n\n    if (!inputs) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: inputs''\n      });\n    }\n\n    log.info('🤗 Processing summarization request', LogContext.AI, {')\n      model: model || 'default','\n      inputLength: inputs.length\n    });\n\n    const result = await huggingFaceService.summarizeText({);\n      inputs,\n      parameters,\n      model\n    });\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace summarization failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Summarization failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Sentiment analysis\n */\nrouter.post('/sentiment', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { text, model } = req.body;\n\n    if (!text) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: text''\n      });\n    }\n\n    log.info('🤗 Processing sentiment analysis request', LogContext.AI, {')\n      model: model || 'default','\n      textLength: text.length\n    });\n\n    const result = await huggingFaceService.analyzeSentiment(text, model);\n    \n    return res.json(result);\n\n  } catch (error) {\n    log.error('HuggingFace sentiment analysis failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Sentiment analysis failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\n/**\n * Batch processing endpoint\n */\nrouter.post('/batch', async (req, res) => {'\n  try {\n    if (!huggingFaceService) {\n      return res.status(503).json({);\n        success: false,\n        error: 'HuggingFace service not configured''\n      });\n    }\n\n    const { requests } = req.body;\n\n    if (!Array.isArray(requests) || requests.length === 0) {\n      return res.status(400).json({);\n        success: false,\n        error: 'Missing required, field: requests (must be array)''\n      });\n    }\n\n    log.info('🤗 Processing batch requests', LogContext.AI, {')\n      requestCount: requests.length\n    });\n\n    const results = await Promise.allSettled();\n      requests.map(async (request: any) => {\n        const { type, ...params } = request;\n        \n        if (!huggingFaceService) {\n          throw new Error('HuggingFace service not initialized');';\n        }\n        \n        switch (type) {\n          case 'generate':'\n            return await huggingFaceService.generateText(params);\n          case 'embeddings':'\n            return await huggingFaceService.generateEmbeddings(params);\n          case 'qa':'\n            return await huggingFaceService.answerQuestion(params);\n          case 'summarize':'\n            return await huggingFaceService.summarizeText(params);\n          case 'sentiment':'\n            return await huggingFaceService.analyzeSentiment(params.text, params.model);\n          default: throw new Error(`Unknown request, type: ${type}`);\n        }\n      })\n    );\n\n    const processedResults = results.map((result, index) => ({\n      index,\n      status: result.status,\n      ...(result.status === 'fulfilled' ? { data: result.value } : { error: result.reason })'\n    }));\n\n    return res.json({);\n      success: true,\n      data: {,\n        results: processedResults,\n        summary: {,\n          total: requests.length,\n          successful: results.filter(r => r.status === 'fulfilled').length,'\n          failed: results.filter(r => r.status === 'rejected').length'\n        }\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    log.error('HuggingFace batch processing failed', LogContext.API, { error });'\n    return res.status(500).json({);\n      success: false,\n      error: 'Batch processing failed','\n      details: error instanceof Error ? error.message : String(error)\n    });\n  }\n});\n\nexport default router;"
      }
    ],
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "missing_comma_object_literal",
        "count": 53,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport { createServer } from 'http';\nimport { Server as SocketIOServer } from 'socket.io';\nimport { createClient } from '@supabase/supabase-js';\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';\nimport { log, LogContext } from '@/utils/logger';\nimport { apiResponseMiddleware } from '@/utils/api-response';\nimport { getPorts, logPortConfiguration } from '@/config/ports';\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';\n\n// Types\nimport type { ServiceConfig } from '@/types';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {\n        directives: {\n          defaultSrc: [\"'self'\"],\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000',\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok',\n          timestamp: new Date().toISOString(),\n          version: '1.0.0',\n          environment: config.environment,\n          services: {\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok',\n          timestamp: new Date().toISOString(),\n          version: '1.0.0',\n          environment: config.environment,\n          services: {\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {\n      res.json({\n        service: 'Universal AI Tools',\n        status: 'running',\n        version: '1.0.0',\n        description: 'AI-powered tool orchestration platform',\n        endpoints: {\n          health: '/health',\n          api: {\n            base: '/api/v1',\n            docs: '/api/docs'\n          }\n        },\n        features: [\n          'Agent Orchestration',\n          'Agent-to-Agent (A2A) Communication',\n          'Alpha Evolve Self-Improvement',\n          'Multi-Tier LLM Architecture',\n          'Memory Management', \n          'DSPy Integration',\n          'WebSocket Support',\n          'Authentication'\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {\n      res.json({\n        message: 'Universal AI Tools API v1',\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents',\n          '/api/v1/agents/execute',\n          '/api/v1/agents/parallel',\n          '/api/v1/agents/orchestrate',\n          '/api/v1/a2a',\n          '/api/v1/memory',\n          '/api/v1/orchestration',\n          '/api/v1/knowledge',\n          '/api/v1/auth',\n          '/api/v1/vision',\n          '/api/v1/huggingface',\n          '/api/v1/monitoring',\n          '/api/v1/ab-mcts',\n          '/api/v1/mlx'\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;\n    // this.app.use('/api/v1/a2a', a2aRouter);\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required'\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {\n          throw new Error('Multi-tier LLM service not properly configured');\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm'\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {\n            status: metrics.isInitialized ? 'healthy' : 'initializing',\n            services: {\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {\n            status: 'initializing',\n            services: {\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {\n              available: true, // MPS is available\n              memory: 'Unified Memory'\n            },\n            metrics: {\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {\n              available: false,\n              models: []\n            },\n            gpu: {\n              available: false,\n              memory: '0GB'\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided'\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: { x: 10, y: 10, width: 100, height: 100 } }\n            ],\n            scene: {\n              description: 'Mock scene analysis - Vision system ready for implementation',\n              tags: ['mock', 'test', 'ready'],\n              mood: 'neutral'\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided'\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error });\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32',\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32',\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')\n              .insert({\n                source_type: 'service',\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision\n                content: `Visual content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,\n                content_type: 'image',\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api',\n                memory_type: 'visual',\n                importance: 0.8,\n                metadata: {\n                  type: 'vision_embedding',\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { \n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search'\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available'\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query'\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed'\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32'\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {\n            code: 'SERVICE_UNAVAILABLE',\n            message: 'Agent registry not available'\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown'\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', \n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Agent name and user request are required'\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous',\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {\n            code: 'AGENT_EXECUTION_ERROR',\n            message: 'Agent execution failed',\n            details: errorMessage\n          },\n          metadata: {\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown'\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Agent requests array is required'\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {\n                code: 'INVALID_FORMAT',\n                message: 'Each agent request must have agentName and userRequest'\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;\n        const userId = (req as any).user?.id || 'anonymous';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({\n          agentName: request.agentName,\n          context: {\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel'\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {\n            code: 'PARALLEL_EXECUTION_ERROR',\n            message: 'Parallel agent execution failed',\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Primary agent and user request are required'\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous',\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated'\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {\n            code: 'ORCHESTRATION_ERROR',\n            message: 'Agent orchestration failed',\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000',\n          methods: ['GET', 'POST']\n        }\n      });\n\n      this.io.on('connection', (socket) => {\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {\n          socket.emit('pong', { timestamp: new Date().toISOString() });\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {\n          code: 'NOT_FOUND',\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';\n\n      log.error('Unhandled server error', LogContext.SERVER, {\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {\n          code: 'INTERNAL_SERVER_ERROR',\n          message: config.environment === 'development' ? message : 'Something went wrong'\n        },\n        metadata: {\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {\n      log.error('Uncaught Exception', LogContext.SYSTEM, {\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');\n      this.app.use('/api/v1/monitoring', monitoringModule.default);\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');\n      this.app.use('/api/v1/vision', visionModule.default);\n      log.info('✅ Vision routes loaded', LogContext.SERVER);\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');\n      this.app.use('/api/v1/mlx', mlxModule.default);\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {\n          error: healthCheck.error || 'Unknown health check failure',\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment'\n      });\n      \n      // Don't fail server startup if MLX is unavailable\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport { createServer } from 'http';\nimport { Server as SocketIOServer } from 'socket.io';\nimport { createClient } from '@supabase/supabase-js';\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';\nimport { log, LogContext } from '@/utils/logger';\nimport { apiResponseMiddleware } from '@/utils/api-response';\nimport { getPorts, logPortConfiguration } from '@/config/ports';\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';\n\n// Types\nimport type { ServiceConfig } from '@/types';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000',\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok',\n          timestamp: new Date().toISOString(),\n          version: '1.0.0',\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok',\n          timestamp: new Date().toISOString(),\n          version: '1.0.0',\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {\n      res.json({\n        service: 'Universal AI Tools',\n        status: 'running',\n        version: '1.0.0',\n        description: 'AI-powered tool orchestration platform',\n        endpoints: {,\n          health: '/health',\n          api: {,\n            base: '/api/v1',\n            docs: '/api/docs'\n          }\n        },\n        features: [\n          'Agent Orchestration',\n          'Agent-to-Agent (A2A) Communication',\n          'Alpha Evolve Self-Improvement',\n          'Multi-Tier LLM Architecture',\n          'Memory Management', \n          'DSPy Integration',\n          'WebSocket Support',\n          'Authentication'\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {\n      res.json({\n        message: 'Universal AI Tools API v1',\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents',\n          '/api/v1/agents/execute',\n          '/api/v1/agents/parallel',\n          '/api/v1/agents/orchestrate',\n          '/api/v1/a2a',\n          '/api/v1/memory',\n          '/api/v1/orchestration',\n          '/api/v1/knowledge',\n          '/api/v1/auth',\n          '/api/v1/vision',\n          '/api/v1/huggingface',\n          '/api/v1/monitoring',\n          '/api/v1/ab-mcts',\n          '/api/v1/mlx'\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;\n    // this.app.use('/api/v1/a2a', a2aRouter);\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required'\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {\n          throw new Error('Multi-tier LLM service not properly configured');\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm'\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing',\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            status: 'initializing',\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory'\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB'\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided'\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation',\n              tags: ['mock', 'test', 'ready'],\n              mood: 'neutral'\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided'\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error });\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32',\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32',\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')\n              .insert({\n                source_type: 'service',\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,\n                content_type: 'image',\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api',\n                memory_type: 'visual',\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding',\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { \n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search'\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available'\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query'\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed'\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32'\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE',\n            message: 'Agent registry not available'\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown'\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', \n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Agent name and user request are required'\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous',\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR',\n            message: 'Agent execution failed',\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown'\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Agent requests array is required'\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT',\n                message: 'Each agent request must have agentName and userRequest'\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;\n        const userId = (req as any).user?.id || 'anonymous';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel'\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR',\n            message: 'Parallel agent execution failed',\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Primary agent and user request are required'\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous',\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated'\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR',\n            message: 'Agent orchestration failed',\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000',\n          methods: ['GET', 'POST']\n        }\n      });\n\n      this.io.on('connection', (socket) => {\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {\n          socket.emit('pong', { timestamp: new Date().toISOString() });\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {,\n          code: 'NOT_FOUND',\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';\n\n      log.error('Unhandled server error', LogContext.SERVER, {\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR',\n          message: config.environment === 'development' ? message : 'Something went wrong'\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {\n      log.error('Uncaught Exception', LogContext.SYSTEM, {\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');\n      this.app.use('/api/v1/monitoring', monitoringModule.default);\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');\n      this.app.use('/api/v1/vision', visionModule.default);\n      log.info('✅ Vision routes loaded', LogContext.SERVER);\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');\n      this.app.use('/api/v1/mlx', mlxModule.default);\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {\n          error: healthCheck.error || 'Unknown health check failure',\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment'\n      });\n      \n      // Don't fail server startup if MLX is unavailable\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "unterminated_string_single",
        "count": 242,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport { createServer } from 'http';\nimport { Server as SocketIOServer } from 'socket.io';\nimport { createClient } from '@supabase/supabase-js';\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';\nimport { log, LogContext } from '@/utils/logger';\nimport { apiResponseMiddleware } from '@/utils/api-response';\nimport { getPorts, logPortConfiguration } from '@/config/ports';\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';\n\n// Types\nimport type { ServiceConfig } from '@/types';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000',\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok',\n          timestamp: new Date().toISOString(),\n          version: '1.0.0',\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok',\n          timestamp: new Date().toISOString(),\n          version: '1.0.0',\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {\n      res.json({\n        service: 'Universal AI Tools',\n        status: 'running',\n        version: '1.0.0',\n        description: 'AI-powered tool orchestration platform',\n        endpoints: {,\n          health: '/health',\n          api: {,\n            base: '/api/v1',\n            docs: '/api/docs'\n          }\n        },\n        features: [\n          'Agent Orchestration',\n          'Agent-to-Agent (A2A) Communication',\n          'Alpha Evolve Self-Improvement',\n          'Multi-Tier LLM Architecture',\n          'Memory Management', \n          'DSPy Integration',\n          'WebSocket Support',\n          'Authentication'\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {\n      res.json({\n        message: 'Universal AI Tools API v1',\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents',\n          '/api/v1/agents/execute',\n          '/api/v1/agents/parallel',\n          '/api/v1/agents/orchestrate',\n          '/api/v1/a2a',\n          '/api/v1/memory',\n          '/api/v1/orchestration',\n          '/api/v1/knowledge',\n          '/api/v1/auth',\n          '/api/v1/vision',\n          '/api/v1/huggingface',\n          '/api/v1/monitoring',\n          '/api/v1/ab-mcts',\n          '/api/v1/mlx'\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;\n    // this.app.use('/api/v1/a2a', a2aRouter);\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required'\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {\n          throw new Error('Multi-tier LLM service not properly configured');\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm'\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing',\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            status: 'initializing',\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory'\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB'\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided'\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation',\n              tags: ['mock', 'test', 'ready'],\n              mood: 'neutral'\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided'\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error });\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32',\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32',\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')\n              .insert({\n                source_type: 'service',\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,\n                content_type: 'image',\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api',\n                memory_type: 'visual',\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding',\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { \n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search'\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available'\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query'\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed'\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32'\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed',\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE',\n            message: 'Agent registry not available'\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown'\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', \n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Agent name and user request are required'\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous',\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR',\n            message: 'Agent execution failed',\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown'\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Agent requests array is required'\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT',\n                message: 'Each agent request must have agentName and userRequest'\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;\n        const userId = (req as any).user?.id || 'anonymous';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel'\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR',\n            message: 'Parallel agent execution failed',\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE',\n              message: 'Agent registry not available'\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD',\n              message: 'Primary agent and user request are required'\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous',\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated'\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR',\n            message: 'Agent orchestration failed',\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000',\n          methods: ['GET', 'POST']\n        }\n      });\n\n      this.io.on('connection', (socket) => {\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {\n          socket.emit('pong', { timestamp: new Date().toISOString() });\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {,\n          code: 'NOT_FOUND',\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';\n\n      log.error('Unhandled server error', LogContext.SERVER, {\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR',\n          message: config.environment === 'development' ? message : 'Something went wrong'\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {\n      log.error('Uncaught Exception', LogContext.SYSTEM, {\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');\n      this.app.use('/api/v1/monitoring', monitoringModule.default);\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');\n      this.app.use('/api/v1/vision', visionModule.default);\n      log.info('✅ Vision routes loaded', LogContext.SERVER);\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');\n      this.app.use('/api/v1/mlx', mlxModule.default);\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {\n          error: healthCheck.error || 'Unknown health check failure',\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment'\n      });\n      \n      // Don't fail server startup if MLX is unavailable\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';'\nimport cors from 'cors';'\nimport helmet from 'helmet';'\nimport { createServer } from 'http';'\nimport { Server as SocketIOServer } from 'socket.io';'\nimport { createClient } from '@supabase/supabase-js';'\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { apiResponseMiddleware } from '@/utils/api-response';'\nimport { getPorts, logPortConfiguration } from '@/config/ports';'\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';'\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';'\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';'\n\n// Types\nimport type { ServiceConfig } from '@/types';'\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');'\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');'\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');'\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');'\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {'\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {'\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error });'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')'\n              .insert({\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { '\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')'\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {'\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {'\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', '\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {'\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const userId = (req as any).user?.id || 'anonymous';'\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';'\n\n      log.error('Unhandled server error', LogContext.SERVER, {'\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {'\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {'\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');'\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');'\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');'\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');'\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');'\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');'\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');'\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');'\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';'\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {'\n          error: healthCheck.error || 'Unknown health check failure','\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {'\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "unterminated_string_double",
        "count": 4,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';'\nimport cors from 'cors';'\nimport helmet from 'helmet';'\nimport { createServer } from 'http';'\nimport { Server as SocketIOServer } from 'socket.io';'\nimport { createClient } from '@supabase/supabase-js';'\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { apiResponseMiddleware } from '@/utils/api-response';'\nimport { getPorts, logPortConfiguration } from '@/config/ports';'\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';'\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';'\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';'\n\n// Types\nimport type { ServiceConfig } from '@/types';'\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');'\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');'\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');'\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');'\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {'\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {'\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error });'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')'\n              .insert({\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { '\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')'\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {'\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {'\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', '\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {'\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const userId = (req as any).user?.id || 'anonymous';'\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';'\n\n      log.error('Unhandled server error', LogContext.SERVER, {'\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {'\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {'\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');'\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');'\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');'\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');'\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');'\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');'\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');'\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');'\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';'\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {'\n          error: healthCheck.error || 'Unknown health check failure','\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {'\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';'\nimport cors from 'cors';'\nimport helmet from 'helmet';'\nimport { createServer } from 'http';'\nimport { Server as SocketIOServer } from 'socket.io';'\nimport { createClient } from '@supabase/supabase-js';'\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { apiResponseMiddleware } from '@/utils/api-response';'\nimport { getPorts, logPortConfiguration } from '@/config/ports';'\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';'\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';'\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';'\n\n// Types\nimport type { ServiceConfig } from '@/types';'\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');'\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');'\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');'\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');'\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {'\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {'\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error });'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')'\n              .insert({\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { '\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')'\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {'\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {'\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', '\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {'\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const userId = (req as any).user?.id || 'anonymous';'\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';'\n\n      log.error('Unhandled server error', LogContext.SERVER, {'\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {'\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {'\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');'\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');'\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');'\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');'\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');'\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');'\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');'\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');'\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';'\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {'\n          error: healthCheck.error || 'Unknown health check failure','\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {'\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "missing_parentheses_logger",
        "count": 2,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';'\nimport cors from 'cors';'\nimport helmet from 'helmet';'\nimport { createServer } from 'http';'\nimport { Server as SocketIOServer } from 'socket.io';'\nimport { createClient } from '@supabase/supabase-js';'\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { apiResponseMiddleware } from '@/utils/api-response';'\nimport { getPorts, logPortConfiguration } from '@/config/ports';'\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';'\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';'\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';'\n\n// Types\nimport type { ServiceConfig } from '@/types';'\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');'\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');'\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');'\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');'\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {'\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {'\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error });'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')'\n              .insert({\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { '\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')'\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {'\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {'\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', '\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {'\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const userId = (req as any).user?.id || 'anonymous';'\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';'\n\n      log.error('Unhandled server error', LogContext.SERVER, {'\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {'\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {'\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');'\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');'\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');'\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');'\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');'\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');'\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');'\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');'\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';'\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {'\n          error: healthCheck.error || 'Unknown health check failure','\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {'\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';'\nimport cors from 'cors';'\nimport helmet from 'helmet';'\nimport { createServer } from 'http';'\nimport { Server as SocketIOServer } from 'socket.io';'\nimport { createClient } from '@supabase/supabase-js';'\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { apiResponseMiddleware } from '@/utils/api-response';'\nimport { getPorts, logPortConfiguration } from '@/config/ports';'\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';'\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';'\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';'\n\n// Types\nimport type { ServiceConfig } from '@/types';'\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');'\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');'\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');'\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');'\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {'\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {'\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')'\n              .insert({\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { '\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')'\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {'\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {'\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', '\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {'\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const userId = (req as any).user?.id || 'anonymous';'\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';'\n\n      log.error('Unhandled server error', LogContext.SERVER, {'\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {'\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {'\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');'\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');'\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');'\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');'\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');'\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');'\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');'\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');'\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';'\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {'\n          error: healthCheck.error(|| 'Unknown health check failure','\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {'\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "missing_closing_parenthesis",
        "count": 75,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';'\nimport cors from 'cors';'\nimport helmet from 'helmet';'\nimport { createServer } from 'http';'\nimport { Server as SocketIOServer } from 'socket.io';'\nimport { createClient } from '@supabase/supabase-js';'\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { apiResponseMiddleware } from '@/utils/api-response';'\nimport { getPorts, logPortConfiguration } from '@/config/ports';'\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';'\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';'\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';'\n\n// Types\nimport type { ServiceConfig } from '@/types';'\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');'\n      }\n\n      this.supabase = createClient(\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info(\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');'\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');'\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');'\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {'\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {'\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')'\n              .insert({\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { '\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')'\n                .insert({\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {'\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {'\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {'\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', '\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {'\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const userId = (req as any).user?.id || 'anonymous';'\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents(\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {'\n          error: errorMessage\n        });\n\n        res.status(500).json({\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';'\n\n      log.error('Unhandled server error', LogContext.SERVER, {'\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {'\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {'\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');'\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info(\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');'\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');'\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');'\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');'\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');'\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {'\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');'\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');'\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';'\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {'\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {'\n          error: healthCheck.error(|| 'Unknown health check failure','\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {'\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';'\nimport cors from 'cors';'\nimport helmet from 'helmet';'\nimport { createServer } from 'http';'\nimport { Server as SocketIOServer } from 'socket.io';'\nimport { createClient } from '@supabase/supabase-js';'\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { apiResponseMiddleware } from '@/utils/api-response';'\nimport { getPorts, logPortConfiguration } from '@/config/ports';'\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';'\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';'\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';'\n\n// Types\nimport type { ServiceConfig } from '@/types';'\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');'\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');'\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({)\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');'\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');'\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({)\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({)\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({)\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({)\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({)\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({)\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({)\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({)\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({)\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({)\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({)\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({)\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({)\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({)\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({)\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({)\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const userId = (req as any).user?.id || 'anonymous';'\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({)\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({)\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents()\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';'\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');'\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');'\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');'\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');'\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');'\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');'\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');'\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');'\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';'\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "semicolon_missing_statements",
        "count": 62,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';'\nimport cors from 'cors';'\nimport helmet from 'helmet';'\nimport { createServer } from 'http';'\nimport { Server as SocketIOServer } from 'socket.io';'\nimport { createClient } from '@supabase/supabase-js';'\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { apiResponseMiddleware } from '@/utils/api-response';'\nimport { getPorts, logPortConfiguration } from '@/config/ports';'\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';'\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';'\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';'\n\n// Types\nimport type { ServiceConfig } from '@/types';'\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');'\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');'\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({)\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');'\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');'\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({)\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({)\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({)\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({)\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({)\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({)\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({)\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({)\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({)\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({)\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({)\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({)\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({)\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({)\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({)\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({)\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const userId = (req as any).user?.id || 'anonymous';'\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({)\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({)\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;'\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents()\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';'\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');'\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');'\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');'\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');'\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');'\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');'\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');'\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');'\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');'\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';'\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer } from 'http';';\nimport { Server as SocketIOServer } from 'socket.io';';\nimport { createClient } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';';\nimport { log, LogContext } from '@/utils/logger';';\nimport { apiResponseMiddleware } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "type_annotation_spacing",
        "count": 3,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer } from 'http';';\nimport { Server as SocketIOServer } from 'socket.io';';\nimport { createClient } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';';\nimport { log, LogContext } from '@/utils/logger';';\nimport { apiResponseMiddleware } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer } from 'http';';\nimport { Server as SocketIOServer } from 'socket.io';';\nimport { createClient } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';';\nimport { log, LogContext } from '@/utils/logger';';\nimport { apiResponseMiddleware } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "import_spacing",
        "count": 9,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer } from 'http';';\nimport { Server as SocketIOServer } from 'socket.io';';\nimport { createClient } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig } from '@/config/environment';';\nimport { log, LogContext } from '@/utils/logger';';\nimport { apiResponseMiddleware } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer  } from 'http';';\nimport { Server as SocketIOServer  } from 'socket.io';';\nimport { createClient  } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig  } from '@/config/environment';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { apiResponseMiddleware  } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration  } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter  } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters  } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "object_property_spacing",
        "count": 398,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer  } from 'http';';\nimport { Server as SocketIOServer  } from 'socket.io';';\nimport { createClient  } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig  } from '@/config/environment';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { apiResponseMiddleware  } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration  } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter  } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters  } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data:\", \"https:\"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer  } from 'http';';\nimport { Server as SocketIOServer  } from 'socket.io';';\nimport { createClient  } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig  } from '@/config/environment';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { apiResponseMiddleware  } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration  } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter  } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters  } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data: \", \"https: \"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message: String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file: //${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/server.ts",
        "pattern": "arrow_function_spacing",
        "count": 27,
        "originalContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer  } from 'http';';\nimport { Server as SocketIOServer  } from 'socket.io';';\nimport { createClient  } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig  } from '@/config/environment';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { apiResponseMiddleware  } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration  } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter  } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters  } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data: \", \"https: \"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message: String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file: //${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };",
        "fixedContent": "/**\n * Universal AI Tools Service - Clean Implementation\n * Main server with Express, TypeScript, and comprehensive error handling\n */\n\nimport express from 'express';';\nimport cors from 'cors';';\nimport helmet from 'helmet';';\nimport { createServer  } from 'http';';\nimport { Server as SocketIOServer  } from 'socket.io';';\nimport { createClient  } from '@supabase/supabase-js';';\n\n// Configuration and utilities\nimport { config, validateConfig  } from '@/config/environment';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { apiResponseMiddleware  } from '@/utils/api-response';';\nimport { getPorts, logPortConfiguration  } from '@/config/ports';';\n\n// Middleware\nimport { createRateLimiter  } from '@/middleware/rate-limiter-enhanced';';\nimport { intelligentParametersMiddleware, optimizeParameters  } from '@/middleware/intelligent-parameters';';\n\n// Agent system\nimport AgentRegistry from '@/agents/agent-registry';';\n\n// Types\nimport type { ServiceConfig } from '@/types';';\n\nclass UniversalAIToolsServer {\n  private app: express.Application;\n  private server: any;\n  private io: SocketIOServer | null = null;\n  private supabase: any = null;\n  private agentRegistry: AgentRegistry | null = null;\n  private isShuttingDown = false;\n\n  constructor() {\n    this.app = express();\n    this.server = createServer(this.app);\n    this.initializeServices();\n    this.setupMiddleware();\n    this.setupRoutesSync();\n    this.setupWebSocket();\n    this.setupErrorHandling();\n    \n    // Load async routes after server starts\n    this.loadAsyncRoutes();\n  }\n\n  private initializeServices(): void {\n    this.initializeSupabase();\n    this.initializeAgentRegistry();\n  }\n\n  private initializeAgentRegistry(): void {\n    try {\n      this.agentRegistry = new AgentRegistry();\n      log.info('✅ Agent Registry initialized', LogContext.AGENT);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Agent Registry', LogContext.AGENT, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without agents'\n    }\n  }\n\n  private initializeSupabase(): void {\n    try {\n      if (!config.supabase.url || !config.supabase.serviceKey) {\n        throw new Error('Supabase configuration missing');';\n      }\n\n      this.supabase = createClient()\n        config.supabase.url,\n        config.supabase.serviceKey\n      );\n\n      log.info('✅ Supabase client initialized', LogContext.DATABASE);'\n    } catch (error) {\n      log.error('❌ Failed to initialize Supabase client', LogContext.DATABASE, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      // Don't throw - allow server to start without Supabase for testing'\n    }\n  }\n\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet({)\n      contentSecurityPolicy: {,\n        directives: {\n          defaultSrc: [\"'self'\"],'\"\n          scriptSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          styleSrc: [\"'self'\", \"'unsafe-inline'\"],'\"\n          imgSrc: [\"'self'\", \"data: \", \"https: \"],'\"\n        },\n      },\n    }));\n\n    // CORS middleware\n    this.app.use(cors({)\n      origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n      credentials: true,\n      methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],'\n      allowedHeaders: ['Content-Type', 'Authorization', 'X-API-Key', 'X-AI-Service']'\n    }));\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '50mb' }));'\n    this.app.use(express.urlencoded({ extended: true, limit: '50mb' }));'\n\n    // Rate limiting middleware - Apply globally\n    this.app.use(createRateLimiter());\n\n    // API response middleware\n    this.app.use(apiResponseMiddleware);\n\n    // Request logging middleware\n    this.app.use((req, res, next) => {\n      const startTime = Date.now();\n      \n      res.on('finish', () => {'\n        const duration = Date.now() - startTime;\n        log.info()\n          `${req.method} ${req.path} - ${res.statusCode}`,\n          LogContext.API,\n          {\n            method: req.method,\n            path: req.path,\n            statusCode: res.statusCode,\n            duration: `${duration}ms`,\n            userAgent: req.get('User-Agent'),'\n            ip: req.ip\n          }\n        );\n      });\n      \n      next();\n    });\n\n    log.info('✅ Middleware setup completed', LogContext.SERVER);'\n  }\n\n  private setupRoutesSync(): void {\n    // Health check endpoint\n    this.app.get('/health', async (req, res) => {'\n      try {\n        // Check MLX service health\n        let mlxHealth = false;\n        try {\n          const { mlxService } = await import('./services/mlx-service');';\n          const mlxStatus = await mlxService.healthCheck();\n          mlxHealth = mlxStatus.healthy;\n        } catch (error) {\n          // MLX service not available\n          mlxHealth = false;\n        }\n\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false, // Will be updated when Redis is added\n            mlx: mlxHealth\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      } catch (error) {\n        // Fallback to synchronous health check if async fails\n        const health = {\n          status: 'ok','\n          timestamp: new Date().toISOString(),\n          version: '1.0.0','\n          environment: config.environment,\n          services: {,\n            supabase: !!this.supabase,\n            websocket: !!this.io,\n            agentRegistry: !!this.agentRegistry,\n            redis: false,\n            mlx: false\n          },\n          agents: this.agentRegistry ? {,\n            total: this.agentRegistry.getAvailableAgents().length,\n            loaded: this.agentRegistry.getLoadedAgents().length,\n            available: this.agentRegistry.getAvailableAgents().map(a => a.name)\n          } : null,\n          uptime: process.uptime()\n        };\n\n        res.json(health);\n      }\n    });\n\n    // Root endpoint\n    this.app.get('/', (req, res) => {'\n      res.json({)\n        service: 'Universal AI Tools','\n        status: 'running','\n        version: '1.0.0','\n        description: 'AI-powered tool orchestration platform','\n        endpoints: {,\n          health: '/health','\n          api: {,\n            base: '/api/v1','\n            docs: '/api/docs''\n          }\n        },\n        features: [\n          'Agent Orchestration','\n          'Agent-to-Agent (A2A) Communication','\n          'Alpha Evolve Self-Improvement','\n          'Multi-Tier LLM Architecture','\n          'Memory Management', '\n          'DSPy Integration','\n          'WebSocket Support','\n          'Authentication''\n        ]\n      });\n    });\n\n    // API base route\n    this.app.get('/api/v1', (req, res) => {'\n      res.json({)\n        message: 'Universal AI Tools API v1','\n        timestamp: new Date().toISOString(),\n        availableEndpoints: [\n          '/api/v1/agents','\n          '/api/v1/agents/execute','\n          '/api/v1/agents/parallel','\n          '/api/v1/agents/orchestrate','\n          '/api/v1/a2a','\n          '/api/v1/memory','\n          '/api/v1/orchestration','\n          '/api/v1/knowledge','\n          '/api/v1/auth','\n          '/api/v1/vision','\n          '/api/v1/huggingface','\n          '/api/v1/monitoring','\n          '/api/v1/ab-mcts','\n          '/api/v1/mlx''\n        ]\n      });\n    });\n\n    // Agent API endpoints\n    this.setupAgentRoutes();\n\n    // Vision API endpoints\n    this.setupVisionRoutes();\n\n    // A2A Communication mesh endpoints (temporarily disabled until import fixed)\n    // TODO: Fix import issue with a2a-collaboration router\n    // const a2aRouter = require('./routers/a2a-collaboration').default;'\n    // this.app.use('/api/v1/a2a', a2aRouter);'\n\n    // Multi-tier LLM test endpoint\n    this.app.post('/api/v1/multi-tier/execute', async (req, res) => {'\n      try {\n        const { userRequest, context = {} } = req.body;\n\n        if (!userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: 'userRequest is required''\n          });\n        }\n\n        // Import multi-tier service\n        const { multiTierLLM } = await import('./services/multi-tier-llm-service');';\n        \n        // Check if execute method exists\n        if (!multiTierLLM || typeof multiTierLLM.execute !== 'function') {'\n          throw new Error('Multi-tier LLM service not properly configured');';\n        }\n        \n        const result = await multiTierLLM.execute(userRequest, context);\n\n        return res.json({);\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            service: 'multi-tier-llm''\n          }\n        });\n\n        log.info('🚀 Multi-tier LLM execution completed', LogContext.AI, {')\n          tier: result.metadata.tier,\n          modelUsed: result.metadata.modelUsed,\n          executionTime: `${result.metadata.executionTime}ms`,\n          complexity: result.metadata.classification.complexity\n        });\n\n      } catch (error) {\n        log.error('❌ Multi-tier LLM execution failed', LogContext.SERVER, { error });'\n        res.status(500).json({)\n          success: false,\n          error: 'Multi-tier LLM execution failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Basic routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupVisionRoutes(): void {\n    // Basic vision endpoints (these will be replaced by the full vision router)\n    this.app.get('/api/v1/vision/health', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            status: metrics.isInitialized ? 'healthy' : 'initializing','\n            services: {,\n              pyVision: metrics.isInitialized,\n              resourceManager: true\n            },\n            timestamp: Date.now()\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            status: 'initializing','\n            services: {,\n              pyVision: false,\n              resourceManager: false\n            },\n            timestamp: Date.now()\n          }\n        });\n      }\n    });\n\n    this.app.get('/api/v1/vision/status', async (req, res) => {'\n      try {\n        // Try to get actual PyVision status\n        const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n        const metrics = pyVisionBridge.getMetrics();\n        \n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: metrics.isInitialized,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: metrics.isInitialized,\n              models: metrics.modelsLoaded\n            },\n            gpu: {,\n              available: true, // MPS is available\n              memory: 'Unified Memory''\n            },\n            metrics: {,\n              totalRequests: metrics.totalRequests,\n              successRate: metrics.successRate,\n              avgResponseTime: `${metrics.avgResponseTime.toFixed(0)}ms`,\n              cacheHitRate: `${(metrics.cacheHitRate * 100).toFixed(1)}%`\n            }\n          }\n        });\n      } catch (error) {\n        res.json({)\n          success: true,\n          data: {,\n            service: {\n              initialized: false,\n              uptime: process.uptime(),\n            },\n            python: {,\n              available: false,\n              models: []\n            },\n            gpu: {,\n              available: false,\n              memory: '0GB''\n            }\n          }\n        });\n      }\n    });\n\n    // Mock analyze endpoint for testing\n    this.app.post('/api/v1/vision/analyze', (req, res) => {'\n      const { imagePath, imageBase64 } = req.body;\n      \n      if (!imagePath && !imageBase64) {\n        return res.status(400).json({);\n          success: false,\n          error: 'Either imagePath or imageBase64 must be provided''\n        });\n      }\n\n      // Mock response\n      return res.json({);\n        success: true,\n        data: {,\n          analysis: {\n            objects: [\n              { class: 'mock_object', confidence: 0.95, bbox: {, x: 10, y: 10, width: 100, height: 100 } }'\n            ],\n            scene: {,\n              description: 'Mock scene analysis - Vision system ready for implementation','\n              tags: ['mock', 'test', 'ready'],'\n              mood: 'neutral''\n            },\n            text: [],\n            confidence: 0.9,\n            processingTimeMs: 100\n          },\n          processingTime: 100,\n          cached: false,\n          mock: true\n        }\n      });\n    });\n\n    // Vision embedding endpoint with Supabase integration\n    this.app.post('/api/v1/vision/embed', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, saveToMemory = true } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided''\n          });\n        }\n\n        log.info('🔢 Processing vision embedding request', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          saveToMemory\n        });\n\n        let embeddingResult: any = null;\n        let isRealEmbedding = false;\n\n        // Try to use PyVision bridge\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success) {\n            embeddingResult = result;\n            isRealEmbedding = true;\n            log.info('✅ Real CLIP embedding generated', LogContext.AI, {')\n              model: result.model,\n              dimension: result.data?.dimension\n            });\n          } else {\n            log.warn('PyVision embedding failed, using mock', LogContext.AI, { error: result.error(});'\n          }\n        } catch (error) {\n          log.warn('PyVision bridge not available, using mock', LogContext.AI, { error });'\n        }\n\n        // Fallback to mock embedding if needed\n        if (!embeddingResult) {\n          const mockEmbedding = new Array(512).fill(0).map(() => Math.random() * 0.1 - 0.05);\n          embeddingResult = {\n            success: true,\n            data: {,\n              vector: mockEmbedding,\n              model: 'mock-clip-vit-b32','\n              dimension: 512\n            },\n            model: 'mock-clip-vit-b32','\n            processingTime: 50 + Math.random() * 100,\n            cached: false\n          };\n        }\n\n        // Save to Supabase if requested and we have a real embedding\n        let memoryId = null;\n        if (saveToMemory && this.supabase && isRealEmbedding) {\n          try {\n            // First create a memory record\n            const { data: memoryData, error: memoryError } = await this.supabase;\n              .from('memories')'\n              .insert({)\n                source_type: 'service','\n                source_id: '00000000-0000-0000-0000-000000000001', // Static service UUID for vision'\n                content: `Visual, content: ${imagePath ? `image at ${imagePath}` : 'base64 image'}`,'\n                content_type: 'image','\n                visual_embedding: embeddingResult.data.vector,\n                image_metadata: {,\n                  model: embeddingResult.model,\n                  dimension: embeddingResult.data.dimension,\n                  processingTime: embeddingResult.processingTime,\n                  timestamp: new Date().toISOString()\n                },\n                image_path: imagePath || null,\n                is_generated: false,\n                source: 'vision-embedding-api','\n                memory_type: 'visual','\n                importance: 0.8,;\n                metadata: {,\n                  type: 'vision_embedding','\n                  model: embeddingResult.model\n                }\n              })\n              .select('id')'\n              .single();\n\n            if (memoryError) {\n              log.error('Failed to save memory record', LogContext.DATABASE, { error: memoryError });'\n            } else {\n              memoryId = memoryData?.id;\n              log.info('✅ Vision embedding saved to memory', LogContext.DATABASE, { ')\n                memoryId,\n                model: embeddingResult.model\n              });\n\n              // Also save to vision_embeddings table for faster lookups\n              const { error: embeddingError } = await this.supabase;\n                .from('vision_embeddings')'\n                .insert({)\n                  memory_id: memoryId,\n                  embedding: embeddingResult.data.vector,\n                  model_version: embeddingResult.model,\n                  confidence: 0.95 // Default confidence for real embeddings\n                });\n\n              if (embeddingError) {\n                log.error('Failed to save embedding record', LogContext.DATABASE, { error: embeddingError });'\n              } else {\n                log.info('✅ Vision embedding indexed for fast search', LogContext.DATABASE, { memoryId });'\n              }\n            }\n          } catch (supabaseError) {\n            log.error('Supabase integration error', LogContext.DATABASE, { error: supabaseError });'\n          }\n        }\n\n        return res.json({);\n          success: true,\n          data: embeddingResult.data,\n          model: embeddingResult.model,\n          processingTime: embeddingResult.processingTime,\n          cached: embeddingResult.cached || false,\n          mock: !isRealEmbedding,\n          memoryId,\n          savedToDatabase: !!memoryId\n        });\n\n      } catch (error) {\n        log.error('Vision embedding endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision embedding failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    // Vision similarity search endpoint\n    this.app.post('/api/v1/vision/search', async (req, res) => {'\n      try {\n        const { imagePath, imageBase64, limit = 10, threshold = 0.8 } = req.body;\n        \n        if (!imagePath && !imageBase64) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Either imagePath or imageBase64 must be provided for search''\n          });\n        }\n\n        if (!this.supabase) {\n          return res.status(503).json({);\n            success: false,\n            error: 'Database service not available''\n          });\n        }\n\n        log.info('🔍 Processing vision similarity search', LogContext.AI, {')\n          hasImagePath: !!imagePath,\n          hasImageBase64: !!imageBase64,\n          limit,\n          threshold\n        });\n\n        // First generate embedding for the search query\n        let queryEmbedding = null;\n        try {\n          const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n          const imageData = imageBase64 || imagePath;\n          const result = await pyVisionBridge.generateEmbedding(imageData);\n\n          if (result.success && result.data) {\n            queryEmbedding = result.data.vector;\n            log.info('✅ Query embedding generated for search', LogContext.AI);'\n          }\n        } catch (error) {\n          log.warn('PyVision not available for search', LogContext.AI, { error });'\n        }\n\n        if (!queryEmbedding) {\n          return res.status(400).json({);\n            success: false,\n            error: 'Unable to generate embedding for search query''\n          });\n        }\n\n        // Search for similar images using the database function\n        const { data: searchResults, error: searchError } = await this.supabase;\n          .rpc('search_similar_images', {')\n            query_embedding: queryEmbedding,\n            limit_count: limit,\n            threshold: threshold\n          });\n\n        if (searchError) {\n          log.error('Vision similarity search failed', LogContext.DATABASE, { error: searchError });'\n          return res.status(500).json({);\n            success: false,\n            error: 'Similarity search failed''\n          });\n        }\n\n        log.info('✅ Vision similarity search completed', LogContext.AI, {')\n          resultsFound: searchResults?.length || 0\n        });\n\n        return res.json({);\n          success: true,\n          data: {,\n            results: searchResults || [],\n            query: {\n              threshold,\n              limit,\n              embeddingModel: 'clip-vit-b32''\n            }\n          },\n          resultsCount: searchResults?.length || 0\n        });\n\n      } catch (error) {\n        log.error('Vision search endpoint error', LogContext.API, { error });'\n        return res.status(500).json({);\n          success: false,\n          error: 'Vision search failed','\n          details: error instanceof Error ? error.message : String(error)\n        });\n      }\n    });\n\n    log.info('✅ Vision routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupAgentRoutes(): void {\n    // List available agents\n    this.app.get('/api/v1/agents', (req, res) => {'\n      if (!this.agentRegistry) {\n        res.status(503).json({)\n          success: false,\n          error: {,\n            code: 'SERVICE_UNAVAILABLE','\n            message: 'Agent registry not available''\n          }\n        });\n        return;\n      }\n\n      const agents = this.agentRegistry.getAvailableAgents();\n      const loadedAgents = this.agentRegistry.getLoadedAgents();\n\n      res.json({)\n        success: true,\n        data: {,\n          total: agents.length,\n          loaded: loadedAgents.length,\n          agents: agents.map(agent => ({,)\n            name: agent.name,\n            description: agent.description,\n            category: agent.category,\n            priority: agent.priority,\n            capabilities: agent.capabilities,\n            memoryEnabled: agent.memoryEnabled,\n            maxLatencyMs: agent.maxLatencyMs,\n            loaded: loadedAgents.includes(agent.name)\n          }))\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.headers['x-request-id'] || 'unknown''\n        }\n      });\n    });\n\n    // Execute agent\n    this.app.post('/api/v1/agents/execute', ')\n      intelligentParametersMiddleware(), // Apply intelligent parameters for agent tasks\n      async (req, res) => {\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentName, userRequest, context = {} } = req.body;\n\n        if (!agentName || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent name and user request are required''\n            }\n          });\n        }\n\n        const agentContext = {\n          userRequest,\n          requestId: req.headers['x-request-id'] as string || `req_${Date.now()}`,'\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const result = await this.agentRegistry.processRequest(agentName, agentContext);\n\n        res.json({)\n          success: true,\n          data: result,\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: agentContext.requestId,\n            agentName\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Agent execution error', LogContext.API, {')\n          error: errorMessage,\n          agentName: req.body.agentName\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'AGENT_EXECUTION_ERROR','\n            message: 'Agent execution failed','\n            details: errorMessage\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId: req.headers['x-request-id'] || 'unknown''\n          }\n        });\n        return;\n      }\n    });\n\n    // Parallel agent execution\n    this.app.post('/api/v1/agents/parallel', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { agentRequests } = req.body;\n\n        if (!Array.isArray(agentRequests) || agentRequests.length === 0) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Agent requests array is required''\n            }\n          });\n        }\n\n        // Validate each request\n        for (const request of agentRequests) {\n          if (!request.agentName || !request.userRequest) {\n            return res.status(400).json({);\n              success: false,\n              error: {,\n                code: 'INVALID_FORMAT','\n                message: 'Each agent request must have agentName and userRequest''\n              }\n            });\n          }\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const userId = (req as any).user?.id || 'anonymous';';\n\n        // Prepare contexts for parallel execution\n        const parallelRequests = agentRequests.map((request: any) => ({,;\n          agentName: request.agentName,\n          context: {,\n            userRequest: request.userRequest,\n            requestId: `${requestId}_${request.agentName}`,\n            workingDirectory: process.cwd(),\n            userId,\n            ...request.context\n          }\n        }));\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.processParallelRequests(parallelRequests);\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            results,\n            summary: {,\n              total: results.length,\n              successful: results.filter(r => !r.error).length,\n              failed: results.filter(r => r.error).length,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'parallel''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Parallel agent execution error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'PARALLEL_EXECUTION_ERROR','\n            message: 'Parallel agent execution failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    // Agent orchestration\n    this.app.post('/api/v1/agents/orchestrate', async (req, res) => {'\n      try {\n        if (!this.agentRegistry) {\n          return res.status(503).json({);\n            success: false,\n            error: {,\n              code: 'SERVICE_UNAVAILABLE','\n              message: 'Agent registry not available''\n            }\n          });\n        }\n\n        const { primaryAgent, supportingAgents = [], userRequest, context = {} } = req.body;\n\n        if (!primaryAgent || !userRequest) {\n          return res.status(400).json({);\n            success: false,\n            error: {,\n              code: 'MISSING_REQUIRED_FIELD','\n              message: 'Primary agent and user request are required''\n            }\n          });\n        }\n\n        const requestId = req.headers['x-request-id'] as string || `req_${Date.now()}`;';\n        const orchestrationContext = {\n          userRequest,\n          requestId,\n          workingDirectory: process.cwd(),\n          userId: (req as any).user?.id || 'anonymous','\n          ...context\n        };\n\n        const startTime = Date.now();\n        const results = await this.agentRegistry.orchestrateAgents();\n          primaryAgent,\n          supportingAgents,\n          orchestrationContext\n        );\n        const executionTime = Date.now() - startTime;\n\n        res.json({)\n          success: true,\n          data: {\n            ...results,\n            summary: {\n              primaryAgent,\n              supportingAgents: supportingAgents.length,\n              synthesized: !!results.synthesis,\n              executionTime: `${executionTime}ms`\n            }\n          },\n          metadata: {,\n            timestamp: new Date().toISOString(),\n            requestId,\n            executionMode: 'orchestrated''\n          }\n        });\n        return;\n\n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message: String(error);\n        log.error('Agent orchestration error', LogContext.API, {')\n          error: errorMessage\n        });\n\n        res.status(500).json({)\n          success: false,\n          error: {,\n            code: 'ORCHESTRATION_ERROR','\n            message: 'Agent orchestration failed','\n            details: errorMessage\n          }\n        });\n        return;\n      }\n    });\n\n    log.info('✅ Agent routes setup completed', LogContext.SERVER);'\n  }\n\n  private setupWebSocket(): void {\n    try {\n      this.io = new SocketIOServer(this.server, {)\n        cors: {,\n          origin: process.env.FRONTEND_URL || 'http://localhost:3000','\n          methods: ['GET', 'POST']'\n        }\n      });\n\n      this.io.on('connection', (socket) => {'\n        log.info(`WebSocket client connected: ${socket.id}`, LogContext.WEBSOCKET);\n\n        socket.on('disconnect', () => {'\n          log.info(`WebSocket client disconnected: ${socket.id}`, LogContext.WEBSOCKET);\n        });\n\n        // Basic ping-pong for connection testing\n        socket.on('ping', () => {'\n          socket.emit('pong', { timestamp: new Date().toISOString() });'\n        });\n      });\n\n      log.info('✅ WebSocket server initialized', LogContext.WEBSOCKET);'\n    } catch (error) {\n      log.error('❌ Failed to initialize WebSocket server', LogContext.WEBSOCKET, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n  }\n\n  private setupErrorHandling(): void {\n    // 404 handler\n    this.app.use((req, res) => {\n      res.status(404).json({)\n        success: false,\n        error: {,\n          code: 'NOT_FOUND','\n          message: `Path ${req.path} not found`,\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          path: req.path,\n          method: req.method\n        }\n      });\n    });\n\n    // Global error handler\n    this.app.use((error: any, req: any, res: any, next: any) => {\n      const statusCode = error.status || error.statusCode || 500;\n      const message = error.message || 'Internal server error';';\n\n      log.error('Unhandled server error', LogContext.SERVER, {')\n        error: message,\n        stack: error.stack,\n        path: req.path,\n        method: req.method\n      });\n\n      res.status(statusCode).json({)\n        success: false,\n        error: {,\n          code: 'INTERNAL_SERVER_ERROR','\n          message: config.environment === 'development' ? message : 'Something went wrong''\n        },\n        metadata: {,\n          timestamp: new Date().toISOString(),\n          requestId: req.id\n        }\n      });\n    });\n\n    // Process error handlers\n    process.on('uncaughtException', (error) => {'\n      log.error('Uncaught Exception', LogContext.SYSTEM, {')\n        error: error.message,\n        stack: error.stack\n      });\n      this.gracefulShutdown('uncaughtException');'\n    });\n\n    process.on('unhandledRejection', (reason, promise) => {'\n      log.error('Unhandled Rejection', LogContext.SYSTEM, {')\n        reason: reason instanceof Error ? reason.message : String(reason),\n        promise: String(promise)\n      });\n      this.gracefulShutdown('unhandledRejection');'\n    });\n\n    // Graceful shutdown handlers\n    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));'\n    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));'\n\n    log.info('✅ Error handling setup completed', LogContext.SERVER);'\n  }\n\n  private async gracefulShutdown(signal: string): Promise<void> {\n    if (this.isShuttingDown) {\n      return;\n    }\n\n    this.isShuttingDown = true;\n    log.info(`Received ${signal}, shutting down gracefully...`, LogContext.SYSTEM);\n\n    try {\n      // Close HTTP server\n      if (this.server) {\n        await new Promise<void>((resolve) => {\n          this.server.close(() => {\n            log.info('HTTP server closed', LogContext.SERVER);'\n            resolve();\n          });\n        });\n      }\n\n      // Close WebSocket server\n      if (this.io) {\n        this.io.close(() => {\n          log.info('WebSocket server closed', LogContext.WEBSOCKET);'\n        });\n      }\n\n      // Shutdown agent registry\n      if (this.agentRegistry) {\n        await this.agentRegistry.shutdown();\n      }\n\n      // Stop health monitor\n      try {\n        const { healthMonitor } = await import('./services/health-monitor');';\n        healthMonitor.stop();\n        log.info('Health monitor stopped', LogContext.SYSTEM);'\n      } catch (error) {\n        // Health monitor might not be loaded\n      }\n\n      // Close database connections would go here\n      // await this.supabase?.close?.();\n\n      log.info('Graceful shutdown completed', LogContext.SYSTEM);'\n      process.exit(0);\n    } catch (error) {\n      log.error('Error during shutdown', LogContext.SYSTEM, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public async start(): Promise<void> {\n    try {\n      // Validate configuration\n      validateConfig();\n      \n      // Get automated port configuration\n      const ports = await getPorts();\n      const port = ports.mainServer;\n\n      // Start server\n      await new Promise<void>((resolve, reject) => {\n        this.server.listen(port, () => {\n          log.info()\n            `🚀 Universal AI Tools Service running on port ${port}`,\n            LogContext.SERVER,\n            {\n              environment: config.environment,\n              port: port,\n              healthCheck: `http://localhost:${port}/health`\n            }\n          );\n          resolve();\n        }).on('error', reject);'\n      });\n\n    } catch (error) {\n      log.error('❌ Failed to start server', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n      process.exit(1);\n    }\n  }\n\n  public getApp(): express.Application {\n    return this.app;\n  }\n\n  public getSupabase(): any {\n    return this.supabase;\n  }\n\n  private async loadAsyncRoutes(): Promise<void> {\n    try {\n      // Load monitoring routes\n      const monitoringModule = await import('./routers/monitoring');';\n      this.app.use('/api/v1/monitoring', monitoringModule.default);'\n      log.info('✅ Monitoring routes loaded', LogContext.SERVER);'\n      \n      // Start automated health monitoring\n      const { healthMonitor } = await import('./services/health-monitor');';\n      await healthMonitor.start();\n      log.info('✅ Health monitor service started', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ Monitoring routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load AB-MCTS routes\n    try {\n      const abMCTSModule = await import('./routers/ab-mcts-fixed');';\n      this.app.use('/api/v1/ab-mcts', abMCTSModule.default);'\n      log.info('✅ AB-MCTS orchestration endpoints loaded', LogContext.SERVER);'\n    } catch (error) {\n      log.error('❌ Failed to load AB-MCTS router', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load Vision routes\n    try {\n      const visionModule = await import('./routers/vision');';\n      this.app.use('/api/v1/vision', visionModule.default);'\n      log.info('✅ Vision routes loaded', LogContext.SERVER);'\n      \n      // Initialize PyVision service for embeddings\n      const { pyVisionBridge } = await import('./services/pyvision-bridge');';\n      log.info('✅ PyVision service initialized for embeddings', LogContext.AI);'\n    } catch (error) {\n      log.warn('⚠️ Vision routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load HuggingFace routes (now routed through LM Studio)\n    try {\n      const huggingFaceModule = await import('./routers/huggingface');';\n      this.app.use('/api/v1/huggingface', huggingFaceModule.default);'\n      log.info('✅ HuggingFace routes loaded (using LM Studio adapter)', LogContext.SERVER);'\n    } catch (error) {\n      log.warn('⚠️ HuggingFace routes failed to load', LogContext.SERVER, {')\n        error: error instanceof Error ? error.message : String(error)\n      });\n    }\n\n    // Load MLX routes - Apple Silicon ML framework\n    try {\n      const mlxModule = await import('./routers/mlx');';\n      this.app.use('/api/v1/mlx', mlxModule.default);'\n      log.info('✅ MLX routes loaded for Apple Silicon ML', LogContext.SERVER);'\n      \n      // Initialize MLX service and check platform compatibility\n      const { mlxService } = await import('./services/mlx-service');';\n      \n      // Check if running on Apple Silicon\n      const isAppleSilicon = process.arch === 'arm64' && process.platform === 'darwin';';\n      if (!isAppleSilicon) {\n        log.warn('⚠️ MLX is optimized for Apple Silicon but running on different platform', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n      \n      const healthCheck = await mlxService.healthCheck();\n      if (healthCheck.healthy) {\n        log.info('✅ MLX service initialized successfully', LogContext.AI, {')\n          platform: process.platform,\n          arch: process.arch,\n          optimized: isAppleSilicon\n        });\n      } else {\n        log.warn('⚠️ MLX service loaded but health check failed', LogContext.AI, {')\n          error: healthCheck.error(|| 'Unknown health check failure',')\n          platform: process.platform,\n          arch: process.arch\n        });\n      }\n    } catch (error) {\n      const errorMessage = error instanceof Error ? error.message: String(error);\n      log.error('❌ Failed to load MLX routes', LogContext.SERVER, {')\n        error: errorMessage,\n        platform: process.platform,\n        arch: process.arch,\n        suggestion: 'MLX requires Apple Silicon hardware and proper Python environment''\n      });\n      \n      // Don't fail server startup if MLX is unavailable'\n      log.info('🔄 Server continuing without MLX capabilities', LogContext.SERVER);'\n    }\n\n    // Load other async routes here as needed\n  }\n}\n\n// Start the server if this file is run directly\nif (import.meta.url === `file: //${process.argv[1]}`) {\n  const server = new UniversalAIToolsServer();\n  server.start();\n}\n\nexport default UniversalAIToolsServer;\nexport { UniversalAIToolsServer };"
      }
    ],
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "template_literal_backslash",
        "count": 3,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API\n */\n\nimport { log, LogContext } from '../utils/logger';\nimport { ModelConfig } from '../config/models';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string; content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    message?: { role: string; content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{ role: 'user', content: request.inputs }],\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:\\n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.\\n\\nText: \"${text}\"\\n\\nSentiment:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.\\n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '',\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy',\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy',\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' \n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable',\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API\n */\n\nimport { log, LogContext } from '../utils/logger';\nimport { ModelConfig } from '../config/models';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string; content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    message?: { role: string; content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{ role: 'user', content: request.inputs }],\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '',\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy',\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy',\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' \n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable',\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "missing_comma_object_literal",
        "count": 19,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API\n */\n\nimport { log, LogContext } from '../utils/logger';\nimport { ModelConfig } from '../config/models';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string; content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    message?: { role: string; content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{ role: 'user', content: request.inputs }],\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{ role: 'user', content: prompt }],\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '',\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy',\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy',\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' \n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable',\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API\n */\n\nimport { log, LogContext } from '../utils/logger';\nimport { ModelConfig } from '../config/models';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({,\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '',\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy',\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy',\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' \n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable',\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "unterminated_string_single",
        "count": 41,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API\n */\n\nimport { log, LogContext } from '../utils/logger';\nimport { ModelConfig } from '../config/models';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({,\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || ''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '',\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy',\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy',\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio'\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET',\n        headers: { 'Content-Type': 'application/json' }\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' \n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable',\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport { ModelConfig } from '../config/models';'\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify({,\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();'\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { 'Content-Type': 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { 'Content-Type': 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "unterminated_string_double",
        "count": 1,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport { ModelConfig } from '../config/models';'\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify({,\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();'\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { 'Content-Type': 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { 'Content-Type': 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport { ModelConfig } from '../config/models';'\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify({,\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\"\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();'\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { 'Content-Type': 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { 'Content-Type': 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "content_type_header",
        "count": 7,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport { ModelConfig } from '../config/models';'\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify({,\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\"\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();'\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { 'Content-Type': 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { 'Content-Type': 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { 'Content-Type': 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport { ModelConfig } from '../config/models';'\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\"\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();'\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "missing_closing_parenthesis",
        "count": 13,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport { ModelConfig } from '../config/models';'\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\"\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();'\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info(\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport { ModelConfig } from '../config/models';'\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\"\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();'\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {)\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {)\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "semicolon_missing_statements",
        "count": 15,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';'\nimport { ModelConfig } from '../config/models';'\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\"\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();'\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {)\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {)\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {)\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';';\nimport { ModelConfig } from '../config/models';';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\";\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();';\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "type_annotation_spacing",
        "count": 9,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';';\nimport { ModelConfig } from '../config/models';';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\";\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();';\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';';\nimport { ModelConfig } from '../config/models';';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\";\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();';\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "import_spacing",
        "count": 2,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext } from '../utils/logger';';\nimport { ModelConfig } from '../config/models';';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\";\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();';\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext  } from '../utils/logger';';\nimport { ModelConfig  } from '../config/models';';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\";\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();';\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/huggingface-to-lmstudio.ts",
        "pattern": "object_property_spacing",
        "count": 122,
        "originalContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext  } from '../utils/logger';';\nimport { ModelConfig  } from '../config/models';';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs : [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely:n\\n${request.inputs}\\n\\nSummary:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment:`;\";\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();';\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer:`;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);",
        "fixedContent": "/**\n * HuggingFace to LM Studio Adapter\n * Routes HuggingFace-style requests to LM Studio's OpenAI-compatible API'\n */\n\nimport { log, LogContext  } from '../utils/logger';';\nimport { ModelConfig  } from '../config/models';';\n\ninterface LMStudioRequest {\n  model: string;\n  messages?: Array<{ role: string;, content: string }>;\n  prompt?: string;\n  temperature?: number;\n  max_tokens?: number;\n  stream?: boolean;\n}\n\ninterface LMStudioResponse {\n  id: string;,\n  object: string;\n  created: number;,\n  model: string;\n  choices: Array<{,\n    index: number;\n    message?: { role: string;, content: string };\n    text?: string;\n    finish_reason: string;\n  }>;\n  usage?: {\n    prompt_tokens: number;,\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class HuggingFaceToLMStudioAdapter {\n  private baseUrl: string;\n  \n  constructor() {\n    this.baseUrl = ModelConfig.lmStudio.url;\n  }\n  \n  /**\n   * Text generation - maps to LM Studio chat completions\n   */\n  async generateText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_new_tokens?: number;\n      temperature?: number;\n      top_p?: number;\n      do_sample?: boolean;\n    };\n    model?: string;\n  }): Promise<any> {\n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: request.inputs }],'\n      temperature: request.parameters?.temperature || 0.7,\n      max_tokens: request.parameters?.max_new_tokens || 500\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        generated_text: data.choices[0]?.message?.content || data.choices[0]?.text || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio text generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Embeddings - maps to LM Studio embeddings endpoint\n   */\n  async generateEmbeddings(request: {,)\n    inputs: string | string[];\n    model?: string;\n  }): Promise<any> {\n    const input = Array.isArray(request.inputs) ? request.inputs: [request.inputs];\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/embeddings`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify({,)\n          model: request.model || ModelConfig.lmStudio.models.embedding,\n          input: input\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      // Convert to HuggingFace format\n      return data.data.map((item: any) => item.embedding);\n      \n    } catch (error) {\n      log.error('LM Studio embedding generation failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Summarization - uses chat completion with specific prompt\n   */\n  async summarizeText(request: {,)\n    inputs: string;\n    parameters?: {\n      max_length?: number;\n      min_length?: number;\n      temperature?: number;\n    };\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Please summarize the following text concisely: n\\n${request.inputs}\\n\\nSummary: `;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.summarization,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: request.parameters?.temperature || 0.5,\n      max_tokens: request.parameters?.max_length || 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return [{\n        summary_text: data.choices[0]?.message?.content || '''\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio summarization failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Sentiment analysis - uses chat completion with specific prompt\n   */\n  async analyzeSentiment(text: string, model?: string): Promise<any> {\n    const prompt = `Analyze the sentiment of the following text and respond with only one word: POSITIVE, NEGATIVE, or NEUTRAL.n\\nText: \"${text}\"\\n\\nSentiment: `;\";\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: model || ModelConfig.lmStudio.models.sentiment,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.1,\n      max_tokens: 10\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      const sentiment = (data.choices[0]?.message?.content || '').trim().toUpperCase();';\n      \n      // Convert to HuggingFace format with scores\n      const scores = {\n        POSITIVE: sentiment === 'POSITIVE' ? 0.9 : 0.05,'\n        NEGATIVE: sentiment === 'NEGATIVE' ? 0.9 : 0.05,'\n        NEUTRAL: sentiment === 'NEUTRAL' ? 0.9 : 0.05'\n      };\n      \n      return [{\n        label: sentiment,\n        score: scores[sentiment as keyof typeof scores] || 0.33\n      }];\n      \n    } catch (error) {\n      log.error('LM Studio sentiment analysis failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Question answering - uses chat completion with context\n   */\n  async answerQuestion(request: {,)\n    question: string;\n    context: string;\n    model?: string;\n  }): Promise<any> {\n    const prompt = `Based on the following context, answer the question.n\\nContext: ${request.context}\\n\\nQuestion: ${request.question}\\n\\nAnswer: `;\n    \n    const lmStudioRequest: LMStudioRequest = {,;\n      model: request.model || ModelConfig.lmStudio.models.textGeneration,\n      messages: [{, role: 'user', content: prompt }],'\n      temperature: 0.3,\n      max_tokens: 200\n    };\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {);\n        method: 'POST','\n        headers: { \"content-type\": 'application/json' },'\n        body: JSON.stringify(lmStudioRequest)\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio error: ${response.statusText}`);\n      }\n      \n      const data: LMStudioResponse = await response.json();\n      \n      // Convert to HuggingFace format\n      return {\n        answer: data.choices[0]?.message?.content || '','\n        score: 0.85, // Confidence score\n        start: 0,\n        end: request.context.length\n      };\n      \n    } catch (error) {\n      log.error('LM Studio question answering failed', LogContext.AI, { error });'\n      throw error;\n    }\n  }\n  \n  /**\n   * Check if LM Studio is available\n   */\n  async healthCheck(): Promise<any> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`LM Studio not available: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      \n      return {\n        status: 'healthy','\n        models: data.data?.map((m: any) => m.id) || [],\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n      \n    } catch (error) {\n      return {\n        status: 'unhealthy','\n        error: error instanceof Error ? error.message : String(error),\n        baseUrl: this.baseUrl,\n        adapter: 'huggingface-to-lmstudio''\n      };\n    }\n  }\n  \n  /**\n   * Get metrics (mock implementation)\n   */\n  getMetrics() {\n    return {\n      totalRequests: 0,\n      successfulRequests: 0,\n      failedRequests: 0,\n      averageResponseTime: 100,\n      modelsUsed: new Set(['lm-studio']),'\n      lastRequestTime: Date.now()\n    };\n  }\n  \n  /**\n   * List available models from LM Studio\n   */\n  async listModels(): Promise<string[]> {\n    try {\n      const response = await fetch(`${this.baseUrl}/v1/models`, {);\n        method: 'GET','\n        headers: { \"content-type\": 'application/json' }'\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Failed to list models: ${response.statusText}`);\n      }\n      \n      const data = await response.json();\n      return data.data?.map((m: any) => m.id) || [];\n      \n    } catch (error) {\n      log.error('Failed to list LM Studio models', LogContext.AI, { error });'\n      return [];\n    }\n  }\n}\n\n// Export singleton instance that replaces HuggingFace service\nexport const huggingFaceService = ModelConfig.lmStudio.enabled \n  ? new HuggingFaceToLMStudioAdapter()\n  : null;\n\nlog.info()\n  huggingFaceService \n    ? '✅ HuggingFace requests will be routed to LM Studio' '\n    : '⚠️ LM Studio disabled, HuggingFace functionality unavailable','\n  LogContext.AI\n);"
      }
    ],
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "template_literal_backslash",
        "count": 2,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';\nimport { log, LogContext } from '@/utils/logger';\nimport { ollamaService } from '@/services/ollama-service';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';\n}\n\nexport interface LFM2Response {\n  content: string;\n  tokens: number;\n  executionTime: number;\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;\n  totalRequests: number;\n  successRate: number;\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{\n    id: string;\n    request: LFM2Request;\n    resolve: (response: LFM2Response) => void;\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;\n    reject: (error: Error) => void;\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';\n    confidence: number;\n    reasoning: string;\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing'\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {\n      primary_priority: number;\n      supporting_priorities: number[];\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination'\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion',\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + '\\n');\n      } else {\n        reject(new Error('Python process not available'));\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b',\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER REQUEST: \"${userRequest}\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n- lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.',\n      simple_qa: 'Answer this question quickly and concisely.'\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY TASK: \"${primaryTask}\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\n    \"primary_priority\": 1-5,\n    \"supporting_priorities\": [1-5, ...],\n    \"parallel_execution\": boolean,\n    \"estimated_total_time\": seconds\n  },\n  \"resource_allocation\": {\n    \"primary_service\": \"service_name\",\n    \"supporting_services\": [\"service1\", \"service2\", ...]\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama',\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision',\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error',\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {\n        primary_service: 'ollama',\n        supporting_services: ['lfm2', 'ollama']\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B',\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),\n          { role: 'user' as const, content: request.prompt }\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b',\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)',\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback',\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response',\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';\nimport { log, LogContext } from '@/utils/logger';\nimport { ollamaService } from '@/services/ollama-service';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';\n}\n\nexport interface LFM2Response {\n  content: string;\n  tokens: number;\n  executionTime: number;\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;\n  totalRequests: number;\n  successRate: number;\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{\n    id: string;\n    request: LFM2Request;\n    resolve: (response: LFM2Response) => void;\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;\n    reject: (error: Error) => void;\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';\n    confidence: number;\n    reasoning: string;\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing'\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {\n      primary_priority: number;\n      supporting_priorities: number[];\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination'\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion',\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');\n      } else {\n        reject(new Error('Python process not available'));\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b',\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER REQUEST: \"${userRequest}\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n- lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.',\n      simple_qa: 'Answer this question quickly and concisely.'\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY TASK: \"${primaryTask}\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\n    \"primary_priority\": 1-5,\n    \"supporting_priorities\": [1-5, ...],\n    \"parallel_execution\": boolean,\n    \"estimated_total_time\": seconds\n  },\n  \"resource_allocation\": {\n    \"primary_service\": \"service_name\",\n    \"supporting_services\": [\"service1\", \"service2\", ...]\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama',\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision',\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error',\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {\n        primary_service: 'ollama',\n        supporting_services: ['lfm2', 'ollama']\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B',\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),\n          { role: 'user' as const, content: request.prompt }\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b',\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)',\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback',\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response',\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "missing_comma_object_literal",
        "count": 21,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';\nimport { log, LogContext } from '@/utils/logger';\nimport { ollamaService } from '@/services/ollama-service';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';\n}\n\nexport interface LFM2Response {\n  content: string;\n  tokens: number;\n  executionTime: number;\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;\n  totalRequests: number;\n  successRate: number;\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{\n    id: string;\n    request: LFM2Request;\n    resolve: (response: LFM2Response) => void;\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;\n    reject: (error: Error) => void;\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';\n    confidence: number;\n    reasoning: string;\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing'\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {\n      primary_priority: number;\n      supporting_priorities: number[];\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination'\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion',\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');\n      } else {\n        reject(new Error('Python process not available'));\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b',\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER REQUEST: \"${userRequest}\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n- lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.',\n      simple_qa: 'Answer this question quickly and concisely.'\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY TASK: \"${primaryTask}\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\n    \"primary_priority\": 1-5,\n    \"supporting_priorities\": [1-5, ...],\n    \"parallel_execution\": boolean,\n    \"estimated_total_time\": seconds\n  },\n  \"resource_allocation\": {\n    \"primary_service\": \"service_name\",\n    \"supporting_services\": [\"service1\", \"service2\", ...]\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama',\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision',\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error',\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {\n        primary_service: 'ollama',\n        supporting_services: ['lfm2', 'ollama']\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B',\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),\n          { role: 'user' as const, content: request.prompt }\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b',\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)',\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback',\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response',\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';\nimport { log, LogContext } from '@/utils/logger';\nimport { ollamaService } from '@/services/ollama-service';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing'\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination'\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion',\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');\n      } else {\n        reject(new Error('Python process not available'));\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b',\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.',\n      simple_qa: 'Answer this question quickly and concisely.'\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\n    \"primary_priority\": 1-5,\n    \"supporting_priorities\": [1-5, ...],\n    \"parallel_execution\": boolean,\n    \"estimated_total_time\": seconds\n  },\n  \"resource_allocation\": {\n    \"primary_service\": \"service_name\",\n    \"supporting_services\": [\"service1\", \"service2\", ...]\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama',\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision',\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error',\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama',\n        supporting_services: ['lfm2', 'ollama']\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B',\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),\n          { role: 'user' as const, content: request.prompt }\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b',\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)',\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback',\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response',\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "unterminated_string_single",
        "count": 64,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';\nimport { log, LogContext } from '@/utils/logger';\nimport { ollamaService } from '@/services/ollama-service';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing'\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination'\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion',\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');\n      } else {\n        reject(new Error('Python process not available'));\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b',\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.',\n      simple_qa: 'Answer this question quickly and concisely.'\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\n    \"primary_priority\": 1-5,\n    \"supporting_priorities\": [1-5, ...],\n    \"parallel_execution\": boolean,\n    \"estimated_total_time\": seconds\n  },\n  \"resource_allocation\": {\n    \"primary_service\": \"service_name\",\n    \"supporting_services\": [\"service1\", \"service2\", ...]\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama',\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision',\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error',\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama',\n        supporting_services: ['lfm2', 'ollama']\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B',\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa'\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),\n          { role: 'user' as const, content: request.prompt }\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b',\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)',\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback',\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response',\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { ollamaService } from '@/services/ollama-service';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';'\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');'\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);'\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');'\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';'\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\n    \"primary_priority\": 1-5,\n    \"supporting_priorities\": [1-5, ...],\n    \"parallel_execution\": boolean,\n    \"estimated_total_time\": seconds\n  },\n  \"resource_allocation\": {\n    \"primary_service\": \"service_name\",\n    \"supporting_services\": [\"service1\", \"service2\", ...]\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {'\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "unterminated_string_double",
        "count": 14,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { ollamaService } from '@/services/ollama-service';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';'\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');'\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);'\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');'\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';'\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\n    \"primary_priority\": 1-5,\n    \"supporting_priorities\": [1-5, ...],\n    \"parallel_execution\": boolean,\n    \"estimated_total_time\": seconds\n  },\n  \"resource_allocation\": {\n    \"primary_service\": \"service_name\",\n    \"supporting_services\": [\"service1\", \"service2\", ...]\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {'\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { ollamaService } from '@/services/ollama-service';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';'\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');'\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);'\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');'\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';'\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {'\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "missing_parentheses_logger",
        "count": 1,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { ollamaService } from '@/services/ollama-service';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';'\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');'\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);'\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');'\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';'\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error || 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {'\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { ollamaService } from '@/services/ollama-service';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';'\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');'\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);'\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');'\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';'\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {'\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "missing_closing_parenthesis",
        "count": 13,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { ollamaService } from '@/services/ollama-service';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';'\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');'\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);'\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision(\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents(\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all(\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');'\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';'\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse(\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute(\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {'\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat(\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { ollamaService } from '@/services/ollama-service';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';'\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');'\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);'\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({)\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({)\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({)\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all()\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');'\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';'\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute()\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat()\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "semicolon_missing_statements",
        "count": 18,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';'\nimport { log, LogContext } from '@/utils/logger';'\nimport { ollamaService } from '@/services/ollama-service';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';'\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');'\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);'\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({)\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({)\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({)\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all()\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');'\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';'\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute()\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat()\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';';\nimport { log, LogContext } from '@/utils/logger';';\nimport { ollamaService } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "type_annotation_spacing",
        "count": 15,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';';\nimport { log, LogContext } from '@/utils/logger';';\nimport { ollamaService } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';';\nimport { log, LogContext } from '@/utils/logger';';\nimport { ollamaService } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "import_spacing",
        "count": 4,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess } from 'child_process';';\nimport { log, LogContext } from '@/utils/logger';';\nimport { ollamaService } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { ollamaService  } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "object_property_spacing",
        "count": 163,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { ollamaService  } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION:;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS:\n-, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON:\n{\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse:`;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN:;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON:\n{\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2:3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { ollamaService  } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION: ;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS: -, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON: {\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse: `;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN: ;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON: {\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2: 3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-bridge.ts",
        "pattern": "arrow_function_spacing",
        "count": 10,
        "originalContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { ollamaService  } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION: ;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS: -, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON: {\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse: `;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN: ;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON: {\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2: 3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;",
        "fixedContent": "/**\n * LFM2 Bridge Service - Fast Local Model Integration\n * Bridges TypeScript services with Python LFM2-1.2B model\n * Optimized for speed and coordination tasks\n */\n\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { log, LogContext  } from '@/utils/logger';';\nimport { ollamaService  } from '@/services/ollama-service';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '@/utils/circuit-breaker';';\n\nexport interface LFM2Request {\n  prompt: string;\n  systemPrompt?: string;\n  maxLength?: number;\n  maxTokens?: number;\n  temperature?: number;\n  taskType: 'routing' | 'coordination' | 'simple_qa' | 'classification';'\n}\n\nexport interface LFM2Response {\n  content: string;,\n  tokens: number;\n  executionTime: number;,\n  model: string; // Allow different model names for fallback\n  confidence?: number;\n}\n\nexport interface LFM2Metrics {\n  avgResponseTime: number;,\n  totalRequests: number;\n  successRate: number;,\n  tokenThroughput: number;\n}\n\nexport class LFM2BridgeService {\n  private pythonProcess: ChildProcess | null = null;\n  private isInitialized: boolean = false;\n  private requestQueue: Array<{,\n    id: string;\n    request: LFM2Request;,\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;\n  }> = [];\n  private pendingRequests: Map<string, {\n    resolve: (response: LFM2Response) => void;,\n    reject: (error: Error) => void;,\n    startTime: number;\n  }> = new Map();\n  private metrics: LFM2Metrics = {,\n    avgResponseTime: 0,\n    totalRequests: 0,\n    successRate: 1.0,\n    tokenThroughput: 0\n  };\n\n  constructor() {\n    this.initializeLFM2();\n  }\n\n  private async initializeLFM2(): Promise<void> {\n    try {\n      log.info('🚀 Initializing LFM2-1.2B bridge service', LogContext.AI);'\n\n      // Create Python bridge server\n      const pythonScript = `/Users/christianmerrill/Desktop/universal-ai-tools/src/services/lfm2-server.py`;\n      \n      this.pythonProcess = spawn('python3', [pythonScript], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        env: { ...process.env, PYTHONPATH: '/Users/christianmerrill/Desktop/universal-ai-tools' }'\n      });\n\n      if (!this.pythonProcess.stdout || !this.pythonProcess.stderr || !this.pythonProcess.stdin) {\n        throw new Error('Failed to create Python process stdio');';\n      }\n\n      // Handle responses\n      this.pythonProcess.stdout.on('data', (data) => {'\n        this.handlePythonResponse(data.toString());\n      });\n\n      // Handle errors\n      this.pythonProcess.stderr.on('data', (data) => {'\n        log.error('❌ LFM2 Python error', LogContext.AI, { error: data.toString() });'\n      });\n\n      // Handle process exit\n      this.pythonProcess.on('exit', (code) => {'\n        log.warn(`⚠️ LFM2 Python process exited with code ${code}`, LogContext.AI);\n        this.isInitialized = false;\n        this.restartProcess();\n      });\n\n      // Wait for initialization\n      await new Promise((resolve, reject) => {\n        const timeout = setTimeout(() => reject(new Error('LFM2 initialization timeout')), 30000);';\n        \n        const checkInit = () => {\n          if (this.isInitialized) {\n            clearTimeout(timeout);\n            resolve(true);\n          } else {\n            setTimeout(checkInit, 100);\n          }\n        };\n        checkInit();\n      });\n\n      log.info('✅ LFM2-1.2B bridge service initialized', LogContext.AI);'\n    } catch (error) {\n      log.error('❌ Failed to initialize LFM2 bridge service', LogContext.AI, { error });'\n      // Fall back to mock implementation\n      this.initializeMockLFM2();\n    }\n  }\n\n  private initializeMockLFM2(): void {\n    log.warn('⚠️ Using mock LFM2 implementation for testing', LogContext.AI);'\n    this.isInitialized = true;\n  }\n\n  /**\n   * Fast routing decision using LFM2\n   */\n  public async routingDecision()\n    userRequest: string,\n    context: Record<string, any>\n  ): Promise<{\n    targetService: 'lfm2' | 'ollama' | 'lm-studio' | 'openai' | 'anthropic';,'\n    confidence: number;\n    reasoning: string;,\n    estimatedTokens: number;\n  }> {\n    const prompt = this.createRoutingPrompt(userRequest, context);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 200,\n      temperature: 0.3,\n      taskType: 'routing''\n    });\n\n    // Parse LFM2 routing response\n    return this.parseRoutingResponse(response.content);\n  }\n\n  /**\n   * Quick classification and simple Q&A\n   */\n  public async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    const prompt = this.createQuickResponsePrompt(userRequest, taskType);\n    \n    return this.generate({);\n      prompt,\n      maxLength: 150,\n      temperature: 0.6,\n      taskType\n    });\n  }\n\n  /**\n   * Coordinate multiple agent tasks\n   */\n  public async coordinateAgents()\n    primaryTask: string,\n    supportingTasks: string[]\n  ): Promise<{\n    execution_plan: {,\n      primary_priority: number;\n      supporting_priorities: number[];,\n      parallel_execution: boolean;\n      estimated_total_time: number;\n    };\n    resource_allocation: {,\n      primary_service: string;\n      supporting_services: string[];\n    };\n  }> {\n    const prompt = this.createCoordinationPrompt(primaryTask, supportingTasks);\n    \n    const response = await this.generate({);\n      prompt,\n      maxLength: 300,\n      temperature: 0.4,\n      taskType: 'coordination''\n    });\n\n    return this.parseCoordinationResponse(response.content);\n  }\n\n  /**\n   * Core generation method\n   */\n  public async generate(request: LFM2Request): Promise<LFM2Response> {\n    if (!this.isInitialized) {\n      return this.generateMockResponse(request);\n    }\n\n    const requestId = this.generateRequestId();\n    const startTime = Date.now();\n\n    return new Promise((resolve, reject) => {\n      this.pendingRequests.set(requestId, { resolve, reject, startTime });\n      \n      // Send request to Python process with correct format\n      const pythonRequest = {\n        type: request.taskType === 'routing' || request.taskType === 'coordination' ? request.taskType : 'completion','\n        requestId: requestId,\n        prompt: request.prompt,\n        maxTokens: request.maxTokens || request.maxLength || 512,\n        temperature: request.temperature || 0.7\n      };\n\n      if (this.pythonProcess && this.pythonProcess.stdin) {\n        this.pythonProcess.stdin.write(JSON.stringify(pythonRequest) + 'n');'\n      } else {\n        reject(new Error('Python process not available'));'\n      }\n\n      // Timeout after 10 seconds\n      setTimeout(() => {\n        if (this.pendingRequests.has(requestId)) {\n          this.pendingRequests.delete(requestId);\n          reject(new Error('LFM2 request timeout'));'\n        }\n      }, 10000);\n    });\n  }\n\n  /**\n   * Batch processing for efficiency\n   */\n  public async generateBatch(requests: LFM2Request[]): Promise<LFM2Response[]> {\n    log.info('📦 Processing LFM2 batch request', LogContext.AI, { count: requests.length });'\n    \n    // Process requests in parallel but limit concurrency\n    const batchSize = 3;\n    const results: LFM2Response[] = [];\n    \n    for (let i = 0; i < requests.length; i += batchSize) {\n      const batch = requests.slice(i, i + batchSize);\n      const batchResults = await Promise.all();\n        batch.map(request => this.generate(request))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  private handlePythonResponse(data: string): void {\n    const lines = data.trim().split('\\n');';\n    \n    for (const line of lines) {\n      if (line === 'INITIALIZED') {'\n        this.isInitialized = true;\n        continue;\n      }\n\n      try {\n        const response = JSON.parse(line);\n        \n        // Handle Python server response format\n        if (response.requestId && this.pendingRequests.has(response.requestId)) {\n          const { resolve, reject, startTime } = this.pendingRequests.get(response.requestId)!;\n          this.pendingRequests.delete(response.requestId);\n          \n          const executionTime = response.processingTime || (Date.now() - startTime);\n          \n          if (response.success) {\n            const content = response.text || response.strategy || response.category || '';';\n            \n            // Update metrics\n            this.updateMetrics(executionTime, content.length);\n            \n            resolve({)\n              content: content,\n              tokens: Math.ceil(content.length / 4),\n              executionTime,\n              model: response.model || 'lfm2-1.2b','\n              confidence: response.confidence\n            });\n          } else {\n            reject(new Error(response.error(|| 'LFM2 processing failed'));'\n          }\n        }\n      } catch (error) {\n        log.error('❌ Failed to parse LFM2 response', LogContext.AI, { error, data: line });'\n      }\n    }\n  }\n\n  private createRoutingPrompt(userRequest: string, context: Record<string, any>): string {\n    return `FAST ROUTING DECISION: ;\n\nUSER, REQUEST: \"${userRequest}\"\"\nCONTEXT: ${JSON.stringify(context)}\n\nROUTING OPTIONS: -, lfm2: Simple questions, quick responses (<100 tokens)\n- ollama: Medium complexity, general purpose (<1000 tokens)\n- lm-studio: Code generation, technical tasks (<2000 tokens)\n- openai: Complex reasoning, creative tasks (>1000 tokens)\n- anthropic: Analysis, research, long-form content\n\nRespond with JSON: {\"service\": \"...\", \"confidence\": 0.0-1.0, \"reasoning\": \"...\", \"tokens\": number}`;\"\n  }\n\n  private createQuickResponsePrompt(userRequest: string, taskType: string): string {\n    const taskInstructions = {\n      classification: 'Classify this request into categories and respond briefly.','\n      simple_qa: 'Answer this question quickly and concisely.''\n    };\n\n    return `${taskInstructions[taskType as keyof typeof taskInstructions]}\n\nREQUEST: \"${userRequest}\"\"\n\nResponse: `;\n  }\n\n  private createCoordinationPrompt(primaryTask: string, supportingTasks: string[]): string {\n    return `AGENT COORDINATION PLAN: ;\n\nPRIMARY, TASK: \"${primaryTask}\"\"\nSUPPORTING TASKS: ${supportingTasks.map((task, i) => `${i+1}. \"${task}\"`).join(', ')}'\"\n\nCreate execution plan with priorities, resource allocation, and timing.\n\nRespond with JSON: {\n  \"execution_plan\": {\"\n    \"primary_priority\": 1-5,\"\n    \"supporting_priorities\": [1-5, ...],\"\n    \"parallel_execution\": boolean,\"\n    \"estimated_total_time\": seconds\"\n  },\n  \"resource_allocation\": {\"\n    \"primary_service\": \"service_name\",\"\n    \"supporting_services\": [\"service1\", \"service2\", ...]\"\n  }\n}`;\n  }\n\n  private parseRoutingResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/{.*\\}/s);\n      if (jsonMatch) {\n        const parsed = JSON.parse(jsonMatch[0]);\n        return {\n          targetService: parsed.service || 'ollama','\n          confidence: parsed.confidence || 0.7,\n          reasoning: parsed.reasoning || 'Automatic routing decision','\n          estimatedTokens: parsed.tokens || 100\n        };\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 routing response', LogContext.AI);'\n    }\n\n    // Fallback parsing\n    return {\n      targetService: 'ollama' as const,'\n      confidence: 0.5,\n      reasoning: 'Fallback routing due to parsing error','\n      estimatedTokens: 100\n    };\n  }\n\n  private parseCoordinationResponse(content: string): any {\n    try {\n      const jsonMatch = content.match(/\\{.*\\}/s);\n      if (jsonMatch) {\n        return JSON.parse(jsonMatch[0]);\n      }\n    } catch (error) {\n      log.warn('⚠️ Failed to parse LFM2 coordination response', LogContext.AI);'\n    }\n\n    // Fallback coordination plan\n    return {\n      execution_plan: {,\n        primary_priority: 1,\n        supporting_priorities: [2, 3],\n        parallel_execution: true,\n        estimated_total_time: 30\n      },\n      resource_allocation: {,\n        primary_service: 'ollama','\n        supporting_services: ['lfm2', 'ollama']'\n      }\n    };\n  }\n\n  private generateMockResponse(request: LFM2Request): LFM2Response {\n    // Fast mock response for development/testing\n    const mockContent = `Mock LFM2 response for: ${request.prompt.substring(0, 50)}...`;\n    \n    return {\n      content: mockContent,\n      tokens: Math.ceil(mockContent.length / 4),\n      executionTime: 50 + Math.random() * 100, // 50-150ms\n      model: 'LFM2-1.2B','\n      confidence: 0.8\n    };\n  }\n\n  private generateRequestId(): string {\n    return `lfm2_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private updateMetrics(executionTime: number, responseLength: number): void {\n    this.metrics.totalRequests++;\n    \n    // Exponential moving average\n    const alpha = 0.1;\n    this.metrics.avgResponseTime = alpha * executionTime + (1 - alpha) * this.metrics.avgResponseTime;\n    \n    // Token throughput (tokens per second)\n    const tokens = Math.ceil(responseLength / 4);\n    const throughput = tokens / (executionTime / 1000);\n    this.metrics.tokenThroughput = alpha * throughput + (1 - alpha) * this.metrics.tokenThroughput;\n  }\n\n  private async restartProcess(): Promise<void> {\n    log.info('🔄 Restarting LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    // Wait a bit before restarting\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    this.initializeLFM2();\n  }\n\n  public getMetrics(): LFM2Metrics & { isInitialized: boolean } {\n    return {\n      ...this.metrics,\n      isInitialized: this.isInitialized\n    };\n  }\n\n  public isAvailable(): boolean {\n    return this.isInitialized && this.pythonProcess !== null;\n  }\n\n  public async shutdown(): Promise<void> {\n    log.info('🛑 Shutting down LFM2 bridge service', LogContext.AI);'\n    \n    if (this.pythonProcess) {\n      this.pythonProcess.kill();\n      this.pythonProcess = null;\n    }\n    \n    this.isInitialized = false;\n  }\n}\n\n// Create singleton with lazy initialization and circuit breaker protection\nclass SafeLFM2Bridge {\n  private instance: LFM2BridgeService | null = null;\n  private initAttempted = false;\n  private circuitBreaker: CircuitBreaker<LFM2Response>;\n\n  constructor() {\n    // Create circuit breaker with optimized settings\n    this.circuitBreaker = new CircuitBreaker<LFM2Response>('lfm2-bridge', {'\n      failureThreshold: 3,\n      successThreshold: 2,\n      timeout: 30000, // 30 seconds\n      errorThresholdPercentage: 60,\n      volumeThreshold: 5\n    });\n    \n    // Register for monitoring\n    CircuitBreakerRegistry.register('lfm2-bridge', this.circuitBreaker);'\n  }\n\n  async quickResponse()\n    userRequest: string,\n    taskType: 'classification' | 'simple_qa' = 'simple_qa''\n  ): Promise<LFM2Response> {\n    return this.circuitBreaker.execute(async () => {\n      if (!this.initAttempted && !this.instance) {\n        this.initAttempted = true;\n        try {\n          this.instance = new LFM2BridgeService();\n          log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n        } catch (error) {\n          log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, { error });'\n          return this.createFallbackResponse(userRequest);\n        }\n      }\n\n      if (this.instance) {\n        return this.instance.quickResponse(userRequest, taskType);\n      } else {\n        return this.createFallbackResponse(userRequest);\n      }\n    });\n  }\n\n  async execute(request: LFM2Request): Promise<LFM2Response> {\n    // Use circuit breaker for resilient execution\n    return this.circuitBreaker.execute();\n      async () => {\n        // Try to initialize LFM2 if not attempted\n        if (!this.initAttempted && !this.instance) {\n          this.initAttempted = true;\n          try {\n            this.instance = new LFM2BridgeService();\n            log.info('✅ LFM2 bridge initialized successfully', LogContext.AI);'\n          } catch (error) {\n            log.warn('⚠️ LFM2 bridge initialization failed, using fallback', LogContext.AI, {')\n              error: error instanceof Error ? error.message : String(error)\n            });\n          }\n        }\n\n        // Try native LFM2 if available\n        if (this.instance && this.instance.isAvailable()) {\n          return this.instance.generate(request);\n        }\n\n        // Use Ollama as primary fallback\n        const messages = [;\n          ...(request.systemPrompt ? [{ role: 'system' as const, content: request.systemPrompt }] : []),'\n          { role: 'user' as const, content: request.prompt }'\n        ];\n        \n        const response = await ollamaService.chat();\n          'llama3.2: 3b','\n          messages,\n          {\n            temperature: request.temperature || 0.1,\n            num_predict: request.maxTokens || 100\n          }\n        );\n\n        return {\n          content: response.message.content,\n          tokens: response.eval_count || 50,\n          executionTime: (response.total_duration || 100000000) / 1000000,\n          model: 'llama3.2:3b (LFM2 fallback)','\n          confidence: 0.85\n        };\n      },\n      // Fallback function when circuit is open\n      async () => {\n        log.warn('⚡ Circuit breaker active, using emergency fallback', LogContext.AI);'\n        return {\n          content: \"I'm currently experiencing high load. Please try again in a moment.\",'\"\n          tokens: 10,\n          executionTime: 1,\n          model: 'circuit-breaker-fallback','\n          confidence: 0.3\n        };\n      }\n    );\n  }\n\n  private createFallbackResponse(userRequest: string): LFM2Response {\n    return {\n      content: `I understand you're asking, about: ${userRequest.substring(0, 50)}... I'm currently experiencing connectivity issues but will help as soon as possible.`,'\n      tokens: 25,\n      executionTime: 1,\n      model: 'fallback-response','\n      confidence: 0.4\n    };\n  }\n\n  isAvailable(): boolean {\n    return this.instance?.isAvailable() || true; // Always available with fallback;\n  }\n\n  getMetrics() {\n    if (this.instance) {\n      return this.instance.getMetrics();\n    }\n    return {\n      avgResponseTime: 100,\n      totalRequests: 0,\n      successRate: 1.0,\n      tokenThroughput: 500\n    };\n  }\n\n  shutdown() {\n    if (this.instance) {\n      this.instance.shutdown();\n    }\n  }\n\n  // Get circuit breaker health status\n  getCircuitBreakerMetrics() {\n    return this.circuitBreaker.getMetrics();\n  }\n}\n\n// Export safe singleton that won't crash on startup'\nexport const lfm2Bridge = new SafeLFM2Bridge();\nexport default lfm2Bridge;"
      }
    ],
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "template_literal_backslash",
        "count": 3,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';\nimport { spawn, ChildProcess } from 'child_process';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';\nimport { join, dirname, basename, extname } from 'path';\nimport { v4 as uuidv4 } from 'uuid';\nimport { log, LogContext } from '../utils/logger';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';\nimport { mlxService } from './mlx-service';\nimport { createClient } from '@supabase/supabase-js';\nimport { config } from '../config/environment';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;\n  name: string;\n  path: string;\n  format: 'json' | 'jsonl' | 'csv';\n  totalSamples: number;\n  trainingSamples: number;\n  validationSamples: number;\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;\n  statistics: DatasetStatistics;\n  createdAt: Date;\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;\n  errors: string[];\n  warnings: string[];\n  qualityScore: number;\n  sampleSize: number;\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;\n  truncation: boolean;\n  padding: boolean;\n  removeDuplicates: boolean;\n  shuffle: boolean;\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;\n  minLength: number;\n  maxLength: number;\n  vocabSize: number;\n  uniqueTokens: number;\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;\n  jobName: string;\n  userId: string;\n  status: JobStatus;\n  baseModelName: string;\n  baseModelPath: string;\n  outputModelName: string;\n  outputModelPath: string;\n  datasetPath: string;\n  datasetFormat: 'json' | 'jsonl' | 'csv';\n  hyperparameters: Hyperparameters;\n  validationConfig: ValidationConfig;\n  progress: JobProgress;\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' \n  | 'preparing' \n  | 'training' \n  | 'evaluating' \n  | 'completed' \n  | 'failed' \n  | 'cancelled' \n  | 'paused';\n\nexport interface Hyperparameters {\n  learningRate: number;\n  batchSize: number;\n  epochs: number;\n  maxSeqLength: number;\n  gradientAccumulation: number;\n  warmupSteps: number;\n  weightDecay: number;\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';\n  scheduler?: 'linear' | 'cosine' | 'polynomial';\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;\n  validationMetrics: string[];\n  earlyStopping: boolean;\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;\n  totalEpochs: number;\n  currentStep: number;\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;\n  jobId: string;\n  modelPath: string;\n  evaluationType: 'training' | 'validation' | 'test' | 'final';\n  metrics: EvaluationMetrics;\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;\n  maxTokens: number;\n  temperature: number;\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;\n  details: any;\n  retryCount: number;\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;\n  experimentName: string;\n  baseJobId: string;\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: { min: number; max: number; step?: number } | number[];\n  batchSize: number[];\n  epochs: { min: number; max: number } | number[];\n  dropout: { min: number; max: number; step?: number };\n  weightDecay: { min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;\n  status: 'running' | 'completed' | 'failed';\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;\n  jobId: string;\n  priority: number;\n  queuePosition: number;\n  estimatedResources: {\n    memoryMB: number;\n    gpuMemoryMB: number;\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');\n    this.datasetsPath = join(process.cwd(), 'datasets');\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl',\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created',\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });\n\n      // Update status\n      job.status = 'preparing';\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process\n      job.status = 'paused';\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic',\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created',\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {\n          log.info('🛑 Early stopping optimization', LogContext.AI);\n          break;\n        }\n      }\n\n      experiment.status = 'completed';\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final',\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });\n\n      const config: EvaluationConfig = {\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];\n    queued: FineTuningJob[];\n    totalCapacity: number;\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('user_id', userId)\n        .order('created_at', { ascending: false });\n\n      if (status) {\n        query = query.eq('status', status);\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('id', jobId)\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .delete()\n        .eq('id', jobId);\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });\n      this.emit('jobDeleted', { jobId });\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';\n    activeJobs: number;\n    queuedJobs: number;\n    totalJobs: number;\n    resourceUsage: {\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*', { count: 'exact', head: true });\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });\n      return {\n        status: 'unhealthy',\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: { memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';\n      case '.jsonl': return 'jsonl';\n      case '.csv': return 'csv';\n      default: return 'jsonl';\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');\n    \n    switch (format) {\n      case 'json':\n        return JSON.parse(content);\n      case 'jsonl':\n        return content.split('\\n').filter(line => line.trim()).map(line => JSON.parse(line));\n      case 'csv':\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());\n        const headers = lines[0].split(',');\n        return lines.slice(1).map(line => {\n          const values = line.split(',');\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');\n    const tokens = allText.split(/\\s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');\n    writeFileSync(filePath, content, 'utf8');\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")\n        \n        with open(self.job_config['datasetPath'], 'r') as f:\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])\n            \n            # Training loop\n            for epoch in range(config['epochs']):\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,\n                    'training_loss': epoch_loss,\n                    'validation_loss': val_loss,\n                    'learning_rate': config['learningRate'],\n                    'timestamp': time.time()\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\n            print(\"TRAINING_COMPLETE\")\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });\n    });\n\n    process.on('exit', async (code) => {\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: { exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('\\n').filter(line => line.trim());\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);\n        \n      } else if (line.startsWith('METRICS|')) {\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: { source: 'training_process' },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {\n      id: trialId,\n      parameters,\n      metrics: { perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running',\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {\n          trial.status = 'failed';\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });\n      trial.status = 'failed';\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\n        { input: \"Test question 2\", output: \"Test answer 2\" }\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model from: ${modelPath}\")\n        model, tokenizer = load(\"${modelPath}\")\n        \n        print(f\"Exporting to: ${outputPath}\")\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\n        elif \"${format}\" == \"gguf\":\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\n        elif \"${format}\" == \"safetensors\":\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\n        \n        print(\"Export completed successfully\")\n        \n    except Exception as e:\n        print(f\"Export failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });\n      \n      process.on('exit', (code) => {\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && \n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued',\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status\n    log.debug('📋 Job queue updated', LogContext.AI, { \n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';\nimport { spawn, ChildProcess } from 'child_process';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';\nimport { join, dirname, basename, extname } from 'path';\nimport { v4 as uuidv4 } from 'uuid';\nimport { log, LogContext } from '../utils/logger';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';\nimport { mlxService } from './mlx-service';\nimport { createClient } from '@supabase/supabase-js';\nimport { config } from '../config/environment';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;\n  name: string;\n  path: string;\n  format: 'json' | 'jsonl' | 'csv';\n  totalSamples: number;\n  trainingSamples: number;\n  validationSamples: number;\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;\n  statistics: DatasetStatistics;\n  createdAt: Date;\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;\n  errors: string[];\n  warnings: string[];\n  qualityScore: number;\n  sampleSize: number;\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;\n  truncation: boolean;\n  padding: boolean;\n  removeDuplicates: boolean;\n  shuffle: boolean;\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;\n  minLength: number;\n  maxLength: number;\n  vocabSize: number;\n  uniqueTokens: number;\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;\n  jobName: string;\n  userId: string;\n  status: JobStatus;\n  baseModelName: string;\n  baseModelPath: string;\n  outputModelName: string;\n  outputModelPath: string;\n  datasetPath: string;\n  datasetFormat: 'json' | 'jsonl' | 'csv';\n  hyperparameters: Hyperparameters;\n  validationConfig: ValidationConfig;\n  progress: JobProgress;\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' \n  | 'preparing' \n  | 'training' \n  | 'evaluating' \n  | 'completed' \n  | 'failed' \n  | 'cancelled' \n  | 'paused';\n\nexport interface Hyperparameters {\n  learningRate: number;\n  batchSize: number;\n  epochs: number;\n  maxSeqLength: number;\n  gradientAccumulation: number;\n  warmupSteps: number;\n  weightDecay: number;\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';\n  scheduler?: 'linear' | 'cosine' | 'polynomial';\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;\n  validationMetrics: string[];\n  earlyStopping: boolean;\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;\n  totalEpochs: number;\n  currentStep: number;\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;\n  jobId: string;\n  modelPath: string;\n  evaluationType: 'training' | 'validation' | 'test' | 'final';\n  metrics: EvaluationMetrics;\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;\n  maxTokens: number;\n  temperature: number;\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;\n  details: any;\n  retryCount: number;\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;\n  experimentName: string;\n  baseJobId: string;\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: { min: number; max: number; step?: number } | number[];\n  batchSize: number[];\n  epochs: { min: number; max: number } | number[];\n  dropout: { min: number; max: number; step?: number };\n  weightDecay: { min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;\n  status: 'running' | 'completed' | 'failed';\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;\n  jobId: string;\n  priority: number;\n  queuePosition: number;\n  estimatedResources: {\n    memoryMB: number;\n    gpuMemoryMB: number;\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');\n    this.datasetsPath = join(process.cwd(), 'datasets');\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl',\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created',\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });\n\n      // Update status\n      job.status = 'preparing';\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process\n      job.status = 'paused';\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic',\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created',\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {\n          log.info('🛑 Early stopping optimization', LogContext.AI);\n          break;\n        }\n      }\n\n      experiment.status = 'completed';\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final',\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });\n\n      const config: EvaluationConfig = {\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];\n    queued: FineTuningJob[];\n    totalCapacity: number;\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('user_id', userId)\n        .order('created_at', { ascending: false });\n\n      if (status) {\n        query = query.eq('status', status);\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('id', jobId)\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .delete()\n        .eq('id', jobId);\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });\n      this.emit('jobDeleted', { jobId });\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';\n    activeJobs: number;\n    queuedJobs: number;\n    totalJobs: number;\n    resourceUsage: {\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*', { count: 'exact', head: true });\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });\n      return {\n        status: 'unhealthy',\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: { memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';\n      case '.jsonl': return 'jsonl';\n      case '.csv': return 'csv';\n      default: return 'jsonl';\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');\n    \n    switch (format) {\n      case 'json':\n        return JSON.parse(content);\n      case 'jsonl':\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));\n      case 'csv':\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());\n        const headers = lines[0].split(',');\n        return lines.slice(1).map(line => {\n          const values = line.split(',');\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');\n    writeFileSync(filePath, content, 'utf8');\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")\n        \n        with open(self.job_config['datasetPath'], 'r') as f:\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])\n            \n            # Training loop\n            for epoch in range(config['epochs']):\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,\n                    'training_loss': epoch_loss,\n                    'validation_loss': val_loss,\n                    'learning_rate': config['learningRate'],\n                    'timestamp': time.time()\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\n            print(\"TRAINING_COMPLETE\")\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });\n    });\n\n    process.on('exit', async (code) => {\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: { exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);\n        \n      } else if (line.startsWith('METRICS|')) {\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: { source: 'training_process' },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {\n      id: trialId,\n      parameters,\n      metrics: { perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running',\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {\n          trial.status = 'failed';\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });\n      trial.status = 'failed';\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\n        { input: \"Test question 2\", output: \"Test answer 2\" }\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model from: ${modelPath}\")\n        model, tokenizer = load(\"${modelPath}\")\n        \n        print(f\"Exporting to: ${outputPath}\")\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\n        elif \"${format}\" == \"gguf\":\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\n        elif \"${format}\" == \"safetensors\":\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\n        \n        print(\"Export completed successfully\")\n        \n    except Exception as e:\n        print(f\"Export failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });\n      \n      process.on('exit', (code) => {\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && \n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued',\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status\n    log.debug('📋 Job queue updated', LogContext.AI, { \n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "missing_comma_object_literal",
        "count": 93,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';\nimport { spawn, ChildProcess } from 'child_process';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';\nimport { join, dirname, basename, extname } from 'path';\nimport { v4 as uuidv4 } from 'uuid';\nimport { log, LogContext } from '../utils/logger';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';\nimport { mlxService } from './mlx-service';\nimport { createClient } from '@supabase/supabase-js';\nimport { config } from '../config/environment';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;\n  name: string;\n  path: string;\n  format: 'json' | 'jsonl' | 'csv';\n  totalSamples: number;\n  trainingSamples: number;\n  validationSamples: number;\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;\n  statistics: DatasetStatistics;\n  createdAt: Date;\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;\n  errors: string[];\n  warnings: string[];\n  qualityScore: number;\n  sampleSize: number;\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;\n  truncation: boolean;\n  padding: boolean;\n  removeDuplicates: boolean;\n  shuffle: boolean;\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;\n  minLength: number;\n  maxLength: number;\n  vocabSize: number;\n  uniqueTokens: number;\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;\n  jobName: string;\n  userId: string;\n  status: JobStatus;\n  baseModelName: string;\n  baseModelPath: string;\n  outputModelName: string;\n  outputModelPath: string;\n  datasetPath: string;\n  datasetFormat: 'json' | 'jsonl' | 'csv';\n  hyperparameters: Hyperparameters;\n  validationConfig: ValidationConfig;\n  progress: JobProgress;\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' \n  | 'preparing' \n  | 'training' \n  | 'evaluating' \n  | 'completed' \n  | 'failed' \n  | 'cancelled' \n  | 'paused';\n\nexport interface Hyperparameters {\n  learningRate: number;\n  batchSize: number;\n  epochs: number;\n  maxSeqLength: number;\n  gradientAccumulation: number;\n  warmupSteps: number;\n  weightDecay: number;\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';\n  scheduler?: 'linear' | 'cosine' | 'polynomial';\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;\n  validationMetrics: string[];\n  earlyStopping: boolean;\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;\n  totalEpochs: number;\n  currentStep: number;\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;\n  jobId: string;\n  modelPath: string;\n  evaluationType: 'training' | 'validation' | 'test' | 'final';\n  metrics: EvaluationMetrics;\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;\n  maxTokens: number;\n  temperature: number;\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;\n  details: any;\n  retryCount: number;\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;\n  experimentName: string;\n  baseJobId: string;\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: { min: number; max: number; step?: number } | number[];\n  batchSize: number[];\n  epochs: { min: number; max: number } | number[];\n  dropout: { min: number; max: number; step?: number };\n  weightDecay: { min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;\n  status: 'running' | 'completed' | 'failed';\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;\n  jobId: string;\n  priority: number;\n  queuePosition: number;\n  estimatedResources: {\n    memoryMB: number;\n    gpuMemoryMB: number;\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');\n    this.datasetsPath = join(process.cwd(), 'datasets');\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl',\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created',\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });\n\n      // Update status\n      job.status = 'preparing';\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process\n      job.status = 'paused';\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic',\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created',\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {\n          log.info('🛑 Early stopping optimization', LogContext.AI);\n          break;\n        }\n      }\n\n      experiment.status = 'completed';\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final',\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });\n\n      const config: EvaluationConfig = {\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];\n    queued: FineTuningJob[];\n    totalCapacity: number;\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('user_id', userId)\n        .order('created_at', { ascending: false });\n\n      if (status) {\n        query = query.eq('status', status);\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('id', jobId)\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .delete()\n        .eq('id', jobId);\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });\n      this.emit('jobDeleted', { jobId });\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';\n    activeJobs: number;\n    queuedJobs: number;\n    totalJobs: number;\n    resourceUsage: {\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*', { count: 'exact', head: true });\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });\n      return {\n        status: 'unhealthy',\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: { memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';\n      case '.jsonl': return 'jsonl';\n      case '.csv': return 'csv';\n      default: return 'jsonl';\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');\n    \n    switch (format) {\n      case 'json':\n        return JSON.parse(content);\n      case 'jsonl':\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));\n      case 'csv':\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());\n        const headers = lines[0].split(',');\n        return lines.slice(1).map(line => {\n          const values = line.split(',');\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');\n    writeFileSync(filePath, content, 'utf8');\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")\n        \n        with open(self.job_config['datasetPath'], 'r') as f:\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])\n            \n            # Training loop\n            for epoch in range(config['epochs']):\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,\n                    'training_loss': epoch_loss,\n                    'validation_loss': val_loss,\n                    'learning_rate': config['learningRate'],\n                    'timestamp': time.time()\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\n            print(\"TRAINING_COMPLETE\")\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });\n    });\n\n    process.on('exit', async (code) => {\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: { exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);\n        \n      } else if (line.startsWith('METRICS|')) {\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: { source: 'training_process' },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {\n      id: trialId,\n      parameters,\n      metrics: { perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running',\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {\n          trial.status = 'failed';\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });\n      trial.status = 'failed';\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\n        { input: \"Test question 2\", output: \"Test answer 2\" }\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model from: ${modelPath}\")\n        model, tokenizer = load(\"${modelPath}\")\n        \n        print(f\"Exporting to: ${outputPath}\")\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\n        elif \"${format}\" == \"gguf\":\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\n        elif \"${format}\" == \"safetensors\":\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\n        \n        print(\"Export completed successfully\")\n        \n    except Exception as e:\n        print(f\"Export failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });\n      \n      process.on('exit', (code) => {\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && \n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued',\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status\n    log.debug('📋 Job queue updated', LogContext.AI, { \n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';\nimport { spawn, ChildProcess } from 'child_process';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';\nimport { join, dirname, basename, extname } from 'path';\nimport { v4 as uuidv4 } from 'uuid';\nimport { log, LogContext } from '../utils/logger';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';\nimport { mlxService } from './mlx-service';\nimport { createClient } from '@supabase/supabase-js';\nimport { config } from '../config/environment';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' \n  | 'preparing' \n  | 'training' \n  | 'evaluating' \n  | 'completed' \n  | 'failed' \n  | 'cancelled' \n  | 'paused';\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';\n  scheduler?: 'linear' | 'cosine' | 'polynomial';\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');\n    this.datasetsPath = join(process.cwd(), 'datasets');\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl',\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created',\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });\n\n      // Update status\n      job.status = 'preparing';\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process\n      job.status = 'paused';\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic',\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created',\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {\n          log.info('🛑 Early stopping optimization', LogContext.AI);\n          break;\n        }\n      }\n\n      experiment.status = 'completed';\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final',\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('user_id', userId)\n        .order('created_at', { ascending: false });\n\n      if (status) {\n        query = query.eq('status', status);\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('id', jobId)\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .delete()\n        .eq('id', jobId);\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });\n      this.emit('jobDeleted', { jobId });\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*', { count: 'exact', head: true });\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });\n      return {\n        status: 'unhealthy',\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';\n      case '.jsonl': return 'jsonl';\n      case '.csv': return 'csv';\n      default: return 'jsonl';\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');\n    \n    switch (format) {\n      case 'json':\n        return JSON.parse(content);\n      case 'jsonl':\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));\n      case 'csv':\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());\n        const headers = lines[0].split(',');\n        return lines.slice(1).map(line => {\n          const values = line.split(',');\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');\n    writeFileSync(filePath, content, 'utf8');\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")\n        \n        with open(self.job_config['datasetPath'], 'r') as f:\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])\n            \n            # Training loop\n            for epoch in range(config['epochs']):\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,\n                    'training_loss': epoch_loss,\n                    'validation_loss': val_loss,\n                    'learning_rate': config['learningRate'],\n                    'timestamp': time.time()\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\n            print(\"TRAINING_COMPLETE\")\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });\n    });\n\n    process.on('exit', async (code) => {\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);\n        \n      } else if (line.startsWith('METRICS|')) {\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: {, source: 'training_process' },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running',\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {\n          trial.status = 'failed';\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });\n      trial.status = 'failed';\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\n        { input: \"Test question 2\", output: \"Test answer 2\" }\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\n        model, tokenizer = load(\"${modelPath}\")\n        \n        print(f\"Exporting to: ${outputPath}\")\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\n        elif \"${format}\" == \"gguf\":\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\n        elif \"${format}\" == \"safetensors\":\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\n        \n        print(\"Export completed successfully\")\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });\n      \n      process.on('exit', (code) => {\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && \n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued',\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status\n    log.debug('📋 Job queue updated', LogContext.AI, { \n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "unterminated_string_single",
        "count": 206,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';\nimport { spawn, ChildProcess } from 'child_process';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';\nimport { join, dirname, basename, extname } from 'path';\nimport { v4 as uuidv4 } from 'uuid';\nimport { log, LogContext } from '../utils/logger';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';\nimport { mlxService } from './mlx-service';\nimport { createClient } from '@supabase/supabase-js';\nimport { config } from '../config/environment';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' \n  | 'preparing' \n  | 'training' \n  | 'evaluating' \n  | 'completed' \n  | 'failed' \n  | 'cancelled' \n  | 'paused';\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';\n  scheduler?: 'linear' | 'cosine' | 'polynomial';\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');\n    this.datasetsPath = join(process.cwd(), 'datasets');\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl',\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created',\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });\n\n      // Update status\n      job.status = 'preparing';\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {\n        stdio: ['pipe', 'pipe', 'pipe'],\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process\n      job.status = 'paused';\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process\n      job.status = 'training';\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic',\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created',\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {\n          log.info('🛑 Early stopping optimization', LogContext.AI);\n          break;\n        }\n      }\n\n      experiment.status = 'completed';\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final',\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('user_id', userId)\n        .order('created_at', { ascending: false });\n\n      if (status) {\n        query = query.eq('status', status);\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*')\n        .eq('id', jobId)\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .delete()\n        .eq('id', jobId);\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });\n      this.emit('jobDeleted', { jobId });\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')\n        .select('*', { count: 'exact', head: true });\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });\n      return {\n        status: 'unhealthy',\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';\n      case '.jsonl': return 'jsonl';\n      case '.csv': return 'csv';\n      default: return 'jsonl';\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');\n    \n    switch (format) {\n      case 'json':\n        return JSON.parse(content);\n      case 'jsonl':\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));\n      case 'csv':\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());\n        const headers = lines[0].split(',');\n        return lines.slice(1).map(line => {\n          const values = line.split(',');\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');\n    writeFileSync(filePath, content, 'utf8');\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")\n        \n        with open(self.job_config['datasetPath'], 'r') as f:\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])\n            \n            # Training loop\n            for epoch in range(config['epochs']):\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,\n                    'training_loss': epoch_loss,\n                    'validation_loss': val_loss,\n                    'learning_rate': config['learningRate'],\n                    'timestamp': time.time()\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\n            print(\"TRAINING_COMPLETE\")\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });\n    });\n\n    process.on('exit', async (code) => {\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);\n        \n      } else if (line.startsWith('METRICS|')) {\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: {, source: 'training_process' },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running',\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {\n          trial.status = 'failed';\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });\n      trial.status = 'failed';\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\n        { input: \"Test question 2\", output: \"Test answer 2\" }\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\n        model, tokenizer = load(\"${modelPath}\")\n        \n        print(f\"Exporting to: ${outputPath}\")\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\n        elif \"${format}\" == \"gguf\":\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\n        elif \"${format}\" == \"safetensors\":\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\n        \n        print(\"Export completed successfully\")\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });\n      \n      process.on('exit', (code) => {\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && \n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued',\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status\n    log.debug('📋 Job queue updated', LogContext.AI, { \n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';'\nimport { spawn, ChildProcess } from 'child_process';'\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';'\nimport { join, dirname, basename, extname } from 'path';'\nimport { v4 as uuidv4 } from 'uuid';'\nimport { log, LogContext } from '../utils/logger';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';'\nimport { mlxService } from './mlx-service';'\nimport { createClient } from '@supabase/supabase-js';'\nimport { config } from '../config/environment';'\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {'\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);'\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {'\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];'\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {'\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {'\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {'\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {'\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx','\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);'\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);'\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;'\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';'\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');'\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));'\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());'\n        const headers = lines[0].split(',');'\n        return lines.slice(1).map(line => {\n          const values = line.split(',');'\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);'\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');'\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');'\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\n            print(\"TRAINING_COMPLETE\")\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());'\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');'\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\n        { input: \"Test question 2\", output: \"Test answer 2\" }\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\n        model, tokenizer = load(\"${modelPath}\")\n        \n        print(f\"Exporting to: ${outputPath}\")\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\n        elif \"${format}\" == \"gguf\":\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\n        elif \"${format}\" == \"safetensors\":\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\n        \n        print(\"Export completed successfully\")\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });'\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');'\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')'\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')'\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { '\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "unterminated_string_double",
        "count": 31,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';'\nimport { spawn, ChildProcess } from 'child_process';'\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';'\nimport { join, dirname, basename, extname } from 'path';'\nimport { v4 as uuidv4 } from 'uuid';'\nimport { log, LogContext } from '../utils/logger';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';'\nimport { mlxService } from './mlx-service';'\nimport { createClient } from '@supabase/supabase-js';'\nimport { config } from '../config/environment';'\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {'\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);'\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {'\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];'\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {'\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {'\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {'\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {'\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx','\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);'\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);'\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;'\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';'\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');'\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));'\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());'\n        const headers = lines[0].split(',');'\n        return lines.slice(1).map(line => {\n          const values = line.split(',');'\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);'\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');'\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');'\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\n            print(\"TRAINING_COMPLETE\")\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());'\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');'\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\n        { input: \"Test question 2\", output: \"Test answer 2\" }\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\n        model, tokenizer = load(\"${modelPath}\")\n        \n        print(f\"Exporting to: ${outputPath}\")\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\n        elif \"${format}\" == \"gguf\":\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\n        elif \"${format}\" == \"safetensors\":\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\n        \n        print(\"Export completed successfully\")\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });'\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');'\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')'\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')'\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { '\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';'\nimport { spawn, ChildProcess } from 'child_process';'\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';'\nimport { join, dirname, basename, extname } from 'path';'\nimport { v4 as uuidv4 } from 'uuid';'\nimport { log, LogContext } from '../utils/logger';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';'\nimport { mlxService } from './mlx-service';'\nimport { createClient } from '@supabase/supabase-js';'\nimport { config } from '../config/environment';'\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {'\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);'\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {'\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];'\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {'\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {'\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {'\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {'\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx','\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);'\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);'\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;'\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';'\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');'\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));'\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());'\n        const headers = lines[0].split(',');'\n        return lines.slice(1).map(line => {\n          const values = line.split(',');'\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);'\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');'\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');'\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());'\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');'\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });'\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');'\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')'\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')'\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { '\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "missing_parentheses_logger",
        "count": 3,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';'\nimport { spawn, ChildProcess } from 'child_process';'\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';'\nimport { join, dirname, basename, extname } from 'path';'\nimport { v4 as uuidv4 } from 'uuid';'\nimport { log, LogContext } from '../utils/logger';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';'\nimport { mlxService } from './mlx-service';'\nimport { createClient } from '@supabase/supabase-js';'\nimport { config } from '../config/environment';'\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {'\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);'\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {'\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];'\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {'\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error = {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {'\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {'\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {'\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx','\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);'\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);'\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;'\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';'\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');'\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));'\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());'\n        const headers = lines[0].split(',');'\n        return lines.slice(1).map(line => {\n          const values = line.split(',');'\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);'\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');'\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');'\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error = {\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());'\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');'\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error = {\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });'\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');'\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')'\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')'\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { '\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';'\nimport { spawn, ChildProcess } from 'child_process';'\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';'\nimport { join, dirname, basename, extname } from 'path';'\nimport { v4 as uuidv4 } from 'uuid';'\nimport { log, LogContext } from '../utils/logger';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';'\nimport { mlxService } from './mlx-service';'\nimport { createClient } from '@supabase/supabase-js';'\nimport { config } from '../config/environment';'\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {'\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);'\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {'\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];'\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {'\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {'\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {'\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {'\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx','\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);'\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);'\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;'\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';'\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');'\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));'\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());'\n        const headers = lines[0].split(',');'\n        return lines.slice(1).map(line => {\n          const values = line.split(',');'\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);'\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');'\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');'\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());'\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');'\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });'\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');'\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')'\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')'\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { '\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "missing_closing_parenthesis",
        "count": 38,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';'\nimport { spawn, ChildProcess } from 'child_process';'\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';'\nimport { join, dirname, basename, extname } from 'path';'\nimport { v4 as uuidv4 } from 'uuid';'\nimport { log, LogContext } from '../utils/logger';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';'\nimport { mlxService } from './mlx-service';'\nimport { createClient } from '@supabase/supabase-js';'\nimport { config } from '../config/environment';'\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient(\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {'\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset(\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);'\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {'\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];'\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob(\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {'\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {'\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization(\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {'\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations(\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {'\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel(\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {'\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel(\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx','\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);'\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);'\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;'\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';'\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');'\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));'\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());'\n        const headers = lines[0].split(',');'\n        return lines.slice(1).map(line => {\n          const values = line.split(',');'\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);'\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');'\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');'\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());'\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');'\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations(\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial(\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob(\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics(\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs(\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });'\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item => \n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');'\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')'\n      .insert({\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .insert({\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .update({\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')'\n      .insert({\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .insert({\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .update({\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { '\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';'\nimport { spawn, ChildProcess } from 'child_process';'\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';'\nimport { join, dirname, basename, extname } from 'path';'\nimport { v4 as uuidv4 } from 'uuid';'\nimport { log, LogContext } from '../utils/logger';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';'\nimport { mlxService } from './mlx-service';'\nimport { createClient } from '@supabase/supabase-js';'\nimport { config } from '../config/environment';'\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);'\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];'\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations()\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx','\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);'\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);'\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;'\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';'\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');'\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));'\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());'\n        const headers = lines[0].split(',');'\n        return lines.slice(1).map(line => {)\n          const values = line.split(',');'\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);'\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');'\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');'\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());'\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');'\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob()\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({)\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });'\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>)\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');'\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "semicolon_missing_statements",
        "count": 77,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';'\nimport { spawn, ChildProcess } from 'child_process';'\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';'\nimport { join, dirname, basename, extname } from 'path';'\nimport { v4 as uuidv4 } from 'uuid';'\nimport { log, LogContext } from '../utils/logger';'\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';'\nimport { mlxService } from './mlx-service';'\nimport { createClient } from '@supabase/supabase-js';'\nimport { config } from '../config/environment';'\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);'\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];'\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {')\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations()\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx','\n    exportPath?: string\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);'\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);'\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;'\n      \n      // Get total job count from database\n      const { count } = await this.supabase\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';'\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');'\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));'\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());'\n        const headers = lines[0].split(',');'\n        return lines.slice(1).map(line => {)\n          const values = line.split(',');'\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);'\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');'\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');'\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());'\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');'\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate)\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs)\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob()\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({)\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os\nimport sys\nimport mlx.core as mx\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model()\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });'\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>)\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');'\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';';\nimport { spawn, ChildProcess } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';';\nimport { join, dirname, basename, extname } from 'path';';\nimport { v4 as uuidv4 } from 'uuid';';\nimport { log, LogContext } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';';\nimport { mlxService } from './mlx-service';';\nimport { createClient } from '@supabase/supabase-js';';\nimport { config } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "type_annotation_spacing",
        "count": 45,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';';\nimport { spawn, ChildProcess } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';';\nimport { join, dirname, basename, extname } from 'path';';\nimport { v4 as uuidv4 } from 'uuid';';\nimport { log, LogContext } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';';\nimport { mlxService } from './mlx-service';';\nimport { createClient } from '@supabase/supabase-js';';\nimport { config } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';';\nimport { spawn, ChildProcess } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';';\nimport { join, dirname, basename, extname } from 'path';';\nimport { v4 as uuidv4 } from 'uuid';';\nimport { log, LogContext } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';';\nimport { mlxService } from './mlx-service';';\nimport { createClient } from '@supabase/supabase-js';';\nimport { config } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "import_spacing",
        "count": 10,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter } from 'events';';\nimport { spawn, ChildProcess } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync } from 'fs';';\nimport { join, dirname, basename, extname } from 'path';';\nimport { v4 as uuidv4 } from 'uuid';';\nimport { log, LogContext } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry } from '../utils/circuit-breaker';';\nimport { mlxService } from './mlx-service';';\nimport { createClient } from '@supabase/supabase-js';';\nimport { config } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter  } from 'events';';\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync  } from 'fs';';\nimport { join, dirname, basename, extname  } from 'path';';\nimport { v4 as uuidv4  } from 'uuid';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '../utils/circuit-breaker';';\nimport { mlxService  } from './mlx-service';';\nimport { createClient  } from '@supabase/supabase-js';';\nimport { config  } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "object_property_spacing",
        "count": 615,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features:\n * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter  } from 'events';';\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync  } from 'fs';';\nimport { join, dirname, basename, extname  } from 'path';';\nimport { v4 as uuidv4  } from 'uuid';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '../utils/circuit-breaker';';\nimport { mlxService  } from './mlx-service';';\nimport { createClient  } from '@supabase/supabase-js';';\nimport { config  } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress : null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics : null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default:\n        throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner:\n    def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f:'\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx:]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try:\n            print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e:\n            print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate \n        : [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs \n        : [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try:\n        print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e:\n        print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features: * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter  } from 'events';';\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync  } from 'fs';';\nimport { join, dirname, basename, extname  } from 'path';';\nimport { v4 as uuidv4  } from 'uuid';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '../utils/circuit-breaker';';\nimport { mlxService  } from './mlx-service';';\nimport { createClient  } from '@supabase/supabase-js';';\nimport { config  } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress: null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics: null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default: throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner: def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f: '\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx: ]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try: print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e: print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate: [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs: [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try: print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e: print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/services/mlx-fine-tuning-service.ts",
        "pattern": "arrow_function_spacing",
        "count": 13,
        "originalContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features: * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter  } from 'events';';\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync  } from 'fs';';\nimport { join, dirname, basename, extname  } from 'path';';\nimport { v4 as uuidv4  } from 'uuid';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '../utils/circuit-breaker';';\nimport { mlxService  } from './mlx-service';';\nimport { createClient  } from '@supabase/supabase-js';';\nimport { config  } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress: null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics: null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default: throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner: def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f: '\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx: ]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try: print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e: print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate: [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs: [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try: print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e: print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;",
        "fixedContent": "/**\n * MLX Fine-tuning Service\n * Comprehensive service for managing the entire fine-tuning lifecycle on Apple Silicon\n * \n * Features: * - Dataset management and validation\n * - Fine-tuning job orchestration  \n * - Hyperparameter optimization\n * - Real-time progress monitoring\n * - Model evaluation and export\n * - Job persistence with Supabase\n * - Resource management and queuing\n */\n\nimport { EventEmitter  } from 'events';';\nimport { spawn, ChildProcess  } from 'child_process';';\nimport { existsSync, readFileSync, writeFileSync, mkdirSync, statSync, readdirSync  } from 'fs';';\nimport { join, dirname, basename, extname  } from 'path';';\nimport { v4 as uuidv4  } from 'uuid';';\nimport { log, LogContext  } from '../utils/logger';';\nimport { CircuitBreaker, CircuitBreakerRegistry  } from '../utils/circuit-breaker';';\nimport { mlxService  } from './mlx-service';';\nimport { createClient  } from '@supabase/supabase-js';';\nimport { config  } from '../config/environment';';\n\n// ============================================================================\n// Type Definitions\n// ============================================================================\n\nexport interface Dataset {\n  id: string;,\n  name: string;\n  path: string;,\n  format: 'json' | 'jsonl' | 'csv';'\n  totalSamples: number;,\n  trainingSamples: number;\n  validationSamples: number;,\n  validationResults: DatasetValidationResult;\n  preprocessingConfig: PreprocessingConfig;,\n  statistics: DatasetStatistics;\n  createdAt: Date;,\n  updatedAt: Date;\n}\n\nexport interface DatasetValidationResult {\n  isValid: boolean;,\n  errors: string[];\n  warnings: string[];,\n  qualityScore: number;\n  sampleSize: number;,\n  duplicateCount: number;\n  malformedEntries: number;\n}\n\nexport interface PreprocessingConfig {\n  maxLength: number;,\n  truncation: boolean;\n  padding: boolean;,\n  removeDuplicates: boolean;\n  shuffle: boolean;,\n  validationSplit: number;\n  testSplit?: number;\n  customFilters?: string[];\n}\n\nexport interface DatasetStatistics {\n  avgLength: number;,\n  minLength: number;\n  maxLength: number;,\n  vocabSize: number;\n  uniqueTokens: number;,\n  lengthDistribution: { [key: string]: number };\n  tokenFrequency: { [key: string]: number };\n}\n\nexport interface FineTuningJob {\n  id: string;,\n  jobName: string;\n  userId: string;,\n  status: JobStatus;\n  baseModelName: string;,\n  baseModelPath: string;\n  outputModelName: string;,\n  outputModelPath: string;\n  datasetPath: string;,\n  datasetFormat: 'json' | 'jsonl' | 'csv';'\n  hyperparameters: Hyperparameters;,\n  validationConfig: ValidationConfig;\n  progress: JobProgress;,\n  metrics: TrainingMetrics;\n  evaluation: ModelEvaluation | null;,\n  resourceUsage: ResourceUsage;\n  error?: JobError;\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  updatedAt: Date;\n}\n\nexport type JobStatus = \n  | 'created' '\n  | 'preparing' '\n  | 'training' '\n  | 'evaluating' '\n  | 'completed' '\n  | 'failed' '\n  | 'cancelled' '\n  | 'paused';'\n\nexport interface Hyperparameters {\n  learningRate: number;,\n  batchSize: number;\n  epochs: number;,\n  maxSeqLength: number;\n  gradientAccumulation: number;,\n  warmupSteps: number;\n  weightDecay: number;,\n  dropout: number;\n  optimizerType?: 'adam' | 'sgd' | 'adamw';'\n  scheduler?: 'linear' | 'cosine' | 'polynomial';'\n}\n\nexport interface ValidationConfig {\n  splitRatio: number;,\n  validationMetrics: string[];\n  earlyStopping: boolean;,\n  patience: number;\n  minDelta?: number;\n  evaluateEveryNSteps?: number;\n}\n\nexport interface JobProgress {\n  currentEpoch: number;,\n  totalEpochs: number;\n  currentStep: number;,\n  totalSteps: number;\n  progressPercentage: number;\n  estimatedTimeRemaining?: number;\n  lastUpdateTime: Date;\n}\n\nexport interface TrainingMetrics {\n  trainingLoss: number[];,\n  validationLoss: number[];\n  trainingAccuracy?: number[];\n  validationAccuracy?: number[];\n  learningRates: number[];\n  gradientNorms?: number[];\n  perplexity?: number[];\n  epochTimes: number[];\n}\n\nexport interface ModelEvaluation {\n  id: string;,\n  jobId: string;\n  modelPath: string;,\n  evaluationType: 'training' | 'validation' | 'test' | 'final';'\n  metrics: EvaluationMetrics;,\n  sampleOutputs: SampleOutput[];\n  evaluationConfig: EvaluationConfig;,\n  createdAt: Date;\n}\n\nexport interface EvaluationMetrics {\n  perplexity: number;,\n  loss: number;\n  accuracy: number;\n  bleuScore?: number;\n  rougeScores?: {\n    rouge1: number;,\n    rouge2: number;\n    rougeL: number;\n  };\n  customMetrics?: { [key: string]: number };\n}\n\nexport interface SampleOutput {\n  input: string;,\n  output: string;\n  reference?: string;\n  confidence?: number;\n}\n\nexport interface EvaluationConfig {\n  numSamples: number;,\n  maxTokens: number;\n  temperature: number;,\n  topP: number;\n  testDatasetPath?: string;\n}\n\nexport interface ResourceUsage {\n  memoryUsageMB: number;,\n  gpuUtilizationPercentage: number;\n  estimatedDurationMinutes?: number;\n  actualDurationMinutes?: number;\n  powerConsumptionWatts?: number;\n}\n\nexport interface JobError {\n  message: string;,\n  details: any;\n  retryCount: number;,\n  maxRetries: number;\n  recoverable: boolean;\n}\n\nexport interface HyperparameterOptimization {\n  id: string;,\n  experimentName: string;\n  baseJobId: string;,\n  userId: string;\n  optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic';,'\n  parameterSpace: ParameterSpace;\n  status: 'created' | 'running' | 'completed' | 'failed' | 'cancelled';,'\n  trials: OptimizationTrial[];\n  bestTrial?: OptimizationTrial;\n  createdAt: Date;\n  completedAt?: Date;\n}\n\nexport interface ParameterSpace {\n  learningRate: {, min: number; max: number; step?: number } | number[];\n  batchSize: number[];,\n  epochs: { min: number;, max: number } | number[];\n  dropout: {, min: number; max: number; step?: number };\n  weightDecay: {, min: number; max: number; step?: number };\n  [key: string]: any;\n}\n\nexport interface OptimizationTrial {\n  id: string;,\n  parameters: Hyperparameters;\n  metrics: EvaluationMetrics;,\n  status: 'running' | 'completed' | 'failed';'\n  startTime: Date;\n  endTime?: Date;\n  jobId?: string;\n}\n\nexport interface JobQueue {\n  id: string;,\n  jobId: string;\n  priority: number;,\n  queuePosition: number;\n  estimatedResources: {,\n    memoryMB: number;\n    gpuMemoryMB: number;,\n    durationMinutes: number;\n  };\n  dependsOnJobIds: string[];,\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';'\n  scheduledAt?: Date;\n  startedAt?: Date;\n  createdAt: Date;\n}\n\n// ============================================================================\n// Main Service Class\n// ============================================================================\n\nexport class MLXFineTuningService extends EventEmitter {\n  private activeJobs: Map<string, ChildProcess> = new Map();\n  private jobQueue: JobQueue[] = [];\n  private isProcessingQueue = false;\n  private maxConcurrentJobs = 2;\n  private modelsPath: string;\n  private datasetsPath: string;\n  private tempPath: string;\n  private supabase: any;\n\n  constructor() {\n    super();\n    \n    // Initialize Supabase client\n    this.supabase = createClient()\n      config.supabase.url,\n      config.supabase.serviceKey\n    );\n    \n    this.modelsPath = join(process.cwd(), 'models', 'fine-tuned');'\n    this.datasetsPath = join(process.cwd(), 'datasets');'\n    this.tempPath = join(process.cwd(), 'temp', 'mlx-training');'\n    \n    this.ensureDirectories();\n    this.startQueueProcessor();\n    \n    log.info('🍎 MLX Fine-tuning Service initialized', LogContext.AI, {')\n      modelsPath: this.modelsPath,\n      datasetsPath: this.datasetsPath,\n      maxConcurrentJobs: this.maxConcurrentJobs\n    });\n  }\n\n  // ============================================================================\n  // Dataset Management\n  // ============================================================================\n\n  /**\n   * Load and validate a dataset\n   */\n  public async loadDataset()\n    datasetPath: string,\n    name: string,\n    userId: string,\n    preprocessingConfig?: Partial<PreprocessingConfig>\n  ): Promise<Dataset> {\n    try {\n      log.info('📊 Loading dataset', LogContext.AI, { path: datasetPath, name });'\n\n      if (!existsSync(datasetPath)) {\n        throw new Error(`Dataset file not found: ${datasetPath}`);\n      }\n\n      const format = this.detectDatasetFormat(datasetPath);\n      const rawData = await this.readDatasetFile(datasetPath, format);\n      \n      // Validate dataset\n      const validationResults = await this.validateDataset(rawData, format);\n      if (!validationResults.isValid) {\n        throw new Error(`Dataset validation failed: ${validationResults.errors.join(', ')}`);';\n      }\n\n      // Apply preprocessing\n      const config: PreprocessingConfig = {,;\n        maxLength: 2048,\n        truncation: true,\n        padding: true,\n        removeDuplicates: true,\n        shuffle: true,\n        validationSplit: 0.1,\n        ...preprocessingConfig\n      };\n\n      const processedData = await this.preprocessDataset(rawData, config);\n      const statistics = await this.calculateDatasetStatistics(processedData);\n\n      // Save processed dataset\n      const processedPath = join(this.datasetsPath, `${name}_processed.jsonl`);\n      await this.saveProcessedDataset(processedData, processedPath);\n\n      // Create dataset record\n      const dataset: Dataset = {,;\n        id: uuidv4(),\n        name,\n        path: processedPath,\n        format: 'jsonl','\n        totalSamples: processedData.length,\n        trainingSamples: Math.floor(processedData.length * (1 - config.validationSplit)),\n        validationSamples: Math.floor(processedData.length * config.validationSplit),\n        validationResults,\n        preprocessingConfig: config,\n        statistics,\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveDatasetToDatabase(dataset, userId);\n\n      log.info('✅ Dataset loaded successfully', LogContext.AI, {')\n        name,\n        totalSamples: dataset.totalSamples,\n        qualityScore: validationResults.qualityScore\n      });\n\n      return dataset;\n\n    } catch (error) {\n      log.error('❌ Failed to load dataset', LogContext.AI, { error, path: datasetPath });'\n      throw error;\n    }\n  }\n\n  /**\n   * Validate dataset quality and format\n   */\n  private async validateDataset(data: any[], format: string): Promise<DatasetValidationResult> {\n    const result: DatasetValidationResult = {,;\n      isValid: true,\n      errors: [],\n      warnings: [],\n      qualityScore: 1.0,\n      sampleSize: data.length,\n      duplicateCount: 0,\n      malformedEntries: 0\n    };\n\n    if (data.length === 0) {\n      result.errors.push('Dataset is empty');'\n      result.isValid = false;\n      return result;\n    }\n\n    // Check for required fields\n    const requiredFields = ['input', 'output'];';\n    const sampleEntry = data[0];\n    \n    for (const field of requiredFields) {\n      if (!(field in sampleEntry)) {\n        result.errors.push(`Missing required field: ${field}`);\n        result.isValid = false;\n      }\n    }\n\n    // Check for duplicates\n    const seen = new Set();\n    let duplicates = 0;\n    let malformed = 0;\n\n    for (const entry of data) {\n      // Check for malformed entries\n      if (!entry.input || !entry.output) {\n        malformed++;\n        continue;\n      }\n\n      // Check for duplicates\n      const key = `${entry.input}|${entry.output}`;\n      if (seen.has(key)) {\n        duplicates++;\n      } else {\n        seen.add(key);\n      }\n    }\n\n    result.duplicateCount = duplicates;\n    result.malformedEntries = malformed;\n\n    // Calculate quality score\n    const duplicateRatio = duplicates / data.length;\n    const malformedRatio = malformed / data.length;\n    result.qualityScore = Math.max(0, 1 - (duplicateRatio * 0.5 + malformedRatio * 0.8));\n\n    // Add warnings\n    if (duplicateRatio > 0.1) {\n      result.warnings.push(`High duplicate ratio: ${(duplicateRatio * 100).toFixed(1)}%`);\n    }\n    if (malformedRatio > 0.05) {\n      result.warnings.push(`High malformed entry ratio: ${(malformedRatio * 100).toFixed(1)}%`);\n    }\n    if (data.length < 100) {\n      result.warnings.push('Dataset size is small, consider adding more samples');'\n    }\n\n    return result;\n  }\n\n  // ============================================================================\n  // Fine-tuning Job Management\n  // ============================================================================\n\n  /**\n   * Create a new fine-tuning job\n   */\n  public async createFineTuningJob()\n    jobName: string,\n    userId: string,\n    baseModelName: string,\n    baseModelPath: string,\n    datasetPath: string,\n    hyperparameters: Partial<Hyperparameters> = {},\n    validationConfig: Partial<ValidationConfig> = {}\n  ): Promise<FineTuningJob> {\n    try {\n      const jobId = uuidv4();\n      const outputModelName = `${baseModelName}_${jobName}_${Date.now()}`;\n      const outputModelPath = join(this.modelsPath, outputModelName);\n\n      const job: FineTuningJob = {,;\n        id: jobId,\n        jobName,\n        userId,\n        status: 'created','\n        baseModelName,\n        baseModelPath,\n        outputModelName,\n        outputModelPath,\n        datasetPath,\n        datasetFormat: this.detectDatasetFormat(datasetPath) as any,\n        hyperparameters: {,\n          learningRate: 0.0001,\n          batchSize: 4,\n          epochs: 3,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1,\n          ...hyperparameters\n        },\n        validationConfig: {,\n          splitRatio: 0.1,\n          validationMetrics: ['loss', 'perplexity', 'accuracy'],'\n          earlyStopping: true,\n          patience: 3,\n          ...validationConfig\n        },\n        progress: {,\n          currentEpoch: 0,\n          totalEpochs: hyperparameters.epochs || 3,\n          currentStep: 0,\n          totalSteps: 0,\n          progressPercentage: 0,\n          lastUpdateTime: new Date()\n        },\n        metrics: {,\n          trainingLoss: [],\n          validationLoss: [],\n          trainingAccuracy: [],\n          validationAccuracy: [],\n          learningRates: [],\n          gradientNorms: [],\n          perplexity: [],\n          epochTimes: []\n        },\n        evaluation: null,\n        resourceUsage: {,\n          memoryUsageMB: 0,\n          gpuUtilizationPercentage: 0\n        },\n        createdAt: new Date(),\n        updatedAt: new Date()\n      };\n\n      // Save to database\n      await this.saveJobToDatabase(job);\n\n      // Add to queue\n      await this.addJobToQueue(job);\n\n      log.info('✅ Fine-tuning job created', LogContext.AI, {')\n        jobId,\n        jobName,\n        baseModel: baseModelName,\n        dataset: basename(datasetPath)\n      });\n\n      this.emit('jobCreated', job);'\n      return job;\n\n    } catch (error) {\n      log.error('❌ Failed to create fine-tuning job', LogContext.AI, { error, jobName });'\n      throw error;\n    }\n  }\n\n  /**\n   * Start a fine-tuning job\n   */\n  public async startFineTuningJob(jobId: string): Promise<void> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job) {\n        throw new Error(`Job not found: ${jobId}`);\n      }\n\n      if (job.status !== 'created') {'\n        throw new Error(`Job cannot be started from status: ${job.status}`);\n      }\n\n      log.info('🚀 Starting fine-tuning job', LogContext.AI, { jobId, jobName: job.jobName });'\n\n      // Update status\n      job.status = 'preparing';'\n      job.startedAt = new Date();\n      await this.updateJobInDatabase(job);\n\n      // Create training script\n      const trainingScript = await this.createTrainingScript(job);\n      const scriptPath = join(this.tempPath, `train_${jobId}.py`);\n      writeFileSync(scriptPath, trainingScript);\n\n      // Start training process\n      const pythonProcess = spawn('python3', [scriptPath], {');\n        stdio: ['pipe', 'pipe', 'pipe'],'\n        cwd: dirname(job.baseModelPath),\n        env: {\n          ...process.env,\n          PYTHONPATH: join(__dirname, '..', '..'),'\n          MLX_JOB_ID: jobId\n        }\n      });\n\n      this.activeJobs.set(jobId, pythonProcess);\n\n      // Handle process output\n      this.setupProcessHandlers(jobId, pythonProcess, job);\n\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n\n      this.emit('jobStarted', job);'\n\n    } catch (error) {\n      log.error('❌ Failed to start fine-tuning job', LogContext.AI, { error, jobId });'\n      const job = await this.getJob(jobId);\n      if (job) {\n        job.status = 'failed';'\n        job.error(= {)\n          message: error instanceof Error ? error.message : String(error),\n          details: error,\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n        this.emit('jobFailed', job);'\n      }\n      throw error;\n    }\n  }\n\n  /**\n   * Pause a running fine-tuning job\n   */\n  public async pauseJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'training') {'\n      throw new Error(`Cannot pause job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGSTOP'); // Pause the process'\n      job.status = 'paused';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobPaused', job);'\n      \n      log.info('⏸️ Job paused', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Resume a paused fine-tuning job\n   */\n  public async resumeJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job || job.status !== 'paused') {'\n      throw new Error(`Cannot resume job ${jobId} with status ${job?.status}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGCONT'); // Resume the process'\n      job.status = 'training';'\n      await this.updateJobInDatabase(job);\n      this.emit('jobResumed', job);'\n      \n      log.info('▶️ Job resumed', LogContext.AI, { jobId });'\n    }\n  }\n\n  /**\n   * Cancel a fine-tuning job\n   */\n  public async cancelJob(jobId: string): Promise<void> {\n    const job = await this.getJob(jobId);\n    if (!job) {\n      throw new Error(`Job not found: ${jobId}`);\n    }\n\n    const process = this.activeJobs.get(jobId);\n    if (process) {\n      process.kill('SIGTERM');'\n      this.activeJobs.delete(jobId);\n    }\n\n    job.status = 'cancelled';'\n    job.completedAt = new Date();\n    await this.updateJobInDatabase(job);\n    await this.removeJobFromQueue(jobId);\n\n    this.emit('jobCancelled', job);'\n    log.info('🛑 Job cancelled', LogContext.AI, { jobId });'\n  }\n\n  // ============================================================================\n  // Hyperparameter Optimization\n  // ============================================================================\n\n  /**\n   * Run hyperparameter optimization experiment\n   */\n  public async runHyperparameterOptimization()\n    experimentName: string,\n    baseJobId: string,\n    userId: string,\n    optimizationMethod: 'grid_search' | 'random_search' | 'bayesian' | 'genetic','\n    parameterSpace: ParameterSpace,\n    maxTrials: number = 20\n  ): Promise<HyperparameterOptimization> {\n    try {\n      log.info('🔬 Starting hyperparameter optimization', LogContext.AI, {')\n        experimentName,\n        method: optimizationMethod,\n        maxTrials\n      });\n\n      const baseJob = await this.getJob(baseJobId);\n      if (!baseJob) {\n        throw new Error(`Base job not found: ${baseJobId}`);\n      }\n\n      const experiment: HyperparameterOptimization = {,;\n        id: uuidv4(),\n        experimentName,\n        baseJobId,\n        userId,\n        optimizationMethod,\n        parameterSpace,\n        status: 'created','\n        trials: [],\n        createdAt: new Date()\n      };\n\n      // Generate parameter combinations\n      const parameterCombinations = this.generateParameterCombinations();\n        parameterSpace,\n        optimizationMethod,\n        maxTrials\n      );\n\n      experiment.status = 'running';'\n      await this.saveExperimentToDatabase(experiment);\n\n      // Run trials\n      for (let i = 0; i < parameterCombinations.length; i++) {\n        const params = parameterCombinations[i];\n        \n        log.info(`🧪 Running trial ${i + 1}/${parameterCombinations.length}`, LogContext.AI, { params });\n\n        const trial = await this.runOptimizationTrial(experiment, params, baseJob);\n        experiment.trials.push(trial);\n\n        // Update best trial\n        if (!experiment.bestTrial || this.compareTrialMetrics(trial, experiment.bestTrial) > 0) {\n          experiment.bestTrial = trial;\n        }\n\n        await this.updateExperimentInDatabase(experiment);\n        \n        // Early stopping for Bayesian optimization\n        if (optimizationMethod === 'bayesian' && this.shouldStopOptimization(experiment)) {'\n          log.info('🛑 Early stopping optimization', LogContext.AI);'\n          break;\n        }\n      }\n\n      experiment.status = 'completed';'\n      experiment.completedAt = new Date();\n      await this.updateExperimentInDatabase(experiment);\n\n      log.info('✅ Hyperparameter optimization completed', LogContext.AI, {')\n        experimentName,\n        totalTrials: experiment.trials.length,\n        bestScore: experiment.bestTrial?.metrics.accuracy || 0\n      });\n\n      this.emit('optimizationCompleted', experiment);'\n      return experiment;\n\n    } catch (error) {\n      log.error('❌ Hyperparameter optimization failed', LogContext.AI, { error, experimentName });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Model Evaluation\n  // ============================================================================\n\n  /**\n   * Evaluate a fine-tuned model\n   */\n  public async evaluateModel()\n    jobId: string,\n    modelPath: string,\n    evaluationType: 'training' | 'validation' | 'test' | 'final','\n    evaluationConfig: Partial<EvaluationConfig> = {}\n  ): Promise<ModelEvaluation> {\n    try {\n      log.info('📊 Evaluating model', LogContext.AI, { jobId, modelPath, evaluationType });'\n\n      const config: EvaluationConfig = {,;\n        numSamples: 100,\n        maxTokens: 256,\n        temperature: 0.7,\n        topP: 0.9,\n        ...evaluationConfig\n      };\n\n      // Load test dataset\n      const testData = await this.loadTestDataset(config.testDatasetPath);\n      const samples = testData.slice(0, config.numSamples);\n\n      // Run evaluation\n      const metrics = await this.calculateEvaluationMetrics(modelPath, samples, config);\n      const sampleOutputs = await this.generateSampleOutputs(modelPath, samples.slice(0, 10), config);\n\n      const evaluation: ModelEvaluation = {,;\n        id: uuidv4(),\n        jobId,\n        modelPath,\n        evaluationType,\n        metrics,\n        sampleOutputs,\n        evaluationConfig: config,\n        createdAt: new Date()\n      };\n\n      // Save to database\n      await this.saveEvaluationToDatabase(evaluation);\n\n      log.info('✅ Model evaluation completed', LogContext.AI, {')\n        jobId,\n        evaluationType,\n        accuracy: metrics.accuracy,\n        perplexity: metrics.perplexity\n      });\n\n      this.emit('evaluationCompleted', evaluation);'\n      return evaluation;\n\n    } catch (error) {\n      log.error('❌ Model evaluation failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Progress Monitoring\n  // ============================================================================\n\n  /**\n   * Get real-time job progress\n   */\n  public async getJobProgress(jobId: string): Promise<JobProgress | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.progress: null;\n  }\n\n  /**\n   * Get job training metrics\n   */\n  public async getJobMetrics(jobId: string): Promise<TrainingMetrics | null> {\n    const job = await this.getJob(jobId);\n    return job ? job.metrics: null;\n  }\n\n  /**\n   * Subscribe to job progress updates\n   */\n  public subscribeToJobProgress(jobId: string, callback: (progress: JobProgress) => void): () => void {\n    const handler = (job: FineTuningJob) => {\n      if (job.id === jobId) {\n        callback(job.progress);\n      }\n    };\n\n    this.on('jobProgressUpdated', handler);'\n    \n    return () => {\n      this.off('jobProgressUpdated', handler);'\n    };\n  }\n\n  // ============================================================================\n  // Model Export and Deployment\n  // ============================================================================\n\n  /**\n   * Export a fine-tuned model\n   */\n  public async exportModel()\n    jobId: string,\n    exportFormat: 'mlx' | 'gguf' | 'safetensors' = 'mlx',';\n    exportPath?: string;\n  ): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot export model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const outputPath = exportPath || join(this.modelsPath, 'exports', `${job.outputModelName}.${exportFormat}`);';\n      \n      log.info('📦 Exporting model', LogContext.AI, { jobId, format: exportFormat, outputPath });'\n\n      // Create export script\n      const exportScript = this.createModelExportScript(job.outputModelPath, outputPath, exportFormat);\n      const scriptPath = join(this.tempPath, `export_${jobId}.py`);\n      writeFileSync(scriptPath, exportScript);\n\n      // Run export\n      await this.runPythonScript(scriptPath);\n\n      log.info('✅ Model exported successfully', LogContext.AI, { jobId, outputPath });'\n      \n      this.emit('modelExported', { jobId, outputPath, format: exportFormat });'\n      return outputPath;\n\n    } catch (error) {\n      log.error('❌ Model export failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy a fine-tuned model for inference\n   */\n  public async deployModel(jobId: string, deploymentName?: string): Promise<string> {\n    try {\n      const job = await this.getJob(jobId);\n      if (!job || job.status !== 'completed') {'\n        throw new Error(`Cannot deploy model for job ${jobId} with status ${job?.status}`);\n      }\n\n      const deploymentId = deploymentName || `${job.outputModelName}_deployment`;\n      \n      log.info('🚀 Deploying model', LogContext.AI, { jobId, deploymentId });'\n\n      // Copy model to deployment directory\n      const deploymentPath = join(this.modelsPath, 'deployed', deploymentId);';\n      await this.copyDirectory(job.outputModelPath, deploymentPath);\n\n      // Register with MLX service\n      // Note: This would integrate with the existing MLX service for inference\n      \n      log.info('✅ Model deployed successfully', LogContext.AI, { jobId, deploymentId });'\n      \n      this.emit('modelDeployed', { jobId, deploymentId, deploymentPath });'\n      return deploymentId;\n\n    } catch (error) {\n      log.error('❌ Model deployment failed', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  // ============================================================================\n  // Queue Management\n  // ============================================================================\n\n  /**\n   * Get current job queue status\n   */\n  public async getQueueStatus(): Promise<{\n    running: FineTuningJob[];,\n    queued: FineTuningJob[];\n    totalCapacity: number;,\n    availableCapacity: number;\n  }> {\n    const runningJobs = Array.from(this.activeJobs.keys());\n    const running = await Promise.all(runningJobs.map(id => this.getJob(id))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n    \n    const queued = this.jobQueue;\n      .filter(item => item.status === 'queued')'\n      .sort((a, b) => a.priority - b.priority || a.queuePosition - b.queuePosition);\n    \n    const queuedJobs = await Promise.all(queued.map(item => this.getJob(item.jobId))).then(jobs => jobs.filter(Boolean) as FineTuningJob[]);\n\n    return {\n      running,\n      queued: queuedJobs,\n      totalCapacity: this.maxConcurrentJobs,\n      availableCapacity: this.maxConcurrentJobs - this.activeJobs.size\n    };\n  }\n\n  /**\n   * Set job priority in queue\n   */\n  public async setJobPriority(jobId: string, priority: number): Promise<void> {\n    const queueItem = this.jobQueue.find(item => item.jobId === jobId);\n    if (queueItem) {\n      queueItem.priority = Math.max(1, Math.min(10, priority));\n      await this.updateJobQueueInDatabase();\n      \n      log.info('📋 Job priority updated', LogContext.AI, { jobId, priority });'\n    }\n  }\n\n  // ============================================================================\n  // Utility Methods\n  // ============================================================================\n\n  /**\n   * List all jobs for a user\n   */\n  public async listJobs(userId: string, status?: JobStatus): Promise<FineTuningJob[]> {\n    try {\n      let query = this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('user_id', userId)'\n        .order('created_at', { ascending: false });'\n\n      if (status) {\n        query = query.eq('status', status);'\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data.map(this.mapDatabaseJobToJob);\n    } catch (error) {\n      log.error('❌ Failed to list jobs', LogContext.AI, { error, userId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get job by ID\n   */\n  public async getJob(jobId: string): Promise<FineTuningJob | null> {\n    try {\n      const { data, error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*')'\n        .eq('id', jobId)'\n        .single();\n\n      if (error || !data) return null;\n      \n      return this.mapDatabaseJobToJob(data);\n    } catch (error) {\n      log.error('❌ Failed to get job', LogContext.AI, { error, jobId });'\n      return null;\n    }\n  }\n\n  /**\n   * Delete a job and its associated data\n   */\n  public async deleteJob(jobId: string): Promise<void> {\n    try {\n      // Cancel if running\n      if (this.activeJobs.has(jobId)) {\n        await this.cancelJob(jobId);\n      }\n\n      // Remove from queue\n      await this.removeJobFromQueue(jobId);\n\n      // Delete from database (cascades to related tables)\n      const { error } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .delete()\n        .eq('id', jobId);'\n\n      if (error) throw error;\n\n      // Clean up files\n      const job = await this.getJob(jobId);\n      if (job && existsSync(job.outputModelPath)) {\n        await this.deleteDirectory(job.outputModelPath);\n      }\n\n      log.info('🗑️ Job deleted', LogContext.AI, { jobId });'\n      this.emit('jobDeleted', { jobId });'\n\n    } catch (error) {\n      log.error('❌ Failed to delete job', LogContext.AI, { error, jobId });'\n      throw error;\n    }\n  }\n\n  /**\n   * Get service health status\n   */\n  public async getHealthStatus(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';,'\n    activeJobs: number;\n    queuedJobs: number;,\n    totalJobs: number;\n    resourceUsage: {,\n      memoryUsageMB: number;\n      diskUsageMB: number;\n    };\n    lastError?: string;\n  }> {\n    try {\n      const activeJobsCount = this.activeJobs.size;\n      const queuedJobsCount = this.jobQueue.filter(item => item.status === 'queued').length;';\n      \n      // Get total job count from database\n      const { count } = await this.supabase;\n        .from('mlx_fine_tuning_jobs')'\n        .select('*', { count: 'exact', head: true });'\n\n      const totalJobs = count || 0;\n\n      // Calculate resource usage\n      const memoryUsage = process.memoryUsage();\n      const diskUsage = this.calculateDiskUsage();\n\n      const status = activeJobsCount > this.maxConcurrentJobs ? 'degraded' : 'healthy';';\n\n      return {\n        status,\n        activeJobs: activeJobsCount,\n        queuedJobs: queuedJobsCount,\n        totalJobs,\n        resourceUsage: {,\n          memoryUsageMB: Math.round(memoryUsage.heapUsed / 1024 / 1024),\n          diskUsageMB: diskUsage\n        }\n      };\n\n    } catch (error) {\n      log.error('❌ Health check failed', LogContext.AI, { error });'\n      return {\n        status: 'unhealthy','\n        activeJobs: 0,\n        queuedJobs: 0,\n        totalJobs: 0,\n        resourceUsage: {, memoryUsageMB: 0, diskUsageMB: 0 },\n        lastError: error instanceof Error ? error.message : String(error)\n      };\n    }\n  }\n\n  // ============================================================================\n  // Private Helper Methods\n  // ============================================================================\n\n  private ensureDirectories(): void {\n    const dirs = [this.modelsPath, this.datasetsPath, this.tempPath, \n                  join(this.modelsPath, 'exports'), join(this.modelsPath, 'deployed')];'\n    \n    for (const dir of dirs) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n    }\n  }\n\n  private detectDatasetFormat(filePath: string): 'json' | 'jsonl' | 'csv' {'\n    const ext = extname(filePath).toLowerCase();\n    switch (ext) {\n      case '.json': return 'json';'\n      case '.jsonl': return 'jsonl';'\n      case '.csv': return 'csv';'\n      default: return 'jsonl';'\n    }\n  }\n\n  private async readDatasetFile(filePath: string, format: string): Promise<any[]> {\n    const content = readFileSync(filePath, 'utf8');';\n    \n    switch (format) {\n      case 'json':'\n        return JSON.parse(content);\n      case 'jsonl':'\n        return content.split('n').filter(line => line.trim()).map(line => JSON.parse(line));';\n      case 'csv':'\n        // Simple CSV parsing - in production, use a proper CSV library\n        const lines = content.split('\\n').filter(line => line.trim());';\n        const headers = lines[0].split(',');';\n        return lines.slice(1).map(line => {);\n          const values = line.split(',');';\n          const obj: any = {};\n          headers.forEach((header, i) => {\n            obj[header.trim()] = values[i]?.trim();\n          });\n          return obj;\n        });\n      default: throw new Error(`Unsupported, format: ${format}`);\n    }\n  }\n\n  private async preprocessDataset(data: any[], config: PreprocessingConfig): Promise<any[]> {\n    let processed = [...data];\n\n    // Remove duplicates\n    if (config.removeDuplicates) {\n      const seen = new Set();\n      processed = processed.filter(item => {)\n        const key = `${item.input}|${item.output}`;\n        if (seen.has(key)) return false;\n        seen.add(key);\n        return true;\n      });\n    }\n\n    // Shuffle\n    if (config.shuffle) {\n      for (let i = processed.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [processed[i], processed[j]] = [processed[j], processed[i]];\n      }\n    }\n\n    // Truncate if needed\n    if (config.maxLength > 0) {\n      processed = processed.map(item => ({)\n        ...item,\n        input: config.truncation && item.input.length > config.maxLength \n          ? item.input.substring(0, config.maxLength) \n          : item.input,\n        output: config.truncation && item.output.length > config.maxLength \n          ? item.output.substring(0, config.maxLength) \n          : item.output\n      }));\n    }\n\n    return processed;\n  }\n\n  private async calculateDatasetStatistics(data: any[]): Promise<DatasetStatistics> {\n    const lengths = data.map(item => (item.input + ' ' + item.output).length);';\n    const allText = data.map(item => item.input + ' ' + item.output).join(' ');';\n    const tokens = allText.split(/s+/);\n    const uniqueTokens = new Set(tokens);\n\n    // Simple token frequency calculation\n    const tokenFreq: { [key: string]: number } = {};\n    tokens.forEach(token => {)\n      tokenFreq[token] = (tokenFreq[token] || 0) + 1;\n    });\n\n    // Length distribution\n    const lengthDistribution: { [key: string]: number } = {};\n    lengths.forEach(length => {)\n      const bucket = Math.floor(length / 100) * 100;\n      lengthDistribution[bucket.toString()] = (lengthDistribution[bucket.toString()] || 0) + 1;\n    });\n\n    return {\n      avgLength: lengths.reduce((a, b) => a + b, 0) / lengths.length,\n      minLength: Math.min(...lengths),\n      maxLength: Math.max(...lengths),\n      vocabSize: uniqueTokens.size,\n      uniqueTokens: uniqueTokens.size,\n      lengthDistribution,\n      tokenFrequency: tokenFreq\n    };\n  }\n\n  private async saveProcessedDataset(data: any[], filePath: string): Promise<void> {\n    const content = data.map(item => JSON.stringify(item)).join('\\n');';\n    writeFileSync(filePath, content, 'utf8');'\n  }\n\n  private createTrainingScript(job: FineTuningJob): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nMLX Fine-tuning Script\nGenerated training script for job: ${job.id}\n\"\"\"\"\n\nimport os;\nimport sys;\nimport json;\nimport time;\nimport mlx.core as mx;\nimport mlx.nn as nn;\nfrom mlx_lm import load, generate, models, utils\nfrom mlx_lm.utils import load_dataset, create_training_loop\nfrom pathlib import Path\n\nclass MLXFineTuner: def __init__(self, job_config):\n        self.job_config = job_config\n        self.job_id = job_config['id']'\n        self.model = None\n        self.tokenizer = None\n        \n    def load_model(self):\n        \"\"\"Load the base model\"\"\"\"\n        print(f\"Loading base model: {self.job_config['baseModelPath']}\")'\"\n        self.model, self.tokenizer = load(self.job_config['baseModelPath'])'\n        \n    def load_dataset(self):\n        \"\"\"Load and prepare training dataset\"\"\"\"\n        print(f\"Loading dataset: {self.job_config['datasetPath']}\")'\"\n        \n        with open(self.job_config['datasetPath'], 'r') as f: '\n            data = [json.loads(line) for line in f]\n        \n        # Split into train/val\n        split_idx = int(len(data) * (1 - self.job_config['validationConfig']['splitRatio']))'\n        train_data = data[:split_idx]\n        val_data = data[split_idx: ]\n        \n        return train_data, val_data;\n        \n    def train(self):\n        \"\"\"Run the fine-tuning process\"\"\"\"\n        try: print(f\"Starting fine-tuning job {self.job_id}\")\"\n            \n            # Load model and data\n            self.load_model()\n            train_data, val_data = self.load_dataset()\n            \n            # Training configuration\n            config = self.job_config['hyperparameters']'\n            \n            # Create optimizer\n            optimizer = mx.optimizers.Adam(learning_rate=config['learningRate'])'\n            \n            # Training loop\n            for epoch in range(config['epochs']):'\n                print(f\"PROGRESS|{epoch + 1}|{config['epochs']}|0|100|0.0\")'\"\n                \n                # Simulate training (replace with actual MLX training code)\n                epoch_loss = 2.5 - (epoch * 0.3)  # Decreasing loss\n                val_loss = 2.3 - (epoch * 0.25)   # Validation loss\n                \n                # Report metrics\n                metrics = {\n                    'epoch': epoch + 1,'\n                    'training_loss': epoch_loss,'\n                    'validation_loss': val_loss,'\n                    'learning_rate': config['learningRate'],'\n                    'timestamp': time.time()'\n                }\n                print(f\"METRICS|{json.dumps(metrics)}\")\"\n                \n                # Simulate training time\n                time.sleep(5)\n            \n            # Save fine-tuned model\n            output_path = self.job_config['outputModelPath']'\n            os.makedirs(output_path, exist_ok=True)\n            \n            # In real implementation, save the actual fine-tuned model\n            print(f\"Saving model to: {output_path}\")\"\n            print(\"TRAINING_COMPLETE\")\"\n            \n        except Exception as e: print(f\"TRAINING_ERROR|{str(e)}\")\"\n            sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    job_config = ${JSON.stringify(job, null, 2)}\n    \n    trainer = MLXFineTuner(job_config)\n    trainer.train()\n`;\n  }\n\n  private setupProcessHandlers(jobId: string, process: ChildProcess, job: FineTuningJob): void {\n    if (!process.stdout || !process.stderr) return;\n\n    process.stdout.on('data', async (data) => {'\n      const output = data.toString();\n      await this.handleTrainingOutput(jobId, output, job);\n    });\n\n    process.stderr.on('data', (data) => {'\n      log.error('Training process error', LogContext.AI, { jobId, error: data.toString() });'\n    });\n\n    process.on('exit', async (code) => {'\n      this.activeJobs.delete(jobId);\n      \n      if (code === 0) {\n        job.status = 'completed';'\n        job.completedAt = new Date();\n      } else {\n        job.status = 'failed';'\n        job.error(= {)\n          message: `Training process exited with code ${code}`,\n          details: {, exitCode: code },\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n      }\n      \n      await this.updateJobInDatabase(job);\n      this.emit(job.status === 'completed' ? 'jobCompleted' : 'jobFailed', job);'\n    });\n  }\n\n  private async handleTrainingOutput(jobId: string, output: string, job: FineTuningJob): Promise<void> {\n    const lines = output.split('n').filter(line => line.trim());';\n    \n    for (const line of lines) {\n      if (line.startsWith('PROGRESS|')) {'\n        const [, currentEpoch, totalEpochs, currentStep, totalSteps, percentage] = line.split('|');';\n        \n        job.progress = {\n          currentEpoch: parseInt(currentEpoch),\n          totalEpochs: parseInt(totalEpochs),\n          currentStep: parseInt(currentStep),\n          totalSteps: parseInt(totalSteps),\n          progressPercentage: parseFloat(percentage),\n          lastUpdateTime: new Date()\n        };\n        \n        await this.updateJobInDatabase(job);\n        this.emit('jobProgressUpdated', job);'\n        \n      } else if (line.startsWith('METRICS|')) {'\n        const metricsJson = line.substring(8);\n        try {\n          const metrics = JSON.parse(metricsJson);\n          \n          // Update training metrics\n          job.metrics.trainingLoss.push(metrics.training_loss);\n          job.metrics.validationLoss.push(metrics.validation_loss);\n          job.metrics.learningRates.push(metrics.learning_rate);\n          \n          if (metrics.perplexity) {\n            job.metrics.perplexity.push(metrics.perplexity);\n          }\n          \n          await this.updateJobInDatabase(job);\n          this.emit('jobMetricsUpdated', job);'\n          \n        } catch (error) {\n          log.error('Failed to parse metrics', LogContext.AI, { error, line });'\n        }\n        \n      } else if (line === 'TRAINING_COMPLETE') {'\n        log.info('✅ Training completed successfully', LogContext.AI, { jobId });'\n        \n      } else if (line.startsWith('TRAINING_ERROR|')) {'\n        const errorMsg = line.substring(15);\n        job.error(= {)\n          message: errorMsg,\n          details: {, source: 'training_process' },'\n          retryCount: 0,\n          maxRetries: 3,\n          recoverable: true\n        };\n        await this.updateJobInDatabase(job);\n      }\n    }\n  }\n\n  private generateParameterCombinations()\n    paramSpace: ParameterSpace,\n    method: string,\n    maxTrials: number\n  ): Hyperparameters[] {\n    const combinations: Hyperparameters[] = [];\n    \n    if (method === 'grid_search') {'\n      // Simple grid search implementation\n      const learningRates = Array.isArray(paramSpace.learningRate) \n        ? paramSpace.learningRate: [paramSpace.learningRate.min, paramSpace.learningRate.max];\n      const batchSizes = paramSpace.batchSize;\n      const epochs = Array.isArray(paramSpace.epochs) \n        ? paramSpace.epochs: [paramSpace.epochs.min, paramSpace.epochs.max];\n\n      for (const lr of learningRates) {\n        for (const bs of batchSizes) {\n          for (const ep of epochs) {\n            if (combinations.length >= maxTrials) break;\n            \n            combinations.push({)\n              learningRate: lr,\n              batchSize: bs,\n              epochs: ep,\n              maxSeqLength: 2048,\n              gradientAccumulation: 1,\n              warmupSteps: 100,\n              weightDecay: 0.01,\n              dropout: 0.1\n            });\n          }\n        }\n      }\n    } else if (method === 'random_search') {'\n      // Random search implementation\n      for (let i = 0; i < maxTrials; i++) {\n        const lr = Array.isArray(paramSpace.learningRate);\n          ? paramSpace.learningRate[Math.floor(Math.random() * paramSpace.learningRate.length)]\n          : paramSpace.learningRate.min + Math.random() * (paramSpace.learningRate.max - paramSpace.learningRate.min);\n        \n        const bs = paramSpace.batchSize[Math.floor(Math.random() * paramSpace.batchSize.length)];\n        \n        const epochs = Array.isArray(paramSpace.epochs);\n          ? paramSpace.epochs[Math.floor(Math.random() * paramSpace.epochs.length)]\n          : Math.floor(paramSpace.epochs.min + Math.random() * (paramSpace.epochs.max - paramSpace.epochs.min + 1));\n\n        combinations.push({)\n          learningRate: lr,\n          batchSize: bs,\n          epochs: epochs,\n          maxSeqLength: 2048,\n          gradientAccumulation: 1,\n          warmupSteps: 100,\n          weightDecay: 0.01,\n          dropout: 0.1\n        });\n      }\n    }\n    \n    return combinations;\n  }\n\n  private async runOptimizationTrial()\n    experiment: HyperparameterOptimization,\n    parameters: Hyperparameters,\n    baseJob: FineTuningJob\n  ): Promise<OptimizationTrial> {\n    const trialId = uuidv4();\n    const trial: OptimizationTrial = {,;\n      id: trialId,\n      parameters,\n      metrics: {, perplexity: 0, loss: 0, accuracy: 0 },\n      status: 'running','\n      startTime: new Date()\n    };\n\n    try {\n      // Create trial job\n      const trialJob = await this.createFineTuningJob();\n        `${baseJob.jobName}_trial_${trialId}`,\n        baseJob.userId,\n        baseJob.baseModelName,\n        baseJob.baseModelPath,\n        baseJob.datasetPath,\n        parameters,\n        baseJob.validationConfig\n      );\n\n      trial.jobId = trialJob.id;\n\n      // Start training\n      await this.startFineTuningJob(trialJob.id);\n\n      // Wait for completion (simplified - in practice, this would be async)\n      let completed = false;\n      let attempts = 0;\n      const maxAttempts = 1200; // 20 minutes timeout;\n\n      while (!completed && attempts < maxAttempts) {\n        await new Promise(resolve => setTimeout(resolve, 1000));\n        const currentJob = await this.getJob(trialJob.id);\n        \n        if (currentJob?.status === 'completed') {'\n          completed = true;\n          \n          // Extract final metrics\n          const finalMetrics = currentJob.metrics;\n          trial.metrics = {\n            perplexity: finalMetrics.perplexity[finalMetrics.perplexity.length - 1] || 0,\n            loss: finalMetrics.validationLoss[finalMetrics.validationLoss.length - 1] || 0,\n            accuracy: finalMetrics.validationAccuracy?.[finalMetrics.validationAccuracy.length - 1] || 0\n          };\n          \n          trial.status = 'completed';'\n          trial.endTime = new Date();\n          \n        } else if (currentJob?.status === 'failed') {'\n          trial.status = 'failed';'\n          trial.endTime = new Date();\n          completed = true;\n        }\n        \n        attempts++;\n      }\n\n      if (!completed) {\n        // Timeout\n        await this.cancelJob(trialJob.id);\n        trial.status = 'failed';'\n        trial.endTime = new Date();\n      }\n\n    } catch (error) {\n      log.error('❌ Optimization trial failed', LogContext.AI, { error, trialId });'\n      trial.status = 'failed';'\n      trial.endTime = new Date();\n    }\n\n    return trial;\n  }\n\n  private compareTrialMetrics(trial1: OptimizationTrial, trial2: OptimizationTrial): number {\n    // Simple comparison based on accuracy (higher is better)\n    return trial1.metrics.accuracy - trial2.metrics.accuracy;\n  }\n\n  private shouldStopOptimization(experiment: HyperparameterOptimization): boolean {\n    // Simple early stopping logic\n    if (experiment.trials.length < 5) return false;\n    \n    const recentTrials = experiment.trials.slice(-5);\n    const improvements = recentTrials.slice(1).map((trial, i) => \n      trial.metrics.accuracy - recentTrials[i].metrics.accuracy\n    );\n    \n    return improvements.every(improvement => improvement < 0.001);\n  }\n\n  private async calculateEvaluationMetrics()\n    modelPath: string,\n    testData: any[],\n    config: EvaluationConfig\n  ): Promise<EvaluationMetrics> {\n    // Simplified evaluation - in practice, this would use the actual model\n    let totalLoss = 0;\n    let totalAccuracy = 0;\n    \n    for (const sample of testData) {\n      // Mock evaluation metrics\n      const sampleLoss = Math.random() * 2 + 0.5; // Random loss between 0.5-2.5;\n      const sampleAccuracy = Math.random() * 0.3 + 0.7; // Random accuracy between 0.7-1.0;\n      \n      totalLoss += sampleLoss;\n      totalAccuracy += sampleAccuracy;\n    }\n    \n    const avgLoss = totalLoss / testData.length;\n    const avgAccuracy = totalAccuracy / testData.length;\n    const perplexity = Math.exp(avgLoss);\n\n    return {\n      perplexity,\n      loss: avgLoss,\n      accuracy: avgAccuracy,\n      bleuScore: Math.random() * 0.4 + 0.3, // Mock BLEU score\n      rougeScores: {,\n        rouge1: Math.random() * 0.3 + 0.4,\n        rouge2: Math.random() * 0.2 + 0.3,\n        rougeL: Math.random() * 0.3 + 0.35\n      }\n    };\n  }\n\n  private async generateSampleOutputs()\n    modelPath: string,\n    samples: any[],\n    config: EvaluationConfig\n  ): Promise<SampleOutput[]> {\n    return samples.map(sample => ({);\n      input: sample.input,\n      output: `Generated response, for: ${sample.input.substring(0, 50)}...`, // Mock output\n      reference: sample.output,\n      confidence: Math.random() * 0.3 + 0.7\n    }));\n  }\n\n  private async loadTestDataset(datasetPath?: string): Promise<any[]> {\n    if (!datasetPath || !existsSync(datasetPath)) {\n      // Return mock test data\n      return [;\n        { input: \"Test question 1\", output: \"Test answer 1\" },\"\n        { input: \"Test question 2\", output: \"Test answer 2\" }\"\n      ];\n    }\n    \n    const format = this.detectDatasetFormat(datasetPath);\n    return this.readDatasetFile(datasetPath, format);\n  }\n\n  private createModelExportScript(modelPath: string, outputPath: string, format: string): string {\n    return `#!/usr/bin/env python3;\n\"\"\"\"\nModel Export Script\nExport MLX model to ${format} format\n\"\"\"\"\n\nimport os;\nimport sys;\nimport mlx.core as mx;\nfrom mlx_lm import load\nfrom pathlib import Path\n\ndef export_model():\n    try: print(f\"Loading model, from: ${modelPath}\")\"\n        model, tokenizer = load(\"${modelPath}\")\"\n        \n        print(f\"Exporting to: ${outputPath}\")\"\n        os.makedirs(os.path.dirname(\"${outputPath}\"), exist_ok=True)\"\n        \n        # Export based on format\n        if \"${format}\" == \"mlx\":\"\n            # Copy MLX format (already in correct format)\n            import shutil;\n            shutil.copytree(\"${modelPath}\", \"${outputPath}\")\"\n        elif \"${format}\" == \"gguf\":\"\n            # Convert to GGUF format (simplified)\n            print(\"GGUF export not yet implemented\")\"\n        elif \"${format}\" == \"safetensors\":\"\n            # Convert to SafeTensors format (simplified)\n            print(\"SafeTensors export not yet implemented\")\"\n        \n        print(\"Export completed successfully\")\"\n        \n    except Exception as e: print(f\"Export, failed: {e}\")\"\n        sys.exit(1)\n\nif __name__ == \"__main__\":\"\n    export_model();\n`;\n  }\n\n  private async runPythonScript(scriptPath: string): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const process = spawn('python3', [scriptPath], { stdio: 'pipe' });';\n      \n      process.on('exit', (code) => {'\n        if (code === 0) {\n          resolve();\n        } else {\n          reject(new Error(`Script exited with code ${code}`));\n        }\n      });\n      \n      process.on('error', reject);'\n    });\n  }\n\n  private startQueueProcessor(): void {\n    if (this.isProcessingQueue) return;\n    \n    this.isProcessingQueue = true;\n    \n    const processQueue = async () => {\n      try {\n        if (this.activeJobs.size >= this.maxConcurrentJobs) {\n          return;\n        }\n        \n        const nextJob = this.jobQueue.find(item =>);\n          item.status === 'queued' && '\n          this.canStartJob(item)\n        );\n        \n        if (nextJob) {\n          nextJob.status = 'running';'\n          nextJob.startedAt = new Date();\n          await this.updateJobQueueInDatabase();\n          \n          const job = await this.getJob(nextJob.jobId);\n          if (job) {\n            await this.startFineTuningJob(job.id);\n          }\n        }\n      } catch (error) {\n        log.error('❌ Queue processing error', LogContext.AI, { error });'\n      }\n    };\n    \n    // Process queue every 10 seconds\n    setInterval(processQueue, 10000);\n  }\n\n  private canStartJob(queueItem: JobQueue): boolean {\n    // Check dependencies\n    if (queueItem.dependsOnJobIds.length > 0) {\n      // Check if all dependencies are completed\n      // Simplified implementation\n      return true;\n    }\n    \n    // Check resource availability (simplified)\n    return true;\n  }\n\n  private async addJobToQueue(job: FineTuningJob): Promise<void> {\n    const queueItem: JobQueue = {,;\n      id: uuidv4(),\n      jobId: job.id,\n      priority: 5,\n      queuePosition: this.jobQueue.length,\n      estimatedResources: {,\n        memoryMB: 8192,\n        gpuMemoryMB: 4096,\n        durationMinutes: job.hyperparameters.epochs * 20\n      },\n      dependsOnJobIds: [],\n      status: 'queued','\n      createdAt: new Date()\n    };\n    \n    this.jobQueue.push(queueItem);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private async removeJobFromQueue(jobId: string): Promise<void> {\n    this.jobQueue = this.jobQueue.filter(item => item.jobId !== jobId);\n    await this.updateJobQueueInDatabase();\n  }\n\n  private calculateDiskUsage(): number {\n    try {\n      const paths = [this.modelsPath, this.datasetsPath, this.tempPath];\n      let totalSize = 0;\n      \n      for (const path of paths) {\n        if (existsSync(path)) {\n          totalSize += this.getDirectorySize(path);\n        }\n      }\n      \n      return Math.round(totalSize / 1024 / 1024); // MB;\n    } catch {\n      return 0;\n    }\n  }\n\n  private getDirectorySize(dirPath: string): number {\n    let size = 0;\n    \n    try {\n      const files = readdirSync(dirPath);\n      \n      for (const file of files) {\n        const filePath = join(dirPath, file);\n        const stats = statSync(filePath);\n        \n        if (stats.isDirectory()) {\n          size += this.getDirectorySize(filePath);\n        } else {\n          size += stats.size;\n        }\n      }\n    } catch {\n      // Ignore errors\n    }\n    \n    return size;\n  }\n\n  private async copyDirectory(src: string, dest: string): Promise<void> {\n    // Simple directory copy implementation\n    if (!existsSync(src)) return;\n    \n    mkdirSync(dest, { recursive: true });\n    \n    const files = readdirSync(src);\n    for (const file of files) {\n      const srcPath = join(src, file);\n      const destPath = join(dest, file);\n      \n      if (statSync(srcPath).isDirectory()) {\n        await this.copyDirectory(srcPath, destPath);\n      } else {\n        const content = readFileSync(srcPath);\n        writeFileSync(destPath, content);\n      }\n    }\n  }\n\n  private async deleteDirectory(dirPath: string): Promise<void> {\n    if (!existsSync(dirPath)) return;\n    \n    const { rmSync } = await import('fs');';\n    rmSync(dirPath, { recursive: true, force: true });\n  }\n\n  // ============================================================================\n  // Database Operations\n  // ============================================================================\n\n  private async saveDatasetToDatabase(dataset: Dataset, userId: string): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_training_datasets')'\n      .insert({)\n        id: dataset.id,\n        dataset_name: dataset.name,\n        dataset_path: dataset.path,\n        user_id: userId,\n        format: dataset.format,\n        total_samples: dataset.totalSamples,\n        training_samples: dataset.trainingSamples,\n        validation_samples: dataset.validationSamples,\n        validation_results: dataset.validationResults,\n        preprocessing_config: dataset.preprocessingConfig,\n        statistics: dataset.statistics\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveJobToDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .insert({)\n        id: job.id,\n        job_name: job.jobName,\n        user_id: job.userId,\n        status: job.status,\n        base_model_name: job.baseModelName,\n        base_model_path: job.baseModelPath,\n        output_model_name: job.outputModelName,\n        output_model_path: job.outputModelPath,\n        dataset_path: job.datasetPath,\n        dataset_format: job.datasetFormat,\n        hyperparameters: job.hyperparameters,\n        validation_config: job.validationConfig,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        validation_metrics: {},\n        estimated_duration_minutes: job.resourceUsage.estimatedDurationMinutes,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateJobInDatabase(job: FineTuningJob): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_fine_tuning_jobs')'\n      .update({)\n        status: job.status,\n        current_epoch: job.progress.currentEpoch,\n        total_epochs: job.progress.totalEpochs,\n        current_step: job.progress.currentStep,\n        total_steps: job.progress.totalSteps,\n        progress_percentage: job.progress.progressPercentage,\n        training_metrics: job.metrics,\n        memory_usage_mb: job.resourceUsage.memoryUsageMB,\n        gpu_utilization_percentage: job.resourceUsage.gpuUtilizationPercentage,\n        actual_duration_minutes: job.resourceUsage.actualDurationMinutes,\n        error_message: job.error?.message,\n        error_details: job.error?.details,\n        retry_count: job.error?.retryCount || 0,\n        started_at: job.startedAt,\n        completed_at: job.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', job.id);'\n\n    if (error) throw error;\n  }\n\n  private async saveEvaluationToDatabase(evaluation: ModelEvaluation): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_model_evaluations')'\n      .insert({)\n        id: evaluation.id,\n        job_id: evaluation.jobId,\n        model_path: evaluation.modelPath,\n        evaluation_type: evaluation.evaluationType,\n        metrics: evaluation.metrics,\n        perplexity: evaluation.metrics.perplexity,\n        loss: evaluation.metrics.loss,\n        accuracy: evaluation.metrics.accuracy,\n        bleu_score: evaluation.metrics.bleuScore,\n        rouge_scores: evaluation.metrics.rougeScores,\n        sample_inputs: evaluation.sampleOutputs.map(s => s.input),\n        sample_outputs: evaluation.sampleOutputs.map(s => s.output),\n        sample_references: evaluation.sampleOutputs.map(s => s.reference || ''),'\n        evaluation_config: evaluation.evaluationConfig\n      });\n\n    if (error) throw error;\n  }\n\n  private async saveExperimentToDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .insert({)\n        id: experiment.id,\n        experiment_name: experiment.experimentName,\n        base_job_id: experiment.baseJobId,\n        user_id: experiment.userId,\n        optimization_method: experiment.optimizationMethod,\n        parameter_space: experiment.parameterSpace,\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt\n      });\n\n    if (error) throw error;\n  }\n\n  private async updateExperimentInDatabase(experiment: HyperparameterOptimization): Promise<void> {\n    const { error } = await this.supabase;\n      .from('mlx_hyperparameter_experiments')'\n      .update({)\n        status: experiment.status,\n        total_trials: experiment.trials.length,\n        completed_trials: experiment.trials.filter(t => t.status === 'completed').length,'\n        best_trial_id: experiment.bestTrial?.id,\n        best_metrics: experiment.bestTrial?.metrics || {},\n        trials: experiment.trials,\n        completed_at: experiment.completedAt,\n        updated_at: new Date().toISOString()\n      })\n      .eq('id', experiment.id);'\n\n    if (error) throw error;\n  }\n\n  private async updateJobQueueInDatabase(): Promise<void> {\n    // In a real implementation, you would update the queue in the database\n    // For now, we'll just log the queue status'\n    log.debug('📋 Job queue updated', LogContext.AI, { ')\n      queueLength: this.jobQueue.length,\n      running: this.activeJobs.size\n    });\n  }\n\n  private mapDatabaseJobToJob(dbJob: any): FineTuningJob {\n    return {\n      id: dbJob.id,\n      jobName: dbJob.job_name,\n      userId: dbJob.user_id,\n      status: dbJob.status,\n      baseModelName: dbJob.base_model_name,\n      baseModelPath: dbJob.base_model_path,\n      outputModelName: dbJob.output_model_name,\n      outputModelPath: dbJob.output_model_path,\n      datasetPath: dbJob.dataset_path,\n      datasetFormat: dbJob.dataset_format,\n      hyperparameters: dbJob.hyperparameters,\n      validationConfig: dbJob.validation_config,\n      progress: {,\n        currentEpoch: dbJob.current_epoch,\n        totalEpochs: dbJob.total_epochs,\n        currentStep: dbJob.current_step,\n        totalSteps: dbJob.total_steps,\n        progressPercentage: dbJob.progress_percentage,\n        lastUpdateTime: new Date(dbJob.updated_at)\n      },\n      metrics: dbJob.training_metrics,\n      evaluation: null, // Would be loaded separately\n      resourceUsage: {,\n        memoryUsageMB: dbJob.memory_usage_mb,\n        gpuUtilizationPercentage: dbJob.gpu_utilization_percentage,\n        estimatedDurationMinutes: dbJob.estimated_duration_minutes,\n        actualDurationMinutes: dbJob.actual_duration_minutes\n      },\n      error: dbJob.error_message ? {,\n        message: dbJob.error_message,\n        details: dbJob.error_details,\n        retryCount: dbJob.retry_count,\n        maxRetries: 3,\n        recoverable: true\n      } : undefined,\n      createdAt: new Date(dbJob.created_at),\n      startedAt: dbJob.started_at ? new Date(dbJob.started_at) : undefined,\n      completedAt: dbJob.completed_at ? new Date(dbJob.completed_at) : undefined,\n      updatedAt: new Date(dbJob.updated_at)\n    };\n  }\n}\n\n// ============================================================================\n// Singleton Export\n// ============================================================================\n\nexport const mlxFineTuningService = new MLXFineTuningService();\nexport default mlxFineTuningService;"
      }
    ],
    "/Users/christianmerrill/Desktop/universal-ai-tools/src/utils/logger.ts": [
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/utils/logger.ts",
        "pattern": "unterminated_string_single",
        "count": 20,
        "originalContent": "import winston from 'winston';\n\nexport enum LogContext {\n  SERVER = 'server',\n  API = 'api',\n  AGENT = 'agent',\n  MEMORY = 'memory',\n  DATABASE = 'database',\n  DSPY = 'dspy',\n  SYSTEM = 'system',\n  AUTH = 'auth',\n  WEBSOCKET = 'websocket',\n  AI = 'ai',\n  CONFIG = 'config',\n  CACHE = 'cache'\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';\nconst isDevelopment = process.env.NODE_ENV === 'development';\n\n// Create Winston logger\nconst logger = winston.createLogger({\n  level: logLevel,\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({\n      format: winston.format.combine(\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({\n    filename: 'logs/error.log',\n    level: 'error',\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({\n    filename: 'logs/combined.log',\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;",
        "fixedContent": "import winston from 'winston';'\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';'\nconst isDevelopment = process.env.NODE_ENV === 'development';'\n\n// Create Winston logger\nconst logger = winston.createLogger({\n  level: logLevel,\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';'\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';'\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({\n      format: winston.format.combine(\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/utils/logger.ts",
        "pattern": "missing_closing_parenthesis",
        "count": 6,
        "originalContent": "import winston from 'winston';'\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';'\nconst isDevelopment = process.env.NODE_ENV === 'development';'\n\n// Create Winston logger\nconst logger = winston.createLogger({\n  level: logLevel,\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';'\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';'\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({\n      format: winston.format.combine(\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;",
        "fixedContent": "import winston from 'winston';'\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';'\nconst isDevelopment = process.env.NODE_ENV === 'development';'\n\n// Create Winston logger\nconst logger = winston.createLogger({)\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';'\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';'\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/utils/logger.ts",
        "pattern": "semicolon_missing_statements",
        "count": 6,
        "originalContent": "import winston from 'winston';'\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';'\nconst isDevelopment = process.env.NODE_ENV === 'development';'\n\n// Create Winston logger\nconst logger = winston.createLogger({)\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';'\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';'\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;",
        "fixedContent": "import winston from 'winston';';\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';';\nconst isDevelopment = process.env.NODE_ENV === 'development';';\n\n// Create Winston logger\nconst logger = winston.createLogger({);\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';';\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';';\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/utils/logger.ts",
        "pattern": "type_annotation_spacing",
        "count": 4,
        "originalContent": "import winston from 'winston';';\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';';\nconst isDevelopment = process.env.NODE_ENV === 'development';';\n\n// Create Winston logger\nconst logger = winston.createLogger({);\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';';\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';';\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;",
        "fixedContent": "import winston from 'winston';';\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';';\nconst isDevelopment = process.env.NODE_ENV === 'development';';\n\n// Create Winston logger\nconst logger = winston.createLogger({);\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';';\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';';\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/utils/logger.ts",
        "pattern": "object_property_spacing",
        "count": 16,
        "originalContent": "import winston from 'winston';';\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';';\nconst isDevelopment = process.env.NODE_ENV === 'development';';\n\n// Create Winston logger\nconst logger = winston.createLogger({);\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';';\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';';\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;",
        "fixedContent": "import winston from 'winston';';\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';';\nconst isDevelopment = process.env.NODE_ENV === 'development';';\n\n// Create Winston logger\nconst logger = winston.createLogger({);\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';';\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';';\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;"
      },
      {
        "file": "/Users/christianmerrill/Desktop/universal-ai-tools/src/utils/logger.ts",
        "pattern": "arrow_function_spacing",
        "count": 5,
        "originalContent": "import winston from 'winston';';\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';';\nconst isDevelopment = process.env.NODE_ENV === 'development';';\n\n// Create Winston logger\nconst logger = winston.createLogger({);\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';';\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';';\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;",
        "fixedContent": "import winston from 'winston';';\n\nexport enum LogContext {\n  SERVER = 'server','\n  API = 'api','\n  AGENT = 'agent','\n  MEMORY = 'memory','\n  DATABASE = 'database','\n  DSPY = 'dspy','\n  SYSTEM = 'system','\n  AUTH = 'auth','\n  WEBSOCKET = 'websocket','\n  AI = 'ai','\n  CONFIG = 'config','\n  CACHE = 'cache''\n}\n\nconst logLevel = process.env.LOG_LEVEL || 'info';';\nconst isDevelopment = process.env.NODE_ENV === 'development';';\n\n// Create Winston logger\nconst logger = winston.createLogger({);\n  level: logLevel,\n  format: winston.format.combine()\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json(),\n    winston.format.printf(({ timestamp, level, message, context, ...meta }) => {\n      const contextStr = context ? `[${context}] ` : '';';\n      const metaStr = Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : '';';\n      return `${timestamp} ${level.toUpperCase()}: ${contextStr}${message}${metaStr}`;\n    })\n  ),\n  transports: [\n    new winston.transports.Console({)\n      format: winston.format.combine()\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\n// Add file transport for production\nif (!isDevelopment) {\n  logger.add(new winston.transports.File({)\n    filename: 'logs/error.log','\n    level: 'error','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n  \n  logger.add(new winston.transports.File({)\n    filename: 'logs/combined.log','\n    maxsize: 5242880, // 5MB\n    maxFiles: 5\n  }));\n}\n\n// Enhanced logging methods with context\nexport const log = {\n  info: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.info(message, { context, ...meta });\n  },\n  \n  error: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.error(message, { context, ...meta });\n  },\n  \n  warn: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.warn(message, { context, ...meta });\n  },\n  \n  debug: (message: string, context?: LogContext, meta?: Record<string, unknown>) => {\n    logger.debug(message, { context, ...meta });\n  }\n};\n\nexport { logger };\nexport default logger;"
      }
    ]
  },
  "errors": [],
  "skippedFiles": []
}